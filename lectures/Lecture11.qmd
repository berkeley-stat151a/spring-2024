---
title: "Estimating prediction uncertainty: Estimating the residual variance"
format:
  html:
    code-fold: false
    code-tools: true
    include-before-body:
     - file: ../macros.md
---

$\LaTeX$



# Goals
- Recall the prediction problem
- Under the Gaussian assumption, prove that the error in the OLS estimate goes to zero
- Under the Gaussian assumption, prove that the error in the residual variance
estimate goes to zero
- Use these facts to provide a consistent prediction interval


# Gaussian assumption

Recall that we want to form predictions for $\y_\new$
using only $\x_\new$ and the OLS predictor $\betahat = (\X^\trans \X)^{-1} \X^\trans \Y$.
We're using the assumptions

  - *A1:* The $\res_n$ (including $\res_\new$) are IID $\gauss{0, \sigma^2}$
    for some (unknown) $\sigma$
  - *A2*: The $\y_n$ (including $\y_\new$) are actually generated according
    to $\y_n = \xv_n^\trans \beta + \res_n$ for some unknown $\beta$

  We'll additionally need the following assumption:

  - *A3*: $\frac{1}{N} \X^\trans \X = \meann \xv_n \xv_n^\trans$ converges
    to a non-singular matrix, $\Xcov$

For now, we're only interested in how much error we'll commit when
we keep the training data fixed.  That is, we're interested in
the distribution of

$$
\begin{aligned}
\y_\new ={}& \xv_\new^\trans \beta + \res_\new &\textrm{(by A2)} \\
        \sim{}& \gauss{\xv_\new^\trans \beta, \sigma^2}. &\textrm{(by A1, and properties of normals)} \\
\end{aligned}
$$

The trouble is, we don't know $\beta$ or $\sigma$.  One of our main
goals today will be to show that, as $N \rightarrow \infty$,

$$
\begin{aligned}
\betahat  \rightarrow{}& \beta \quad\textrm{and}\\
\sigmahat^2 := \meann \reshat_n^2 \rightarrow{}& \sigma^2.
\end{aligned}
$$

That means, for large enough $N$, we may as well use $\betahat$
and $\sigmahat$ as $\beta$ and $\sigma$.

## Gaussian intervals.

But first, let's talk about what we'd do if we did know $\beta$ 
and $\sigma$.  One way of expressing uncertainty is an
interval, $(\y_{low}, \y_{up})$, such that 
$$
\prob{\y_\new \in (\y_{low}, \y_{up})} = 1 - \alpha
$$

for some small $\alpha$.  Here, we're imagining that we know
$\beta$ and $\sigma$.



# Coefficient consistency


## Method 1 for coefficient consistency: Use normality

Plugging in, we get
$$
\betahat = (\X^\trans \X)^{-1} \X^\trans \Y = 
(\X^\trans \X)^{-1} \X^\trans (\X \beta + \resv) =
\beta + (\X^\trans \X)^{-1} \X^\trans \resv.
$$

In this expression, only $\resv$ is random!  In fact, since $\resv$
is Gaussian, and $\betahat$ is an affine transformation of $\resv$,
then $\betahat$ itself is Gaussian --- albeit with an unknown mean
and variance.  Noting that

$$
\begin{aligned}
\expect{\betahat} ={}& \beta + (\X^\trans \X)^{-1} \X^\trans \expect{\resv} = \beta \\
\cov{\betahat} ={}& \expect{(\betahat - \beta) (\betahat - \beta)^\trans}
\\ ={}& \expect{(\X^\trans \X)^{-1} \X^\trans \resv \resv^\trans \X (\X^\trans \X)^{-1} }
\\ ={}& (\X^\trans \X)^{-1} \X^\trans \expect{\resv \resv^\trans} \X (\X^\trans \X)^{-1}
\\ ={}& (\X^\trans \X)^{-1} \X^\trans \sigma^2 \id{} \X (\X^\trans \X)^{-1}
\\ ={}& \sigma^2 (\X^\trans \X)^{-1} \X^\trans \X (\X^\trans \X)^{-1}
\\ ={}& \sigma^2 (\X^\trans \X)^{-1},
\end{aligned}
$$

we get the beautiful expression
$$
\betahat \sim \gauss{\beta, \sigma^2 (\X^\trans \X)^{-1}}.
$$

Now, $\X$ is not random, but $\X^\trans \X = \sumn \xv_n \xv_n^\trans$,
so it's reasonable to assume by analogy with the LLN that:

::: {.callout-note title='Nicely behaved fixed regressors assumption'} 
Assume that the deterministic sequence $\xv_n$ satisfies
$$
\lim_{N\rightarrow\infty} \frac{1}{N} \X^\trans \X =
\lim_{N\rightarrow\infty} \meann \xv_n \xv_n^\trans \rightarrow \Sigmam_\X,
$$
where $\Sigmam_\X$ is positive definite.
:::


::: {.callout-warning title='Exercise'} 
How would the OLS solution behave if the previous assumption were violated?
Note that there are two parts --- positive definiteness of the limit, 
and existence of the limit when scaling $\X^\trans \X$ by $1/N$.
:::


With "nicely behaved regressors" we thus get that

$$
\betahat \sim \gauss{\beta, \frac{1}{N} \sigma^2 \left(\frac{1}{N} \X^\trans \X \right)^{-1}}
\approx \gauss{\beta, \frac{1}{N} \sigma^2 \Sigmam_\X^{-1}}.
$$

It follows that the variance of $\betahat$ goes to zero as $N \rightarrow \infty$,
and so it concentrates at its mean, $\beta$.



## Method 2 for coefficient consistency: Use the LLN

The previous proof really relied on Normality.  However, we can get a similar
result using only the LLN.  Write:

$$
\begin{aligned}
\betahat - \beta ={}&  
  \left(\X^\trans \X\right)^{-1} \X^\trans \resv 
\\={}&
  \left(\frac{1}{N} \X^\trans \X\right)^{-1} \frac{1}{N} \X^\trans \resv.
\end{aligned}
$$

We've already assumed that 

$$
\left(\frac{1}{N} \X^\trans \X \right)^{-1} \rightarrow \Sigmam_\X^{-1}.
$$

What about the second term?  This is something to which we can apply
the vector LLN.  If we write $\zv_n := \xv_n \res_n$, then
$\expect{\zv_n} = \zerov$ and

$\cov{\zv_n} = \sigma^2 \xv_n \xv_n^\trans$.  If we assume that
$\xv_n \xv_n^\trans$ is bounded for all $n$, then we can apply
our LLN to get

$$
\frac{1}{N} \X^\trans \resv = \meann \xv_n \resv_n \rightarrow \expect{\xv_1 \resv_1} = \xv_1  \expect{\resv_1} = \zerov.
$$

From this it follows that

$$
\betahat \rightarrow \beta \quad\textrm{as }N \rightarrow \infty
$$

by the LLN alone.

::: {.callout-warning title='Exercise'} 
In fact, we do not require $\xv_n \xv_n^\trans$ is bounded for all $n$ ---
all we really need is $\max_{n\in\{1, \ldots, N\},p} \frac{1}{N} \xv_{np}^2  \rightarrow 0$
as $N \rightarrow \infty$.  Refer back to our proof of the LLN and show that (a)
this condition suffices for the LLN and (b) that it is implied by our "nice regressor"
assumption.
:::


## Method 3 for coefficient consistency: Plug into the general formula

At the beginning, we derived the (initially disappointing) general formula

$$
\betahat \rightarrow \Xcov^{-1} \expect{\xv_1 \y_1}.
$$

If we revert to letting $\xv_n$ be random, then we can simply plug in to get

$$
\begin{aligned}
\betahat \rightarrow{}& \Xcov^{-1} \expect{\xv_1 (\xv_1^\trans \beta + \res_1)} 
\\={}& \Xcov^{-1} \expect{\xv_1 \xv_1^\trans} \beta + \Xcov^{-1} \expect{\xv_1 \res_1}
\\={}& \Xcov^{-1} \Xcov \beta + \Xcov^{-1} \expect{\xv_1 \expect{\res_1 | \xv_1}}
\\={}& \beta + \Xcov^{-1} \expect{\xv_1 0}
\\={}& \beta.
\end{aligned}
$$

Though superficially different, all three of these methods relied on the same
facts behind the scenes --- namely, that
$\expect{\betahat} = \beta$, and $\cov{\betahat} \rightarrow \zerov$ as $N \rightarrow \infty$.




# Estimating the residual variance


*Work in progress*

## Method 1: Apply LLN to the squared residuals


## Method 2: Projection to get the unbiased estimate


(Note connection to RSS.)



# Putting it all together

For large $N$, we thus have (approximately),

$$
\y_\new - \yhat_\new \sim \gauss{0, \sigmahat^2}
\quad\Leftrightarrow\quad
\y_\new \sim \gauss{\yhat_\new, \sigmahat^2}
$$.

