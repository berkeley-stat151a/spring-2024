<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Univariate statistics and limit theorems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Univariate statistics and limit theorems</li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../stat_bear.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/berkeley-stat151a/spring-2024" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course_policies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Policies</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lectures and labs</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments/assignments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../datasets/data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Datasets</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#goals" id="toc-goals" class="nav-link active" data-scroll-target="#goals">Goals</a></li>
  <li><a href="#uncertainty-in-prediction-problems" id="toc-uncertainty-in-prediction-problems" class="nav-link" data-scroll-target="#uncertainty-in-prediction-problems">Uncertainty in prediction problems</a></li>
  <li><a href="#stochastic-assumptions" id="toc-stochastic-assumptions" class="nav-link" data-scroll-target="#stochastic-assumptions">Stochastic assumptions</a></li>
  <li><a href="#univariate-random-variable-review" id="toc-univariate-random-variable-review" class="nav-link" data-scroll-target="#univariate-random-variable-review">Univariate random variable review</a>
  <ul class="collapse">
  <li><a href="#limit-theorems" id="toc-limit-theorems" class="nav-link" data-scroll-target="#limit-theorems">Limit theorems</a>
  <ul class="collapse">
  <li><a href="#lln" id="toc-lln" class="nav-link" data-scroll-target="#lln">LLN</a></li>
  <li><a href="#clt" id="toc-clt" class="nav-link" data-scroll-target="#clt">CLT</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
$$

\newcommand{\mybold}[1]{\boldsymbol{#1}} 


\newcommand{\trans}{\intercal}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\bbr}{\mathbb{R}}
\newcommand{\bbz}{\mathbb{Z}}
\newcommand{\bbc}{\mathbb{C}}
\newcommand{\gauss}[1]{\mathcal{N}\left(#1\right)}
\newcommand{\chisq}[1]{\mathcal{\chi}^2_{#1}}
\newcommand{\studentt}[1]{\mathrm{StudentT}_{#1}}

\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\,}
\newcommand{\projop}[1]{\underset{#1}{\mathrm{Proj}}\,}
\newcommand{\proj}[1]{\underset{#1}{\mybold{P}}}
\newcommand{\expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\dens}[1]{\mathit{p}\left(#1\right)}
\newcommand{\var}[1]{\mathrm{Var}\left(#1\right)}
\newcommand{\cov}[1]{\mathrm{Cov}\left(#1\right)}
\newcommand{\sumn}{\sum_{n=1}^N}
\newcommand{\meann}{\frac{1}{N} \sumn}
\newcommand{\cltn}{\frac{1}{\sqrt{N}} \sumn}

\newcommand{\trace}[1]{\mathrm{trace}\left(#1\right)}
\newcommand{\diag}[1]{\mathrm{Diag}\left(#1\right)}
\newcommand{\grad}[2]{\nabla_{#1} \left. #2 \right.}
\newcommand{\gradat}[3]{\nabla_{#1} \left. #2 \right|_{#3}}
\newcommand{\fracat}[3]{\left. \frac{#1}{#2} \right|_{#3}}


\newcommand{\W}{\mybold{W}}
\newcommand{\w}{w}
\newcommand{\wbar}{\bar{w}}
\newcommand{\wv}{\mybold{w}}

\newcommand{\X}{\mybold{X}}
\newcommand{\x}{x}
\newcommand{\xbar}{\bar{x}}
\newcommand{\xv}{\mybold{x}}
\newcommand{\Xcov}{\Sigmam_{\X}}
\newcommand{\Xcovhat}{\hat{\Sigmam}_{\X}}
\newcommand{\Covsand}{\Sigmam_{\mathrm{sand}}}
\newcommand{\Covsandhat}{\hat{\Sigmam}_{\mathrm{sand}}}

\newcommand{\Z}{\mybold{Z}}
\newcommand{\z}{z}
\newcommand{\zv}{\mybold{z}}
\newcommand{\zbar}{\bar{z}}

\newcommand{\Y}{\mybold{Y}}
\newcommand{\Yhat}{\hat{\Y}}
\newcommand{\y}{y}
\newcommand{\yv}{\mybold{y}}
\newcommand{\yhat}{\hat{\y}}
\newcommand{\ybar}{\bar{y}}

\newcommand{\res}{\varepsilon}
\newcommand{\resv}{\mybold{\res}}
\newcommand{\resvhat}{\hat{\mybold{\res}}}
\newcommand{\reshat}{\hat{\res}}

\newcommand{\betav}{\mybold{\beta}}
\newcommand{\betavhat}{\hat{\bv}}
\newcommand{\betahat}{\hat{\beta}}
\newcommand{\betastar}{{\beta^{*}}}

\newcommand{\bv}{\mybold{\beta}}
\newcommand{\bvhat}{\hat{\bv}}
\newcommand{\bhat}{\hat{\beta}}

\newcommand{\alphav}{\mybold{\alpha}}
\newcommand{\alphavhat}{\hat{\av}}
\newcommand{\alphahat}{\hat{\alpha}}

\newcommand{\gv}{\mybold{\gamma}}
\newcommand{\gvhat}{\hat{\gv}}
\newcommand{\ghat}{\hat{\gamma}}

\newcommand{\hv}{\mybold{\h}}
\newcommand{\hvhat}{\hat{\hv}}
\newcommand{\hhat}{\hat{\h}}

\newcommand{\gammav}{\mybold{\gamma}}
\newcommand{\gammavhat}{\hat{\gammav}}
\newcommand{\gammahat}{\hat{\gamma}}

\newcommand{\new}{\mathrm{new}}
\newcommand{\zerov}{\mybold{0}}
\newcommand{\onev}{\mybold{1}}
\newcommand{\id}{\mybold{I}}

\newcommand{\sigmahat}{\hat{\sigma}}


\newcommand{\etav}{\mybold{\eta}}
\newcommand{\muv}{\mybold{\mu}}
\newcommand{\Sigmam}{\mybold{\Sigma}}

\newcommand{\rdom}[1]{\mathbb{R}^{#1}}

\newcommand{\RV}[1]{\tilde{#1}}



\def\A{\mybold{A}}

\def\A{\mybold{A}}
\def\av{\mybold{a}}
\def\a{a}

\def\B{\mybold{B}}


\def\S{\mybold{S}}
\def\sv{\mybold{s}}
\def\s{s}

\def\R{\mybold{R}}
\def\rv{\mybold{r}}
\def\r{r}

\def\V{\mybold{V}}
\def\vv{\mybold{v}}
\def\v{v}

\def\U{\mybold{U}}
\def\uv{\mybold{u}}
\def\u{u}

\def\tv{\mybold{t}}
\def\t{t}

\def\Sc{\mathcal{S}}
\def\ev{\mybold{e}}

\def\Lammat{\mybold{\Lambda}}


$$

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Univariate statistics and limit theorems</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><span class="math inline">\(\LaTeX\)</span></p>
<section id="goals" class="level1">
<h1>Goals</h1>
<ul>
<li>Introduce / review univariate probability
<ul>
<li>Expectations and variances</li>
<li>The law of large numbers</li>
<li>The central limit theorem</li>
</ul></li>
</ul>
</section>
<section id="uncertainty-in-prediction-problems" class="level1">
<h1>Uncertainty in prediction problems</h1>
<p>Recall our bodyfat prediction problem. Suppose we decide we want to predict bodyfat using the abdomen measurement, height, and weight; these variables, plus a constant, will be included in <span class="math inline">\(\xv_n\)</span>, and we’ll take <span class="math inline">\(\y_n\)</span> to be measured bodyfat, and regress <span class="math inline">\(\y_n \sim \xv_n^\trans Bv\)</span>. (We will talk later about how to choose which regressors to include, but for now we’ll take them as fixed.)</p>
<p>We fit a line using OLS, giving us <span class="math inline">\(\bvhat\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(bodyfat <span class="sc">~</span> Abdomen <span class="sc">+</span> Height <span class="sc">+</span> Weight, bodyfat_df)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(reg<span class="sc">$</span>coefficients)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)     Abdomen      Height      Weight 
-36.6147193   0.9515631  -0.1270307  -0.1307606 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">PlotFit</span>(reg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Lecture7_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Later, a new person will come into the doctor’s office with measurements <span class="math inline">\(\xv_\new\)</span>. We would like to know their <span class="math inline">\(\y_\new\)</span>, and are considering approximate it with <span class="math inline">\(\yhat_\new = \xv_\new^\trans Bvhat\)</span>, hoping that <span class="math inline">\(\yhat_\new Approx \y_\new\)</span>.</p>
<p>Here are some questions to ask:</p>
<ol type="1">
<li>How much error do we expect to incur on an individual with <span class="math inline">\(\xv_\new\)</span>?</li>
<li>How much error do we expect to incur, on average, over a large number of new individuals?</li>
<li>Would a different set of regressors have given us better prediction?</li>
<li>Would it improve our estimates to gather more data (i.e.&nbsp;to increase <span class="math inline">\(N\)</span>)?</li>
<li>Were we to get a new dataset of the same size, would we come up with a different <span class="math inline">\(Betahat\)</span>? How different?</li>
<li>Are these errors tolerable for our intended usage?</li>
</ol>
<p>Without more assumptions about how the data were gathered, and what sorts of individuals we expect to see in the future, these questions are impossible to answer at all, much less quantitatively.</p>
</section>
<section id="stochastic-assumptions" class="level1">
<h1>Stochastic assumptions</h1>
<p>In order to attempt to provide useful answers to these questions — or at least approximate answers — we will be imposing <em>stochastic assumptions</em>. This means we will model the data as having come from a random distribution, and use probability calculus to make statements about the errors we are likely to make. This does not mean the data <em>did</em> come from a random distribution, nor does it mean that the analysis is worthless if the data did <em>not</em> come from a random distribution. I like to think that we are using stochastic models as metaphors for what we don’t know. Ideally these metaphors are both convincing but also falsifiable.</p>
<p>We will study a variety of stochastic models, but they will all involve the following assumption:</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Assumption 1: IID">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Assumption 1: IID
</div>
</div>
<div class="callout-body-container callout-body">
<p>The pairs <span class="math inline">\((\xv_n, \y_n)\)</span> are drawn IID from some distribution.</p>
</div>
</div>
<p>The benefit of Assumption 1 is that we can use <em>asymptotics</em> to study the behavior of the least squares fit when <span class="math inline">\(N\)</span> is large. This is good beacause (a) <span class="math inline">\(N\)</span> is at least sometimes large and (b) it’s much harder to do statistical inference without aysmpotitics. Additionally, a lot of classical statistical theory that is still in use is asymptotic in nature, and so it is good to critically understand asymptotics in order to understand a lot of statistical practice, even (or especially) questionable statistical practice.</p>
<div class="callout callout-style-default callout-warning callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Describe a circumstance when Assumption 1 is reasonable in our bodyfat example, and one where it is unreasonable.</p>
</div>
</div>
<p>Asymptotic answers to our questions will come down to understanding the stochastic behavior of <span class="math inline">\(Bvhat\)</span> as <span class="math inline">\(N\)</span> gets large, and <span class="math inline">\(Bvhat\)</span> is a vector, which is computed from matrices. So we need to extend the asymptotics you know about scalar-valued quantities to vector and matrix valued-quantities.</p>
<p>There are essentially three asymptotic results we’ll need:</p>
<ul>
<li>Laws of large numbers, or convergence of sample averages to population expectations</li>
<li>Central limit theorems, or convergence of rescaled sample averages to normal distributions and</li>
<li>Theorems that combine the two.</li>
</ul>
</section>
<section id="univariate-random-variable-review" class="level1">
<h1>Univariate random variable review</h1>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some authors use capital letters to denote random variables, and lower-case letters to denote their realizations. I will not do so since that will conflict with the existing matrix notation. Though I recognize the importance of differentiating between random variables and their realizations, I’m not sure I want to commit right now to one particular notation. For now I will use tildes: <span class="math inline">\(\RV{\x}\)</span> will be a random variable, and <span class="math inline">\(\x\)</span> a realization.<br>
Hopefully future lecture notes will settle on one consistent solution.</p>
</div>
</div>
<p>A scalar-valued random variable <span class="math inline">\(\RV{\x}\)</span> is formally a mapping that assigns probabilities to sets of possible values that <span class="math inline">\(\RV{\x}\)</span> can take. That is, for any legitimate set <span class="math inline">\(A\)</span>, we can evaluate <span class="math inline">\(\prob{\RV{\x} \in A}\)</span>, in a way that respects common-sense rules of probability:</p>
<ul>
<li><span class="math inline">\(\prob{\RV{\x} \in A} \in [0, 1]\)</span></li>
<li>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint (<span class="math inline">\(A \bigcap B = \emptyset\)</span>), then <span class="math inline">\(\prob{\RV{\x} \in A \bigcup B} = \prob{\RV{\x} \in A} + \prob{\RV{\x} \in B}\)</span>.<br>
</li>
<li>Furthermore, you can do this for infinitely many sets — if <span class="math inline">\(A_n\)</span> and all mutually disjoint for <span class="math inline">\(n=1, 2, \ldots\)</span>, then <span class="math inline">\(\prob{\RV{\x} \in \bigcup_{n=1}^N A_n} = \sumn \prob{\RV{\x} \in A_n}\)</span>.</li>
<li><span class="math inline">\(\prob{\RV{\x} \in \Omega} = 1\)</span>, where <span class="math inline">\(\Omega\)</span> is the set of all values <span class="math inline">\(\RV{\x}\)</span> might take.</li>
</ul>
<p>Many of the random variables we’ll encounter in this course have <em>densities</em>, which give probabilities as integrals. For example, if <span class="math inline">\(\RV{\x}\)</span> has a density <span class="math inline">\(\dens{x}\)</span>, then</p>
<p><span class="math display">\[
\prob{\RV{\x} \in A} = \int_A \dens{x} dx.
\]</span></p>
<p>Sadly the notation for densities is often ambiguous. Different random variables have different densities, of course, and they are all denoted with the letter <span class="math inline">\(\dens{\cdot}\)</span>. The argument is typically used to denote which random variable the density applies to, as I have done here: <span class="math inline">\(\dens{x}\)</span> is the density for the random variable <span class="math inline">\(\RV{\x}\)</span> evaluated at the point <span class="math inline">\(x\)</span>. Perhaps it would be better to write <span class="math inline">\(p_{\RV{\x}}(x)\)</span>, but often this is not done, and it is hopefully clear from the context precisely which density we mean.</p>
<p>The density is useful to define expectations of functions of <span class="math inline">\(\RV{\x}\)</span>. Specifically, for a (sufficiently well-behaved) function <span class="math inline">\(f(x)\)</span>,</p>
<p><span class="math display">\[
\expect{f(\RV{\x})} = \int f(x) \dens{x} dx.
\]</span></p>
<p>An important property of expectation is that it’s linear. That means that, for any fixed numbers (not random variables!) <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, we have</p>
<p><span class="math display">\[
\expect{a \RV{\x} + b} = a\expect{\RV{\x}}  + b.
\]</span></p>
<p>This property follows from linearity of the integral.</p>
<p>Two special expectations are particularly important: <span class="math inline">\(f(x) = x\)</span> and <span class="math inline">\(f(x) = x^2\)</span>. The expectations of these functions correspond to the “first and second moments” of <span class="math inline">\(\RV{\x}\)</span>. In principle, these moments may not be finite, though in this class we will typically assume that they are finite.</p>
<p>The variance of a random variable, when it exists, is defined in terms of these moments as <span class="math display">\[
\var{\RV{\x}} := \expect{\x^2} - \expect{\x}^2 = \expect{(\x - \expect{\x})^2}.
\]</span></p>
<p>The right hand side shows that the variance can never be negative, although it can be zero.</p>
<div class="callout callout-style-default callout-warning callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Using linearity of expectation, what is <span class="math inline">\(\var{a \RV{\x}}\)</span>?</p>
</div>
</div>
<section id="limit-theorems" class="level2">
<h2 class="anchored" data-anchor-id="limit-theorems">Limit theorems</h2>
<p>There are two important limit theorems that we will use: laws of large numbers (LLN) and central limit theorems (CLT). In this class I will not worry too much about getting the strongest possible results (e.g., weakest possible assumptions for a particular convergence guarantee). So I’ll choose conditions that are simple to work with.</p>
<section id="lln" class="level3">
<h3 class="anchored" data-anchor-id="lln">LLN</h3>
<p>First, let’s state and prove a LLN. Suppose <span class="math inline">\(\RV{\x}_n\)</span> are independent random variables for <span class="math inline">\(n =1, \ldots, \infty\)</span> such that <span class="math inline">\(\max_n \var{\RV{\x}_n} &lt; \infty\)</span>. That is, every variance is finite and the variances do not diverge.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Note to future instructors">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note to future instructors
</div>
</div>
<div class="callout-body-container callout-body">
<p>Actually, all we need is <span class="math inline">\(\lim_{N\rightarrow\infty} \max_{n=1,\ldots,N} \frac{1}{N} \var{\RV{\x}_n} = 0\)</span>, which is more convenient when we actually use this for the fixed regressor setting, but which complicates the exhibition here.</p>
</div>
</div>
<p>Write <span class="math inline">\(\expect{\RV{\x}_n} = \mu_n\)</span> and assume that <span class="math inline">\(\meann \mu_n \rightarrow \overline{\mu} &lt; \infty\)</span>. Then consider the random variable <span class="math display">\[
\overline{x} := \frac{1}{N} \sumn \RV{\x}_n.
\]</span></p>
<p>We can see that</p>
<p><span class="math display">\[
\begin{aligned}
\expect{\overline{x}} ={}&amp; \meann \mu_n \\
\var{\overline{x}} ={}&amp; \
  \expect{\left(\overline{x} - \meann \mu_n\right)^2} \\
={}&amp;  \expect{\left(\meann (\RV{\x}_n - \mu_n) \right)^2} \\
={}&amp;  \frac{1}{N^2} \sumn \expect{(\RV{\x}_n - \mu_n)^2} \\
\le{}&amp;  \frac{1}{N^2}  N \max_n \var{\RV{\x}_n} \rightarrow 0
\quad\textrm{as }N\rightarrow \infty.
\end{aligned}
\]</span></p>
<p>A random variable with zero variance is just a constant, so we have shown that <span class="math inline">\(\overline{x}\)</span> converges to the constant <span class="math inline">\(\overline{\mu}\)</span>, which is the limit of <span class="math inline">\(\meann \mu_n\)</span>. (Markov’s theorem gives a formal proof of convergence in probability, but I think that’s beyond the scope of this class.) <span class="math display">\[
\overline{x} \rightarrow \overline{\mu} \quad
\textrm{ as }N\rightarrow \infty.
\]</span></p>
<p>Note that in the preceding statement, <span class="math inline">\(\overline{x}\)</span> is a random variable, but <span class="math inline">\(\overline{\mu}\)</span> is a constant.</p>
<p>If <span class="math inline">\(\mu_n = \mu = \overline{\mu}\)</span>, then this gives the familiar LLN from introductory probability, but this more general form will be useful in regression.</p>
</section>
<section id="clt" class="level3">
<h3 class="anchored" data-anchor-id="clt">CLT</h3>
<p>The normal distribution is special because it is the limit of a rescaled average. Take the setting of the previous example, and recall that, as <span class="math inline">\(N \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[
\overline{x} - \overline{\mu} = \meann (\RV{\x}_n - \mu_n) \rightarrow 0.
\]</span></p>
<p>One might note that, by the same argument of the previous section,</p>
<p><span class="math display">\[
\var{N (\overline{x} - \overline{\mu})} \rightarrow \infty.
\]</span></p>
<p>That is, if you scale <span class="math inline">\(\overline{x} - \overline{\mu}\)</span> by <span class="math inline">\(N\)</span>, for large <span class="math inline">\(N\)</span>, you get a random variable that takes arbitrarily large values with nonzero probability. However,</p>
<p><span class="math display">\[
\begin{aligned}
\var{\sqrt{N} \overline{x}} ={}&amp; \
  \expect{\left(\sqrt{N} \overline{x} - \sqrt{N} \meann \mu_n\right)^2} \\
={}&amp;  \expect{\left(\sqrt{N} \meann (\RV{\x}_n - \mu_n) \right)^2} \\
={}&amp;  \frac{1}{N} \sumn \expect{(\RV{\x}_n - \mu_n)^2} \\
={}&amp;  \meann \var{\RV{\x}_n},
\end{aligned}
\]</span></p>
<p>a quantity that might reasonably converge to a constant, giving <span class="math inline">\(\sqrt{N} (\overline{x} - \overline{\mu})\)</span> converging to a nondegenerate, but also nondivergent, random variable, as long as <span class="math display">\[
\meann \var{\RV{\x}_n} \rightarrow \overline{v}.
\]</span></p>
<p>(Note that the limit must be finite by assumption, but we do need to additionally assume that the limit exists.)</p>
<p>In fact, the CLT says something stronger. Not only does <span class="math inline">\(\sqrt{N} (\overline{x} - \overline{\mu})\)</span> converge to a nicely behaved random variable, but the variable it converges to is normally distributed:</p>
<p><span class="math display">\[
\sqrt{N} (\overline{x} - \overline{\mu}) =
\frac{1}{\sqrt{N}} \sumn (\RV{\x}_n - \mu_n) \rightarrow \RV{\z}
\quad\textrm{where }\RV{\z} \sim \gauss{0, \overline{v}}.
\]</span></p>
<p>I won’t prove this here, but you can find lots of references that do prove it. As above, taking <span class="math inline">\(\mu_n = \mu = \overline{\mu}\)</span> and <span class="math inline">\(\var{\RV{\x}_n} = \overline{v}\)</span> gives the familiar IID result.</p>


<!-- -->

</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Univariate statistics and limit theorems"</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">    include-before-body:</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">     - file: ../macros.md</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>$\LaTeX$</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="fu"># Goals</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Introduce / review univariate probability</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Expectations and variances</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The law of large numbers</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The central limit theorem</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="fu"># Uncertainty in prediction problems</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">repr.plot.width =</span> <span class="dv">15</span>, <span class="at">repr.plot.height =</span> <span class="dv">8</span>, <span class="at">repr.plot.res =</span> <span class="dv">300</span>)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_update</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">24</span>))</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">repr.plot.width=</span><span class="dv">12</span>, <span class="at">repr.plot.height=</span><span class="dv">6</span>)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>data_location <span class="ot">&lt;-</span> <span class="st">"../datasets"</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>bodyfat_df <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="fu">file.path</span>(data_location, <span class="st">"bodyfat.csv"</span>))</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>PlotFit <span class="ot">&lt;-</span> <span class="cf">function</span>(reg) {</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>reg<span class="sc">$</span>model<span class="sc">$</span>bodyfat, <span class="at">y=</span>reg<span class="sc">$</span>fitted.values)) <span class="sc">+</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        <span class="fu">geom_abline</span>(<span class="fu">aes</span>(<span class="at">slope=</span><span class="dv">1</span>, <span class="at">intercept=</span><span class="dv">0</span>)) <span class="sc">+</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        <span class="fu">xlab</span>(<span class="st">"Actual bodyfat"</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">"Predicted bodyfat"</span>)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>Recall our bodyfat prediction problem.  Suppose we decide we want to</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>predict bodyfat using the abdomen measurement, height, and weight;</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>these variables, plus a constant, will be included in $\xv_n$,</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>and we'll take $\y_n$ to be measured bodyfat, and regress</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>$\y_n \sim \xv_n^\trans Bv$.</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>(We will talk later about how to choose which regressors to </span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>include, but for now we'll take them as fixed.)  </span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>We fit a line using OLS, giving us $\bvhat$.</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(bodyfat <span class="sc">~</span> Abdomen <span class="sc">+</span> Height <span class="sc">+</span> Weight, bodyfat_df)</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(reg<span class="sc">$</span>coefficients)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a><span class="fu">PlotFit</span>(reg)</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>Later, a new person will come into the doctor's office with</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>measurements $\xv_\new$.  We would like to know their</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>$\y_\new$, and are considering approximate it with </span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>$\yhat_\new = \xv_\new^\trans Bvhat$, hoping that $\yhat_\new Approx \y_\new$.</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>Here are some questions to ask:</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>1) How much error do we expect to incur on an individual with $\xv_\new$?</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>1) How much error do we expect to incur, on average, over a large number of new individuals?</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>1) Would a different set of regressors have given us better prediction?</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>1) Would it improve our estimates to gather more data (i.e. to increase $N$)?</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>1) Were we to get a new dataset of the same size, would we come up with a different $Betahat$? How different?</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>1) Are these errors tolerable for our intended usage?</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>Without more assumptions about how the data were gathered, and what</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>sorts of individuals we expect to see in the future, these questions</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>are impossible to answer at all, much less quantitatively.</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a><span class="fu"># Stochastic assumptions</span></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>In order to attempt to provide useful answers to these questions --- or at</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>least approximate answers --- we will be imposing *stochastic assumptions*.  This</span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>means we will model the data as having come from a random distribution, and</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>use probability calculus to make statements about the errors we are</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>likely to make.   This does not mean the data *did* come from a random distribution, nor </span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>does it mean that the analysis is worthless if the data did *not* come from a random</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>distribution.  I like to think that we are using stochastic models as </span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>metaphors for what we don't know.  Ideally these metaphors are both convincing</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>but also falsifiable.</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>We will study a variety of stochastic models, but they will all involve</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>the following assumption:</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Assumption 1: IID"}</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>The pairs $(\xv_n, \y_n)$ are drawn IID from some distribution.</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>The benefit of Assumption 1 is that we can use *asymptotics* to study</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>the behavior of the least squares fit when $N$ is large.  This is good</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>beacause (a) $N$ is at least sometimes large and (b) it's much harder</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>to do statistical inference without aysmpotitics.  Additionally, a lot</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>of classical statistical theory that is still in use is asymptotic in</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>nature, and so it is good to critically understand asymptotics</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>in order to understand a lot of statistical practice, even (or especially)</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>questionable statistical practice.</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Exercise'} </span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>Describe a circumstance when Assumption 1 is reasonable in our bodyfat example,</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>and one where it is unreasonable.</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a>Asymptotic answers to our questions will come down to understanding the </span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a>stochastic behavior of $Bvhat$ as $N$ gets large, and $Bvhat$ is a</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a>vector, which is computed from matrices.  So we need to extend</span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a>the asymptotics you know about scalar-valued quantities</span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>to vector and matrix valued-quantities.</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>There are essentially three asymptotic results we'll need:</span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Laws of large numbers, or convergence of sample averages to population expectations</span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Central limit theorems, or convergence of rescaled sample averages to normal distributions and</span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Theorems that combine the two.</span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a><span class="fu"># Univariate random variable review</span></span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Notation"}</span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a>Some authors use capital letters to denote random variables, and lower-case letters</span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a>to denote their realizations.  I will not do so since that will conflict with</span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a>the existing matrix notation.  Though I recognize the importance of differentiating</span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a>between random variables and their realizations, I'm not sure I want to commit</span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>right now to one particular notation.  For now I will use tildes: </span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>$\RV{\x}$ will be a random variable, and $\x$ a realization.  </span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a>Hopefully future lecture notes will settle on one consistent solution.</span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a>A scalar-valued random variable $\RV{\x}$ is formally a mapping that assigns</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a>probabilities to sets of possible values that $\RV{\x}$ can take.</span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a>That is, for any legitimate set $A$, we can evaluate $\prob{\RV{\x} \in A}$,</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a>in a way that respects common-sense rules of probability:</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\prob{\RV{\x} \in A} \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $A$ and $B$ are disjoint ($A \bigcap B = \emptyset$), then</span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>  $\prob{\RV{\x} \in A \bigcup B} = \prob{\RV{\x} \in A} + \prob{\RV{\x} \in B}$.  </span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Furthermore,</span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a>  you can do this for infinitely many sets --- if $A_n$ and all mutually disjoint for $n=1, 2, \ldots$,</span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>  then $\prob{\RV{\x} \in \bigcup_{n=1}^N A_n} = \sumn \prob{\RV{\x} \in A_n}$.</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\prob{\RV{\x} \in \Omega} = 1$, where $\Omega$ is the set of all values $\RV{\x}$ might take.</span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a>Many of the random variables we'll encounter in this course have *densities*, which </span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a>give probabilities as integrals.  For example, if $\RV{\x}$ has a density $\dens{x}$, then</span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a>\prob{\RV{\x} \in A} = \int_A \dens{x} dx.</span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a>Sadly the notation for densities is often ambiguous.  Different random variables</span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a>have different densities, of course, and they are all denoted with the letter</span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a>$\dens{\cdot}$.  The argument is typically used to denote which random variable the </span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>density applies to, as I have done here: $\dens{x}$ is the density for the random</span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a>variable $\RV{\x}$ evaluated at the point $x$.  Perhaps it would be better to write</span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a>$p_{\RV{\x}}(x)$, but often this is not done, and it is hopefully clear from the context</span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a>precisely which density we mean.</span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a>The density is useful to define expectations of functions of $\RV{\x}$.  Specifically, for</span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a>a (sufficiently well-behaved) function $f(x)$, </span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a>\expect{f(\RV{\x})} = \int f(x) \dens{x} dx.</span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a>An important property of expectation is that it's linear.  That means that,</span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>for any fixed numbers (not random variables!) $a$ and $b$, we have</span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>\expect{a \RV{\x} + b} = a\expect{\RV{\x}}  + b.</span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>This property follows from linearity of the integral.</span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>Two special expectations are particularly important: $f(x) = x$ and $f(x) = x^2$.  The</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a>expectations of these functions correspond to the "first and second moments" of $\RV{\x}$.</span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>In principle, these moments may not be finite, though in this class we will typically</span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a>assume that they are finite.</span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a>The variance of a random variable, when it exists, is defined in terms of these moments as</span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>\var{\RV{\x}} := \expect{\x^2} - \expect{\x}^2 = \expect{(\x - \expect{\x})^2}.</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a>The right hand side shows that the variance can never be negative, although it can be zero.</span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Exercise'} </span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a>Using linearity of expectation, what is $\var{a \RV{\x}}$?</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a><span class="fu">## Limit theorems</span></span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a>There are two important limit theorems that we will use: laws of large numbers (LLN)</span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>and central limit theorems (CLT).  In this class I will not worry too much about</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a>getting the strongest possible results (e.g., weakest possible assumptions</span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a>for a particular convergence guarantee).  So I'll choose conditions that are</span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a>simple to work with.</span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a><span class="fu">### LLN</span></span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a>First, let's state and prove a LLN.  Suppose $\RV{\x}_n$ are independent random variables</span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a>for $n =1, \ldots, \infty$ such that $\max_n \var{\RV{\x}_n} &lt; \infty$.  That is, </span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a>every variance is finite and the variances do not diverge.  </span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a>::: {.callout-important title='Note to future instructors'} </span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a>Actually, all we need is</span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a>$\lim_{N\rightarrow\infty} \max_{n=1,\ldots,N} \frac{1}{N} \var{\RV{\x}_n} = 0$,</span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a>which is more convenient when we actually use this for the fixed regressor</span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a>setting, but which complicates the exhibition here.</span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a>Write $\expect{\RV{\x}_n} = \mu_n$ and assume that</span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a>$\meann \mu_n \rightarrow \overline{\mu} &lt; \infty$.  Then consider the </span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a>random variable</span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a>\overline{x} := \frac{1}{N} \sumn \RV{\x}_n.</span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a>We can see that</span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a>\expect{\overline{x}} ={}&amp; \meann \mu_n <span class="sc">\\</span></span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a>\var{\overline{x}} ={}&amp; \</span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a>  \expect{\left(\overline{x} - \meann \mu_n\right)^2} <span class="sc">\\</span></span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a>={}&amp;  \expect{\left(\meann (\RV{\x}_n - \mu_n) \right)^2} <span class="sc">\\</span></span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a>={}&amp;  \frac{1}{N^2} \sumn \expect{(\RV{\x}_n - \mu_n)^2} <span class="sc">\\</span></span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a>\le{}&amp;  \frac{1}{N^2}  N \max_n \var{\RV{\x}_n} \rightarrow 0</span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a>\quad\textrm{as }N\rightarrow \infty.</span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a>A random variable with zero variance is just a constant, so we have shown</span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a>that $\overline{x}$ converges to the constant $\overline{\mu}$, which</span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a>is the limit of $\meann \mu_n$.  (Markov's theorem gives a formal</span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a>proof of convergence in probability, but I think that's beyond the</span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a>scope of this class.)</span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a>\overline{x} \rightarrow \overline{\mu} \quad</span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a>\textrm{ as }N\rightarrow \infty.</span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a>Note that in the preceding statement, $\overline{x}$ is a random variable,</span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a>but $\overline{\mu}$ is a constant.  </span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a>If $\mu_n = \mu = \overline{\mu}$,</span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>then this gives the familiar LLN from introductory probability, but</span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a>this more general form will be useful in regression.</span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a><span class="fu">### CLT</span></span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a>The normal distribution is special because it is the limit of</span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a>a rescaled average.  Take the setting of the previous example,</span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a>and recall that, as $N \rightarrow \infty$,</span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a>\overline{x} - \overline{\mu} = \meann (\RV{\x}_n - \mu_n) \rightarrow 0.</span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a>One might note that, by the same argument of the previous section,</span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a>\var{N (\overline{x} - \overline{\mu})} \rightarrow \infty.</span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a>That is, if you scale $\overline{x} - \overline{\mu}$ by $N$, for large</span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a>$N$, you get a random variable that takes arbitrarily large values</span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a>with nonzero probability.  However,</span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a>\var{\sqrt{N} \overline{x}} ={}&amp; \</span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a>  \expect{\left(\sqrt{N} \overline{x} - \sqrt{N} \meann \mu_n\right)^2} <span class="sc">\\</span></span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a>={}&amp;  \expect{\left(\sqrt{N} \meann (\RV{\x}_n - \mu_n) \right)^2} <span class="sc">\\</span></span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a>={}&amp;  \frac{1}{N} \sumn \expect{(\RV{\x}_n - \mu_n)^2} <span class="sc">\\</span></span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a>={}&amp;  \meann \var{\RV{\x}_n},</span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a>a quantity that might reasonably converge to a constant, giving </span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a>$\sqrt{N} (\overline{x} - \overline{\mu})$ converging to a nondegenerate,</span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a>but also nondivergent, random variable, as long as</span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a>\meann \var{\RV{\x}_n} \rightarrow \overline{v}.</span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a>(Note that the limit must be finite by assumption, but we do need to</span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a>additionally assume that the limit exists.)</span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a>In fact, the CLT says something stronger.  Not only does </span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a>$\sqrt{N} (\overline{x} - \overline{\mu})$ converge to a nicely behaved</span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a>random variable, but the variable it converges to is normally </span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a>distributed:</span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-321"><a href="#cb4-321" aria-hidden="true" tabindex="-1"></a>\sqrt{N} (\overline{x} - \overline{\mu}) =</span>
<span id="cb4-322"><a href="#cb4-322" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sqrt{N}} \sumn (\RV{\x}_n - \mu_n) \rightarrow \RV{\z}</span>
<span id="cb4-323"><a href="#cb4-323" aria-hidden="true" tabindex="-1"></a>\quad\textrm{where }\RV{\z} \sim \gauss{0, \overline{v}}.</span>
<span id="cb4-324"><a href="#cb4-324" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-325"><a href="#cb4-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-326"><a href="#cb4-326" aria-hidden="true" tabindex="-1"></a>I won't prove this here, but you can find lots of references that do</span>
<span id="cb4-327"><a href="#cb4-327" aria-hidden="true" tabindex="-1"></a>prove it.   As above, taking $\mu_n = \mu = \overline{\mu}$</span>
<span id="cb4-328"><a href="#cb4-328" aria-hidden="true" tabindex="-1"></a>and $\var{\RV{\x}_n} = \overline{v}$ gives the familiar IID result.</span>
<span id="cb4-329"><a href="#cb4-329" aria-hidden="true" tabindex="-1"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>