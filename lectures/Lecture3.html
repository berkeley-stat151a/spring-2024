<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Multilinear regression as loss minimzation.</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Multilinear regression as loss minimzation.</li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../stat_bear.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/berkeley-stat151a/spring-2024" rel="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course_policies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Policies</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lectures and labs</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments/assignments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../datasets/data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Datasets</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#goals" id="toc-goals" class="nav-link active" data-scroll-target="#goals">Goals</a></li>
  <li><a href="#siomple-least-squares" id="toc-siomple-least-squares" class="nav-link" data-scroll-target="#siomple-least-squares">Siomple least squares</a></li>
  <li><a href="#siomple-least-squares-estimator-derivation" id="toc-siomple-least-squares-estimator-derivation" class="nav-link" data-scroll-target="#siomple-least-squares-estimator-derivation">Siomple least squares estimator derivation</a></li>
  <li><a href="#matrix-multiplication-version" id="toc-matrix-multiplication-version" class="nav-link" data-scroll-target="#matrix-multiplication-version">Matrix multiplication version</a></li>
  <li><a href="#matrix-notation" id="toc-matrix-notation" class="nav-link" data-scroll-target="#matrix-notation">Matrix notation</a></li>
  <li><a href="#least-squares-in-matrix-notation" id="toc-least-squares-in-matrix-notation" class="nav-link" data-scroll-target="#least-squares-in-matrix-notation">Least squares in matrix notation</a></li>
  <li><a href="#what-if-the-matrix-is-not-invertible" id="toc-what-if-the-matrix-is-not-invertible" class="nav-link" data-scroll-target="#what-if-the-matrix-is-not-invertible">What if the matrix is not invertible?</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
$$

\newcommand{\mybold}[1]{\boldsymbol{#1}} 


\newcommand{\trans}{\intercal}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\bbr}{\mathbb{R}}
\newcommand{\bbz}{\mathbb{Z}}
\newcommand{\bbc}{\mathbb{C}}
\newcommand{\gauss}[1]{\mathcal{N}\left(#1\right)}
\newcommand{\chisq}[1]{\mathcal{\chi}^2\left(#1\right)}

\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\,}
\newcommand{\projop}[1]{\underset{#1}{\mathrm{Proj}}\,}
\newcommand{\proj}[1]{\underset{#1}{\mybold{P}}}
\newcommand{\expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\dens}[1]{\mathit{p}\left(#1\right)}
\newcommand{\var}[1]{\mathrm{Var}\left(#1\right)}
\newcommand{\cov}[1]{\mathrm{Cov}\left(#1\right)}
\newcommand{\sumn}{\sum_{n=1}^N}
\newcommand{\meann}{\frac{1}{N} \sumn}

\newcommand{\trace}[1]{\mathrm{trace}\left(#1\right)}
\newcommand{\diag}[1]{\mathrm{Diag}\left(#1\right)}
\newcommand{\grad}[2]{\nabla_{#1} \left. #2 \right.}
\newcommand{\gradat}[3]{\nabla_{#1} \left. #2 \right|_{#3}}
\newcommand{\fracat}[3]{\left. \frac{#1}{#2} \right|_{#3}}


\newcommand{\W}{\mybold{W}}
\newcommand{\w}{w}
\newcommand{\wbar}{\bar{w}}
\newcommand{\wv}{\mybold{w}}

\newcommand{\X}{\mybold{X}}
\newcommand{\x}{x}
\newcommand{\xbar}{\bar{x}}
\newcommand{\xv}{\mybold{x}}
\newcommand{\Xcov}{\Sigmam_{\X}}

\newcommand{\Z}{\mybold{Z}}
\newcommand{\z}{z}
\newcommand{\zv}{\mybold{z}}
\newcommand{\zbar}{\bar{z}}

\newcommand{\Y}{\mybold{Y}}
\newcommand{\Yhat}{\hat{\Y}}
\newcommand{\y}{y}
\newcommand{\yv}{\mybold{y}}
\newcommand{\yhat}{\hat{\y}}
\newcommand{\ybar}{\bar{y}}

\newcommand{\res}{\varepsilon}
\newcommand{\resv}{\mybold{\res}}
\newcommand{\resvhat}{\hat{\mybold{\res}}}
\newcommand{\reshat}{\hat{\res}}

\newcommand{\betav}{\mybold{\beta}}
\newcommand{\betavhat}{\hat{\bv}}
\newcommand{\betahat}{\hat{\beta}}

\newcommand{\bv}{\mybold{\beta}}
\newcommand{\bvhat}{\hat{\bv}}
\newcommand{\bhat}{\hat{\beta}}

\newcommand{\alphav}{\mybold{\alpha}}
\newcommand{\alphavhat}{\hat{\av}}
\newcommand{\alphahat}{\hat{\alpha}}

\newcommand{\gv}{\mybold{\gamma}}
\newcommand{\gvhat}{\hat{\gv}}
\newcommand{\ghat}{\hat{\gamma}}

\newcommand{\hv}{\mybold{\h}}
\newcommand{\hvhat}{\hat{\hv}}
\newcommand{\hhat}{\hat{\h}}

\newcommand{\gammav}{\mybold{\gamma}}
\newcommand{\gammavhat}{\hat{\gammav}}
\newcommand{\gammahat}{\hat{\gamma}}

\newcommand{\new}{\mathrm{new}}
\newcommand{\zerov}{\mybold{0}}
\newcommand{\onev}{\mybold{1}}
\newcommand{\id}{\mybold{I}}

\newcommand{\sigmahat}{\hat{\sigma}}


\newcommand{\etav}{\mybold{\eta}}
\newcommand{\muv}{\mybold{\mu}}
\newcommand{\Sigmam}{\mybold{\Sigma}}

\newcommand{\rdom}[1]{\mathbb{R}^{#1}}

\newcommand{\RV}[1]{\tilde{#1}}



\def\A{\mybold{A}}

\def\A{\mybold{A}}
\def\av{\mybold{a}}
\def\a{a}

\def\B{\mybold{B}}


\def\S{\mybold{S}}
\def\sv{\mybold{s}}
\def\s{s}

\def\R{\mybold{R}}
\def\rv{\mybold{r}}
\def\r{r}

\def\V{\mybold{V}}
\def\vv{\mybold{v}}
\def\v{v}

\def\U{\mybold{U}}
\def\uv{\mybold{u}}
\def\u{u}

\def\Sc{\mathcal{S}}
\def\ev{\mybold{e}}

\def\Lammat{\mybold{\Lambda}}


$$

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Multilinear regression as loss minimzation.</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><span class="math inline">\(\textcolor{white}{\LaTeX}\)</span></p>
<section id="goals" class="level1">
<h1>Goals</h1>
<ul>
<li>Derive the general form of the ordinary least squares (OLS) estimator in matrix notation
<ul>
<li>Review simple least squares derivation</li>
<li>Review matrix notation</li>
<li>Review vector calculus</li>
<li>Derive the general OLS formula and show that the simple least squares is a special case</li>
</ul></li>
</ul>
</section>
<section id="siomple-least-squares" class="level1">
<h1>Siomple least squares</h1>
<p>Recall the simple least squares model:</p>
<p><span id="eq-lm-simple"><span class="math display">\[
\begin{align*}
\y_n :={}&amp; \textrm{Response (e.g. body fat)} \\
\x_n :={}&amp; \textrm{Regressor (e.g. waist measurement)}\\
y_n ={}&amp; \beta_2 \x_n + \beta_1 + \res_n \textrm{ Model (straight line through data)}.
\end{align*}
\tag{1}\]</span></span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Here are some key quantities and their names:</p>
<ul>
<li><span class="math inline">\(\y_n\)</span>: The ‘response’</li>
<li><span class="math inline">\(\x_n\)</span>: The ‘regressors’ or ‘explanatory’ variables</li>
</ul>
<p>For a linear model, we also have:</p>
<ul>
<li><span class="math inline">\(\res_n\)</span>: The ‘error’ or ‘residual’</li>
<li><span class="math inline">\(\beta_2, \beta_1\)</span>: The ‘coefficients’, ‘parameters’, ‘slope and intercept’</li>
</ul>
<p>We might also have estimates of these quantities:</p>
<ul>
<li><span class="math inline">\(\bhat_p\)</span>: Estimate of <span class="math inline">\(\beta_p\)</span></li>
<li><span class="math inline">\(\reshat\)</span>: Estimate of <span class="math inline">\(\res_n\)</span></li>
<li><span class="math inline">\(\yhat_n\)</span>: A ‘prediction’ or ‘fitted value’ <span class="math inline">\(\yhat_n = \bhat_1 + \bhat_2 \x_n\)</span></li>
</ul>
<p>When we form the estimator by minimizing the estimated residuals, we might call the estimate</p>
<ul>
<li>‘Ordinary least squares’ (or ‘OLS’)</li>
<li>‘Least-squares’</li>
<li>‘Linear regression’</li>
</ul>
<p>An estimate will implicitly be least-squares estimates, but precisely what we mean by an estimate may have to come from context.</p>
</div>
</div>
<p>Note that for any value of <span class="math inline">\(\beta\)</span>, we get a value of the “error” or “residual” <span class="math inline">\(\res_n\)</span>:</p>
<p><span class="math display">\[
\res_n = \y_n - (\beta_2 \x_n + \beta_1).
\]</span></p>
<p>The “least squares fit” is called this because we choose <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> to make <span class="math inline">\(\sumn \res_n^2\)</span> as small as possible:</p>
<p><span class="math display">\[
\begin{align*}
\textrm{Choose }\bhat_2,\bhat_1\textrm{ so that }
\sumn \res_n^2 = \sumn \left(  \y_n - (\bhat_2 \x_n + \bhat_1) \right)^2
\textrm{ is as small as possible.}
\end{align*}
\]</span></p>
<p>How do we do this for the simple least squares model? And what if we have more regressors?</p>
</section>
<section id="siomple-least-squares-estimator-derivation" class="level1">
<h1>Siomple least squares estimator derivation</h1>
<p>The quantity we’re trying to minimize is smooth and convex, so if there is a minimum it would satisfy</p>
<p><span class="math display">\[
\begin{align*}
\fracat{\partial \sumn \res_n^2}{\partial \beta_1}{\bhat_1, \bhat_2} ={}&amp; 0 \quad\textrm{and} \\
\fracat{\partial \sumn \res_n^2}{\partial \beta_2}{\bhat_1, \bhat_2} ={}&amp; 0.
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-warning callout-titled" title="Question">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<p>When is it sufficient to set the gradient equal to zero to find a minumum?</p>
</div>
</div>
<p>These translate to (after dividing by <span class="math inline">\(-2 N\)</span>)</p>
<p><span class="math display">\[
\begin{align*}
\meann \y_n - \bhat_2 \meann \x_n - \bhat_1 ={}&amp; 0 \quad\textrm{and}\\
\meann \y_n \x_n - \bhat_2 \meann \x_n^2 - \bhat_1 \meann \x_n  ={}&amp; 0.
\end{align*}
\]</span></p>
<p>Let’s introduce the notation</p>
<p><span class="math display">\[
\begin{align*}
\overline{y} ={}&amp; \meann \y_n \\
\overline{xy} ={}&amp; \meann \x_n \y_n \\
\overline{xx} ={}&amp; \meann \x_n ^2,
\end{align*}
\]</span></p>
<p>Our estimator them must satisfy</p>
<p><span class="math display">\[
\begin{align*}
\overline{x} \bhat_2  + \bhat_1 ={}&amp; \overline{y} \quad\textrm{and}\\
\overline{xx} \bhat_2  + \overline{x} \bhat_1  ={}&amp; \overline{yx}.
\end{align*}
\]</span></p>
<p>We have a linear system with two unknowns and two equations. An elegant way to solve them is to subtract <span class="math inline">\(\overline{x}\)</span> times the first equation from the second, giving:</p>
<p><span class="math display">\[
\begin{align*}
\overline{x} \bhat_1 - \overline{x} \bhat_1 +
    \overline{xx} \bhat_2 - \overline{x}^2 \bhat_2 ={}&amp;
    \overline{xy} - \overline{x} \overline{y} \Leftrightarrow\\
\bhat_2 ={}&amp;
    \frac{\overline{xy} - \overline{x} \overline{y}}
         {\overline{xx}  - \overline{x}^2},
\end{align*}
\]</span></p>
<p>as long as <span class="math inline">\(\overline{xx} - \overline{x}^2 \ne 0\)</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled" title="Question">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<p>In ordinary language, what does it mean for <span class="math inline">\(\overline{xx} - \overline{x}^2 = 0\)</span>?</p>
</div>
</div>
<p>We can then plug this into the first equation giving</p>
<p><span class="math display">\[
\bhat_1 = \overline{y} - \bhat_2 \overline{x}.
\]</span></p>
</section>
<section id="matrix-multiplication-version" class="level1">
<h1>Matrix multiplication version</h1>
<p>Alternatively, our criterion can be written in matrix form as</p>
<p><span id="eq-simple-est-as-matrix"><span class="math display">\[
\begin{pmatrix}
1  &amp; \overline{x} \\
\overline{x} &amp; \overline{xx}
\end{pmatrix}
\begin{pmatrix}
\bhat_1 \\
\bhat_2
\end{pmatrix} =
\begin{pmatrix}
\overline{y} \\
\overline{xy}
\end{pmatrix}
\tag{2}\]</span></span></p>
<p>Recall that there is a special matrix that allows us to get an expression for <span class="math inline">\(\bhat_1\)</span> and <span class="math inline">\(\bhat_2\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
\begin{pmatrix}
1  &amp; \overline{x} \\
\overline{x} &amp; \overline{xx}
\end{pmatrix}^{-1} =
\frac{1}{\overline{xx} - \overline{x}^2}
\begin{pmatrix}
\overline{xx}  &amp; - \overline{x} \\
-\overline{x} &amp; 1
\end{pmatrix}
\end{align*}
\]</span></p>
<p>This matrix is called the “inverse” because</p>
<p><span class="math display">\[
\begin{align*}
\begin{pmatrix}
1  &amp; \overline{x} \\
\overline{x} &amp; \overline{xx}
\end{pmatrix}^{-1}
\begin{pmatrix}
1  &amp; \overline{x} \\
\overline{x} &amp; \overline{xx}
\end{pmatrix} =
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix}.
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-warning callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Verify the preceding property.</p>
</div>
</div>
<p>Multiplying both sides of <a href="#eq-simple-est-as-matrix">Equation&nbsp;2</a> by the matrix inverse gives</p>
<p><span class="math display">\[
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix}
\begin{pmatrix}
\bhat_1 \\
\bhat_2
\end{pmatrix} =
\begin{pmatrix}
\bhat_1 \\
\bhat_2
\end{pmatrix} =
\frac{1}{\overline{xx} - \overline{x}^2}
\begin{pmatrix}
\overline{xx}  &amp; - \overline{x} \\
-\overline{x} &amp; 1
\end{pmatrix}
\begin{pmatrix}
\overline{y} \\
\overline{xy}
\end{pmatrix}.
\]</span></p>
<p>From this we can read off the familiar answer</p>
<p><span class="math display">\[
\begin{align*}
\bhat_2 ={}&amp; \frac{\overline{xy} - \overline{x}\,\overline{y}}{\overline{xx} - \overline{x}^2}\\
\bhat_1 ={}&amp; \frac{\overline{xx}\,\overline{y} - \overline{xy}\,\overline{x}}{\overline{xx} - \overline{x}^2}\\
  ={}&amp; \frac{\overline{xx}\,\overline{y} -
      \overline{x}^2 \overline{y} + \overline{x}^2 \overline{y} - \overline{xy}\,\overline{x}}
    {\overline{xx} - \overline{x}^2}\\
  ={}&amp; \overline{y} - \frac{\overline{x}^2 \overline{y} - \overline{xy}\,\overline{x}}
    {\overline{xx} - \overline{x}^2} \\
  ={}&amp; \overline{y} - \bhat_1 \overline{x}.
\end{align*}
\]</span></p>
</section>
<section id="matrix-notation" class="level1">
<h1>Matrix notation</h1>
<p>The preceding formula came from combining the equations that set the univariate gradients equal to zero, and then recognizing a matrix equation. We can in fact do both at the same time! But first we need some notation</p>
<p>Here is a formal definition of the type of model that we will study for the vast majority of the semester:</p>
<p><span id="eq-lm-scalar"><span class="math display">\[
\begin{align*}
\y_n ={}&amp; \beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP} + \res_{n}, \quad\textrm{For }n=1,\ldots,N.
\end{align*}
\tag{3}\]</span></span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>I will always use <span class="math inline">\(N\)</span> for the number of observed data points, and <span class="math inline">\(P\)</span> for the dimension of the regression vector.</p>
</div>
</div>
<p><a href="#eq-lm-scalar">Equation&nbsp;3</a> is a general form of simpler cases. For example, if we take <span class="math inline">\(\x_{n1} \equiv 1\)</span>, <span class="math inline">\(\x_{n2}= \x_n\)</span> to be some scalar, and <span class="math inline">\(P = 2\)</span>, then <a href="#eq-lm-scalar">Equation&nbsp;3</a> becomes <a href="#eq-lm-simple">Equation&nbsp;1</a>:</p>
<p><span class="math display">\[
\begin{align*}
\y_n ={}&amp; \beta_1  + \beta_2 \x_{n} + \res_{n}, \quad\textrm{For }n=1,\ldots,N.
\end{align*}
\]</span></p>
<p>The residuals <span class="math inline">\(\res_n\)</span> measure the “misfit” of the line. If you know <span class="math inline">\(\beta_1, \ldots, \beta_P\)</span>, then you can compute</p>
<p><span class="math display">\[
\begin{align*}
\res_n ={}&amp; \y_n -  (\beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP}).
\end{align*}
\]</span></p>
<p>But in general we only observe <span class="math inline">\(\y_n\)</span> and <span class="math inline">\(\x_{n1}, \ldots, \x_{nP}\)</span>, and we choose <span class="math inline">\(\beta_1, \ldots, \beta_P\)</span> to make the residuals small. (How we do this precisely will be something we talk about at great length.)</p>
<p>The general form of <a href="#eq-lm-scalar">Equation&nbsp;3</a> can be written more compactly using matrix and vector notation. Specifically, if we let</p>
<p><span class="math display">\[
\begin{align*}
\xv_n :=
\begin{pmatrix}
  \x_{n1} \\ \x_{n2} \\ \vdots \\ \x_{nP}
\end{pmatrix}
\quad
\textrm{and}
\quad
\bv :=
\begin{pmatrix}
  \beta_{1} \\ \beta_2 \\ \vdots \\ \beta_{P}
\end{pmatrix}
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Bold lowercase variables are column vectors (unless otherwise specified).</p>
</div>
</div>
<p>Recall that the “transpose” operator <span class="math inline">\((\cdot)^\trans\)</span> flips the row and columns of a matrix. For example,</p>
<p><span class="math display">\[
\begin{align*}
\xv_n ^\trans =
\begin{pmatrix}
  \x_{n1} &amp; \x_{n2} &amp; \ldots &amp; \x_{nP}
\end{pmatrix}.
\end{align*}
\]</span></p>
<p>By matrix multiplication rules,</p>
<p><span class="math display">\[
\begin{align*}
\xv_n^\trans \bv =
\begin{pmatrix}
  \x_{n1} &amp; \x_{n2} &amp; \ldots &amp; \x_{nP}
\end{pmatrix}
\quad\quad\quad
\begin{pmatrix}
  \beta_{1} \\ \beta_2 \\ \vdots \\ \beta_{P}
\end{pmatrix}
= \beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP}.
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>I have written <span class="math inline">\(\xv_n^\trans \bv\)</span> for the “dot product” or “inner product” between <span class="math inline">\(\xv_n\)</span> and <span class="math inline">\(\bv\)</span>. Writing it in this way clarifies the relationship with matrix notation below.</p>
<p>There are many other ways to denote inner products in the literature, including <span class="math inline">\(\xv_n \cdot \bv\)</span> and <span class="math inline">\(&lt;\xv_n, \bv&gt;\)</span>.</p>
</div>
</div>
<p>Then we can compactly write</p>
<p><span class="math display">\[
\begin{align*}
\y_n ={}&amp; \xv_n ^\trans \bv + \res_{n}, \quad\textrm{For }n=1,\ldots,N.
\end{align*}
\]</span></p>
<p>We can compactify it even further if we stack the <span class="math inline">\(n\)</span> observations: % <span class="math display">\[
\begin{align*}
\y_1 ={}&amp; \xv_1 ^\trans \bv + \res_{1} \\
\y_2 ={}&amp; \xv_2 ^\trans \bv + \res_{2} \\
\vdots\\
\y_N ={}&amp; \xv_N ^\trans \bv + \res_{N} \\
\end{align*}
\]</span></p>
<p>As before we can stack the responses and residuals:</p>
<p><span class="math display">\[
\begin{align*}
\Y :=
\begin{pmatrix}
  \y_{1} \\ \y_2 \\ \vdots \\ \y_{P}
\end{pmatrix}
\quad
\textrm{and}
\quad
\resv :=
\begin{pmatrix}
  \res_{1} \\ \res_2 \\ \vdots \\ \res_{P}
\end{pmatrix}
\end{align*}
\]</span></p>
<p>We can also stack the regressors:</p>
<p><span class="math display">\[
\begin{align*}
\X :=
\begin{pmatrix}
  \x_{11} &amp; \x_{12} &amp; \ldots &amp; \x_{1P}\\
  \x_{21} &amp; \x_{22} &amp; \ldots &amp; \x_{2P}\\
  \vdots\\
  \x_{n1} &amp; \x_{n2} &amp; \ldots &amp; \x_{nP}\\
  \vdots\\
    \x_{N1} &amp; \x_{N2} &amp; \ldots &amp; \x_{NP}
\end{pmatrix}
=
\begin{pmatrix}
  \xv_{1}^\trans \\ \xv_{2}^\trans \\ \vdots \\ \xv_n^\trans \\ \vdots \\ \xv_{N}^\trans
\end{pmatrix}
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>I will use upper case bold letters for multi-dimensional matrices like <span class="math inline">\(\X\)</span>. But I may also use upper case bold letters even when the quantity could also be a column vector, when I think it’s more useful to think of the quantity as a matrix with a single column. Examples are <span class="math inline">\(\Y\)</span> above, or <span class="math inline">\(\X\)</span> when <span class="math inline">\(P = 1\)</span>.</p>
</div>
</div>
<p>Note that by matrix multiplication rules,</p>
<p><span class="math display">\[
\begin{align*}
\X  \bv =
\begin{pmatrix}
  \xv_{1}^\trans \\ \xv_{2}^\trans \\ \vdots \\ \xv_n^\trans \\ \vdots \\ \xv_{N}^\trans
\end{pmatrix}
\quad\quad\quad
\bv
=
\begin{pmatrix}
  \xv_{1}^\trans\bv \\ \xv_{2}^\trans\bv \\ \vdots \\ \xv_n^\trans\bv \\ \vdots \\ \xv_{N}^\trans\bv
\end{pmatrix}
\end{align*}
\]</span></p>
<p>so we end up with the extremely tidy expression</p>
<p><span id="eq-y-matrix"><span class="math display">\[
\begin{align*}
\y_n ={}&amp; \beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP} + \res_{n}, \quad\textrm{For }n=1,\ldots,N
\\\\&amp;\textrm{is the same as}\quad\\\\
\Y ={}&amp; \X \bv + \resv.
\end{align*}
\tag{4}\]</span></span></p>
<p>In the case of simple least squares, we can write</p>
<p><span id="eq-simple_matrix"><span class="math display">\[
\begin{align*}
\X :=
\begin{pmatrix}
  1 &amp; \x_{1}\\
  1 &amp; \x_{2}\\
  \vdots &amp; \vdots\\
  1 &amp; \x_{N}\\
\end{pmatrix},
\end{align*}
\tag{5}\]</span></span></p>
<p>and verify that the <span class="math inline">\(n\)</span>–th row of <a href="#eq-y-matrix">Equation&nbsp;4</a> is the same as <a href="#eq-lm-simple">Equation&nbsp;1</a>.</p>
</section>
<section id="least-squares-in-matrix-notation" class="level1">
<h1>Least squares in matrix notation</h1>
<p>Using our tidy expression <a href="#eq-y-matrix">Equation&nbsp;4</a>, we can easily write out the sum of the squared errors as</p>
<p><span class="math display">\[
\begin{align*}
\sumn \res_n^2 =
    \resv^\trans \resv = (\Y - \X \bv)^\trans (\Y - \X \bv).
\end{align*}
\]</span></p>
<p>This is a function of the vector <span class="math inline">\(\bv\)</span>. We wish to find the minimum of this quantity as a function of <span class="math inline">\(\bv\)</span>. My might hope that the minimum occurs at a point where the gradient of this expression is zero. Rather than compute the <em>univariate</em> derivative with respect to each component, we can compute the <em>multivariate gradient</em> with respect to the vector.</p>
<p>Let’s recall some facts from vector calculus.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Take <span class="math inline">\(\z \in \mathbb{R}^P\)</span> to be a <span class="math inline">\(P\)</span>–vector. and let <span class="math inline">\(\mybold{f}(\z) \in \mathbb{R}^Q\)</span> denote a <span class="math inline">\(Q\)</span>–vector. We write</p>
<p><span class="math display">\[
\frac{\partial \mybold{f}}{\partial \zv^\trans}  =
\begin{pmatrix}
\frac{\partial}{\partial \z_1} f_1(\z) &amp; \ldots &amp;
    \frac{\partial}{\partial \z_P} f_1(\zv) \\
    &amp; \vdots &amp; \\
\frac{\partial}{\partial \z_1} f_Q(\zv) &amp; \ldots &amp;
    \frac{\partial}{\partial \z_P} f_Q(\zv)
\end{pmatrix}.
\]</span></p>
<p>That is, the partial <span class="math inline">\(\partial \mybold{f} / \partial \zv^\trans\)</span> is a <span class="math inline">\(Q \times P\)</span> matrix with components of <span class="math inline">\(\mybold{f}\)</span> in the rows and components of the derivative in the columns. This matrix is called the “Jacobian matrix” of the function <span class="math inline">\(\mybold{f}(\zv)\)</span>.</p>
<p>Note that many authors omit the transpose in the denominator of the partial derivative, but I will try to do so to emphasize the dimension of the output.</p>
<p>I will also sometimes write</p>
<p><span class="math display">\[
\frac{\partial \mybold{f}^\trans}{\partial \zv}
=
\left(
    \frac{\partial \mybold{f}}{\partial \zv^\trans}
\right)^\trans.
\]</span></p>
<p>When <span class="math inline">\(Q = 1\)</span> and <span class="math inline">\(f\)</span> is a scalar, I will often write</p>
<p><span class="math display">\[
\frac{\partial f}{\partial \zv} =
\begin{pmatrix}
\frac{\partial}{\partial \zv_1} f(\zv) \\
    \vdots  \\
    \frac{\partial}{\partial \zv_P} f(\zv)
\end{pmatrix}.
\]</span></p>
</div>
</div>
<p>Recall a couple rules from vector calculus:</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial}{\partial \zv} \zv^\trans \zv = 2 \zv
\quad\textrm{and}\quad
\frac{\partial}{\partial \zv^\trans} \mybold{A} \zv = \mybold{A}
\quad\textrm{and}\quad
\frac{\partial}{\partial \zv} f(\mybold{g}(\zv)) =
    \frac{\partial \mybold{g}^\trans}{\partial \zv} \frac{\partial f}{\partial \mybold{g}}
     \quad
    \textrm{($f$ is scalar-valued and $\mybold{g}$ is vector-valued)}
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-warning callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Prove these results above using univariate derivatives and our stacking convention.</p>
</div>
</div>
<p>By the chain rule, we then get</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial}{\partial \bv} \resv^\trans \resv ={}&amp;
2 \frac{\partial \resv^\trans}{\partial \bv}  \resv  \\
={}&amp; 2 \frac{\partial (\Y - \X \bv)^\trans}{\partial \bv}  (\Y - \X \bv)  \\
={}&amp; -2 \X ^\trans (\Y - \X \bv) \\
={}&amp; -2 \X^\trans \Y + 2  \X^\trans \X \bv.
\end{align*}
\]</span></p>
<p>Assuming our estimator <span class="math inline">\(\bhat\)</span> sets these partial derivatives are equal to zero, we then get</p>
<p><span id="eq-ols-esteq"><span class="math display">\[
\begin{align*}
\X^\trans \X \bvhat ={}&amp; \X^\trans \Y.
\end{align*}
\tag{6}\]</span></span></p>
<p>This is a set of <span class="math inline">\(P\)</span> equations in <span class="math inline">\(P\)</span> unknowns. If it is not degenerate, one can solve for <span class="math inline">\(\bvhat\)</span>. That is, if the matrix <span class="math inline">\(\X^\trans \X\)</span> is invertible, then we can multiply both sides of <a href="#eq-ols-esteq">Equation&nbsp;6</a> by <span class="math inline">\((\X^\trans \X)^{-1}\)</span> to get</p>
<p><span class="math display">\[
\bvhat = (\X^\trans \X)^{-1} \X^\trans \Y
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>I will use <span class="math inline">\(\onev\)</span> to denote a vector full of ones. Usually it will be a <span class="math inline">\(P\)</span>–vector, but sometimes its dimension will just be implicit. Similarly, <span class="math inline">\(\zerov\)</span> is a vector of zeros.</p>
</div>
</div>
<p>Indeed, by plugging <a href="#eq-simple_matrix">Equation&nbsp;5</a> into <span class="citation" data-cites="ols-est-eq">@ols-est-eq</span>, we get</p>
<p><span class="math display">\[
\X^\trans \X =
\begin{pmatrix}
1 &amp; \ldots &amp; 1 \\
x_1 &amp; \ldots &amp; x_N \\
\end{pmatrix}
\begin{pmatrix}
  1 &amp; \x_{1}\\
  \vdots &amp; \vdots\\
  1 &amp; \x_{N}\\
\end{pmatrix} =
\begin{pmatrix}
\onev^\trans \onev  &amp; \onev^\trans \xv \\
\xv^\trans \onev  &amp; \xv^\trans \xv \\
\end{pmatrix}
=
N
\begin{pmatrix}
1  &amp; \overline{x} \\
\overline{x}  &amp; \overline{xx} \\
\end{pmatrix} \\
%
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\X^\trans \Y =
\begin{pmatrix}
1 &amp; \ldots &amp; 1 \\
x_1 &amp; \ldots &amp; x_N \\
\end{pmatrix}
\begin{pmatrix}
  \y_{1}\\
  \vdots\\
  \y_{N}\\
\end{pmatrix} =
N
\begin{pmatrix}
\overline{y} \\
\overline{xy}
\end{pmatrix}.
\]</span></p>
<p>Canceling <span class="math inline">\(N\)</span> shows that <a href="#eq-ols-esteq">Equation&nbsp;6</a> is the same as <a href="#eq-simple-est-as-matrix">Equation&nbsp;2</a>.</p>
</section>
<section id="what-if-the-matrix-is-not-invertible" class="level1">
<h1>What if the matrix is not invertible?</h1>
<p>I’ll end with a short note on what happens with simmple linear regression if <span class="math inline">\(\overline{xx} - \overline{x}^2 = 0\)</span>. Recall that if <span class="math inline">\(\overline{xx} - \overline{x}^2 = 0\)</span> then the matrix in <a href="#eq-ols-esteq">Equation&nbsp;6</a> is not invertible.</p>
<div class="callout callout-style-default callout-warning callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Prove that <span class="math inline">\(\overline{xx} - \overline{x}^2 = 0\)</span> means <span class="math inline">\(\x_n\)</span> is a constant. Hint: look at the sample variance of <span class="math inline">\(\x_n\)</span>.</p>
</div>
</div>
<p>For simplicity, let’s take <span class="math inline">\(\x_n = 1\)</span>. In that case we can rewrite our estimating equation as</p>
<p><span class="math display">\[
\y_n = \beta_1 + \beta_2 \x_n + \res_n
     = (\beta_1 + \beta_2) + \res_n.
\]</span></p>
<p>Note that there are an infinite number of combinations <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> that give exactly the same regression line. For example, if I take</p>
<p><span class="math display">\[
\beta_1' = \beta_1 - 5
\quad\textrm{and}\quad
\beta_2' = \beta_2 + 5
\]</span></p>
<p>then</p>
<p><span class="math display">\[
\y_n = (\beta_1 + \beta_2) + \res_n
     = (\beta_1' + 5 + \beta_2' - 5) + \res_n
     = (\beta_1' + \beta_2') + \res_n,
\]</span></p>
<p>even though <span class="math inline">\(\beta_1 \ne \beta_1'\)</span> and <span class="math inline">\(\beta_2 \ne \beta_2'\)</span>. We cannot possibly hope to find a unique least squares solution — even though the expressivity of the line we are able to fit is unchanged.</p>
<p>In that case,</p>
<p><span class="math display">\[
\begin{align*}
\overline{x} ={}&amp; \meann 1 = 1 \\
\overline{xx} ={}&amp; \meann 1^2 = 1 \\
\overline{xy} ={}&amp; \meann 1 \y_n = \overline{y},
\end{align*}
\]</span></p>
<p>and we see that our system of equations is</p>
<p><span class="math display">\[
\begin{pmatrix}
1  &amp; \overline{x} \\
\overline{x} &amp; \overline{xx}
\end{pmatrix}
\begin{pmatrix}
\beta_1 \\ \beta_2
\end{pmatrix}
=
\begin{pmatrix}
1  &amp; 1 \\
1 &amp;  1
\end{pmatrix}
\begin{pmatrix}
\beta_1 \\ \beta_2
\end{pmatrix}
=
\begin{pmatrix}
\overline{y} \\ \overline{y}
\end{pmatrix}
\]</span></p>
<p>The degeneracy we noticed before manifests in linear algebra form by noticing that</p>
<p><span class="math display">\[
\begin{pmatrix}
1  &amp; 1 \\
1 &amp;  1
\end{pmatrix}
\begin{pmatrix}
1 \\ -1
\end{pmatrix}
=
\begin{pmatrix}
1  -1 \\ 1 - 1
\end{pmatrix}
=
\begin{pmatrix}
0 \\ 0
\end{pmatrix}
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
\begin{pmatrix}
1  &amp; 1 \\
1  &amp; 1 \\
\end{pmatrix}
\begin{pmatrix}
\beta_1 + 5\\ \beta_2 - 5
\end{pmatrix}
=
\begin{pmatrix}
1  &amp; 1 \\
1  &amp; 1 \\
\end{pmatrix}
\begin{pmatrix}
\beta_1\\ \beta_2
\end{pmatrix}
+ 5
\begin{pmatrix}
1  &amp; 1 \\
1  &amp; 1 \\
\end{pmatrix}
\begin{pmatrix}
1\\ -1
\end{pmatrix}
=
\begin{pmatrix}
1  &amp; 1 \\
1  &amp; 1 \\
\end{pmatrix}
\begin{pmatrix}
\beta_1\\ \beta_2
\end{pmatrix}.
\]</span></p>
<p>In other words, the fact that the matrix <span class="math inline">\(\X^\trans \X\)</span> maps a non-zero vector to zero results in the non-existence of the OLS solution. This will turn out to be very general, and only one of the ways in which the structure of <span class="math inline">\(\X^\trans \X\)</span> will reveal a lot about the behavior of the OLS estimate and fit.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Multilinear regression as loss minimzation."</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    include-before-body:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">     - file: ../macros.md</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>$\textcolor{white}{\LaTeX}$ </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu"># Goals</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Derive the general form of the ordinary least squares (OLS) estimator in matrix notation</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Review simple least squares derivation</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Review matrix notation</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Review vector calculus</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Derive the general OLS formula and show that the simple least squares is a special case</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu"># Siomple least squares </span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>Recall the simple least squares model:</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>\y_n :={}&amp; \textrm{Response (e.g. body fat)} <span class="sc">\\</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>\x_n :={}&amp; \textrm{Regressor (e.g. waist measurement)}<span class="sc">\\</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>y_n ={}&amp; \beta_2 \x_n + \beta_1 + \res_n \textrm{ Model (straight line through data)}.</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>$${#eq-lm-simple}</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'} </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>Here are some key quantities and their names:</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\y_n$: The 'response'</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\x_n$: The 'regressors' or 'explanatory' variables</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>For a linear model, we also have:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\res_n$: The 'error' or 'residual'</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\beta_2, \beta_1$: The 'coefficients', 'parameters', 'slope and intercept'</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>We might also have estimates of these quantities:</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\bhat_p$: Estimate of $\beta_p$</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\reshat$: Estimate of $\res_n$</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\yhat_n$: A 'prediction' or 'fitted value'</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>     $\yhat_n = \bhat_1 + \bhat_2 \x_n$</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>When we form the estimator by minimizing the estimated residuals, we might call the estimate</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>'Ordinary least squares' (or 'OLS')</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>'Least-squares'</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>'Linear regression'</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>An estimate will implicitly be least-squares estimates, but precisely what we mean by an estimate may have to come from context.</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>Note that for any value of $\beta$, we get a value of the "error"</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>or "residual" $\res_n$:</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>\res_n = \y_n - (\beta_2 \x_n + \beta_1).</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>The "least squares fit" is called this because we choose $\beta_1$ and $\beta_2$</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>to make $\sumn \res_n^2$ as small as possible:</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>\textrm{Choose }\bhat_2,\bhat_1\textrm{ so that }</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>\sumn \res_n^2 = \sumn \left(  \y_n - (\bhat_2 \x_n + \bhat_1) \right)^2</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>\textrm{ is as small as possible.}</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>How do we do this for the simple least squares model?  And what</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>if we have more regressors?</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="fu"># Siomple least squares estimator derivation</span></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>The quantity we're trying to minimize is smooth and convex, so if there</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>is a minimum it would satisfy</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>\fracat{\partial \sumn \res_n^2}{\partial \beta_1}{\bhat_1, \bhat_2} ={}&amp; 0 \quad\textrm{and} <span class="sc">\\</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>\fracat{\partial \sumn \res_n^2}{\partial \beta_2}{\bhat_1, \bhat_2} ={}&amp; 0.</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Question'} </span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>When is it sufficient to set the gradient equal to zero to find a minumum?</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>These translate to (after dividing by $-2 N$)</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>\meann \y_n - \bhat_2 \meann \x_n - \bhat_1 ={}&amp; 0 \quad\textrm{and}<span class="sc">\\</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>\meann \y_n \x_n - \bhat_2 \meann \x_n^2 - \bhat_1 \meann \x_n  ={}&amp; 0.</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>Let's introduce the notation</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>\overline{y} ={}&amp; \meann \y_n <span class="sc">\\</span></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>\overline{xy} ={}&amp; \meann \x_n \y_n <span class="sc">\\</span></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>\overline{xx} ={}&amp; \meann \x_n ^2,</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>Our estimator them must satisfy</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a> \overline{x} \bhat_2  + \bhat_1 ={}&amp; \overline{y} \quad\textrm{and}<span class="sc">\\</span></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a> \overline{xx} \bhat_2  + \overline{x} \bhat_1  ={}&amp; \overline{yx}.</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>We have a linear system with two unknowns and two equations.  An elegant way to solve them is to subtract $\overline{x}$ times the first equation from the second, giving:</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>\overline{x} \bhat_1 - \overline{x} \bhat_1 + </span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>    \overline{xx} \bhat_2 - \overline{x}^2 \bhat_2 ={}&amp;</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>    \overline{xy} - \overline{x} \overline{y} \Leftrightarrow<span class="sc">\\</span></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>\bhat_2 ={}&amp;</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>    \frac{\overline{xy} - \overline{x} \overline{y}}</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>         {\overline{xx}  - \overline{x}^2},</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>as long as $\overline{xx} - \overline{x}^2 \ne 0$.  </span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Question'} </span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>In ordinary language, what does it mean for $\overline{xx} - \overline{x}^2 = 0$?</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>We can then plug this into the first equation giving</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>\bhat_1 = \overline{y} - \bhat_2 \overline{x}.</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a><span class="fu"># Matrix multiplication version</span></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>Alternatively, our criterion can be written in matrix form as</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>1  &amp; \overline{x} <span class="sc">\\</span></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>\overline{x} &amp; \overline{xx} </span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>\bhat_1 <span class="sc">\\</span></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>\bhat_2</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>\overline{y} <span class="sc">\\</span></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>\overline{xy}</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>$${#eq-simple-est-as-matrix}</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>Recall that there is a special matrix that allows us to get an</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>expression for $\bhat_1$ and $\bhat_2$:</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>1  &amp; \overline{x} <span class="sc">\\</span></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>\overline{x} &amp; \overline{xx} </span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}^{-1} =</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>\frac{1}{\overline{xx} - \overline{x}^2}</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>\overline{xx}  &amp; - \overline{x} <span class="sc">\\</span></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>-\overline{x} &amp; 1 </span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>This matrix is called the "inverse" because</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>1  &amp; \overline{x} <span class="sc">\\</span></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>\overline{x} &amp; \overline{xx} </span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}^{-1}</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>1  &amp; \overline{x} <span class="sc">\\</span></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>\overline{x} &amp; \overline{xx} </span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>0 &amp; 1</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Exercise'} </span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>Verify the preceding property.</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>Multiplying both sides of @eq-simple-est-as-matrix by</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>the matrix inverse gives</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>0 &amp; 1</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>\bhat_1 <span class="sc">\\</span></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>\bhat_2</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>\bhat_1 <span class="sc">\\</span></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>\bhat_2</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>\frac{1}{\overline{xx} - \overline{x}^2}</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>\overline{xx}  &amp; - \overline{x} <span class="sc">\\</span></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>-\overline{x} &amp; 1 </span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>\overline{y} <span class="sc">\\</span></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>\overline{xy}</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>From this we can read off the familiar answer</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>\bhat_2 ={}&amp; \frac{\overline{xy} - \overline{x}\,\overline{y}}{\overline{xx} - \overline{x}^2}<span class="sc">\\</span></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>\bhat_1 ={}&amp; \frac{\overline{xx}\,\overline{y} - \overline{xy}\,\overline{x}}{\overline{xx} - \overline{x}^2}<span class="sc">\\</span></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>  ={}&amp; \frac{\overline{xx}\,\overline{y} - </span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>      \overline{x}^2 \overline{y} + \overline{x}^2 \overline{y} - \overline{xy}\,\overline{x}}</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>    {\overline{xx} - \overline{x}^2}<span class="sc">\\</span></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>  ={}&amp; \overline{y} - \frac{\overline{x}^2 \overline{y} - \overline{xy}\,\overline{x}}</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>    {\overline{xx} - \overline{x}^2} <span class="sc">\\</span></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>  ={}&amp; \overline{y} - \bhat_1 \overline{x}.</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a><span class="fu"># Matrix notation</span></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>The preceding formula came from combining the equations that set the univariate</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>gradients equal to zero, and then recognizing a matrix equation.  We can in</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>fact do both at the same time!  But first we need some notation</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>Here is a formal definition of the type of model that we will study for the vast</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>majority of the semester:</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>\y_n ={}&amp; \beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP} + \res_{n}, \quad\textrm{For }n=1,\ldots,N.</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>$${#eq-lm-scalar}</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>I will always use $N$ for the number of observed data points, and $P$ </span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>for the dimension of the regression vector.</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>@eq-lm-scalar is a general form of simpler cases.  For example,</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>if we take $\x_{n1} \equiv 1$, $\x_{n2}= \x_n$ to be some scalar, and $P = 2$, then</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>@eq-lm-scalar becomes @eq-lm-simple:</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>\y_n ={}&amp; \beta_1  + \beta_2 \x_{n} + \res_{n}, \quad\textrm{For }n=1,\ldots,N.</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>The residuals $\res_n$ measure the "misfit" of the line.  If you know $\beta_1, \ldots, \beta_P$,</span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>then you can compute</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>\res_n ={}&amp; \y_n -  (\beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP}).</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>But in general we only observe $\y_n$ and $\x_{n1}, \ldots, \x_{nP}$, and we</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>choose $\beta_1, \ldots, \beta_P$ to make the residuals small.  (How we do</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>this precisely will be something we talk about at great length.)</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>The general form of @eq-lm-scalar can be written more compactly using</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>matrix and vector notation. Specifically, if we let</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>\xv_n := </span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>  \x_{n1} <span class="sc">\\</span> \x_{n2} <span class="sc">\\</span> \vdots <span class="sc">\\</span> \x_{nP}</span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>\textrm{and}</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>\bv := </span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>  \beta_{1} <span class="sc">\\</span> \beta_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \beta_{P}</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a>Bold lowercase variables are column vectors (unless otherwise specified).</span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>Recall that the "transpose" operator $(\cdot)^\trans$ flips the row and</span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a>columns of a matrix.  For example,</span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>\xv_n ^\trans = </span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>  \x_{n1} &amp; \x_{n2} &amp; \ldots &amp; \x_{nP}</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a>By matrix multiplication rules,</span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a>\xv_n^\trans \bv = </span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>  \x_{n1} &amp; \x_{n2} &amp; \ldots &amp; \x_{nP}</span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a>\quad\quad\quad</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>  \beta_{1} <span class="sc">\\</span> \beta_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \beta_{P}</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>= \beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP}.</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a>I have written $\xv_n^\trans \bv$ for the "dot product" or "inner product"</span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a>between $\xv_n$ and $\bv$.  Writing it in this way clarifies the relationship</span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a>with matrix notation below.</span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a>There are many other ways to denote inner products in the literature,</span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a>including $\xv_n \cdot \bv$ and $&lt;\xv_n, \bv&gt;$.</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a>Then we can compactly write</span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a>\y_n ={}&amp; \xv_n ^\trans \bv + \res_{n}, \quad\textrm{For }n=1,\ldots,N.</span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a>We can compactify it even further if we stack the $n$ observations:</span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a>%</span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a>\y_1 ={}&amp; \xv_1 ^\trans \bv + \res_{1} <span class="sc">\\</span></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a>\y_2 ={}&amp; \xv_2 ^\trans \bv + \res_{2} <span class="sc">\\</span></span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a>\y_N ={}&amp; \xv_N ^\trans \bv + \res_{N} <span class="sc">\\</span></span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a>As before we can stack the responses and residuals:</span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a>\Y := </span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a>  \y_{1} <span class="sc">\\</span> \y_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \y_{P}</span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a>\textrm{and}</span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a>\resv := </span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a>  \res_{1} <span class="sc">\\</span> \res_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \res_{P}</span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a>We can also stack the regressors:</span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a>\X := </span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a>  \x_{11} &amp; \x_{12} &amp; \ldots &amp; \x_{1P}<span class="sc">\\</span></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a>  \x_{21} &amp; \x_{22} &amp; \ldots &amp; \x_{2P}<span class="sc">\\</span></span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a>  \vdots<span class="sc">\\</span></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a>  \x_{n1} &amp; \x_{n2} &amp; \ldots &amp; \x_{nP}<span class="sc">\\</span></span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a>  \vdots<span class="sc">\\</span></span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a>    \x_{N1} &amp; \x_{N2} &amp; \ldots &amp; \x_{NP}</span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a>  \xv_{1}^\trans <span class="sc">\\</span> \xv_{2}^\trans <span class="sc">\\</span> \vdots <span class="sc">\\</span> \xv_n^\trans <span class="sc">\\</span> \vdots <span class="sc">\\</span> \xv_{N}^\trans</span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a>I will use upper case bold letters for multi-dimensional matrices like $\X$.  But</span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a>I may also use upper case bold letters even when the quantity could also be</span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a>a column vector, when I think it's more useful to think of the quantity</span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a>as a matrix with a single column.  Examples are $\Y$ above, or $\X$ when $P = 1$.</span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a>Note that by matrix multiplication rules,</span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a>\X  \bv = </span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a>  \xv_{1}^\trans <span class="sc">\\</span> \xv_{2}^\trans <span class="sc">\\</span> \vdots <span class="sc">\\</span> \xv_n^\trans <span class="sc">\\</span> \vdots <span class="sc">\\</span> \xv_{N}^\trans</span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a>\quad\quad\quad</span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a>\bv</span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a>  \xv_{1}^\trans\bv <span class="sc">\\</span> \xv_{2}^\trans\bv <span class="sc">\\</span> \vdots <span class="sc">\\</span> \xv_n^\trans\bv <span class="sc">\\</span> \vdots <span class="sc">\\</span> \xv_{N}^\trans\bv</span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a>so we end up with the extremely tidy expression</span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a>\y_n ={}&amp; \beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP} + \res_{n}, \quad\textrm{For }n=1,\ldots,N</span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a><span class="sc">\\\\</span>&amp;\textrm{is the same as}\quad<span class="sc">\\\\</span></span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a>\Y ={}&amp; \X \bv + \resv.</span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a>$${#eq-y-matrix}</span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a>In the case of simple least squares, we can write</span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a>\X := </span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a>  1 &amp; \x_{1}<span class="sc">\\</span></span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a>  1 &amp; \x_{2}<span class="sc">\\</span></span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a>  \vdots &amp; \vdots<span class="sc">\\</span></span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a>  1 &amp; \x_{N}<span class="sc">\\</span></span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a>\end{pmatrix},</span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a>$${#eq-simple_matrix}</span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a>and verify that the $n$--th row of @eq-y-matrix is the same as @eq-lm-simple.</span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a><span class="fu"># Least squares in matrix notation</span></span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a>Using our tidy expression @eq-y-matrix, we can easily write out the</span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a>sum of the squared errors as</span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a>\sumn \res_n^2 = </span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a>    \resv^\trans \resv = (\Y - \X \bv)^\trans (\Y - \X \bv).</span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a>This is a function of the vector $\bv$.  We wish to find the minimum</span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a>of this quantity as a function of $\bv$.  My might hope that</span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a>the minimum occurs at a point where the gradient of this expression</span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a>is zero.</span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a>Rather than compute the</span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a>*univariate* derivative with respect to each component, we can compute</span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a>the *multivariate gradient* with respect to the vector.</span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a>Let's recall some facts from vector calculus.  </span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a>Take $\z \in \mathbb{R}^P$</span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a>to be a $P$--vector.</span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a>and let $\mybold{f}(\z) \in \mathbb{R}^Q$ denote a $Q$--vector.  We write</span>
<span id="cb1-510"><a href="#cb1-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-511"><a href="#cb1-511" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a>\frac{\partial \mybold{f}}{\partial \zv^\trans}  = </span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a>\frac{\partial}{\partial \z_1} f_1(\z) &amp; \ldots &amp; </span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a>    \frac{\partial}{\partial \z_P} f_1(\zv) <span class="sc">\\</span></span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a>    &amp; \vdots &amp; <span class="sc">\\</span></span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a>\frac{\partial}{\partial \z_1} f_Q(\zv) &amp; \ldots &amp; </span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a>    \frac{\partial}{\partial \z_P} f_Q(\zv)</span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a>That is, the partial $\partial \mybold{f} / \partial \zv^\trans$ is</span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a>a $Q \times P$ matrix with components of $\mybold{f}$ in the rows</span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a>and components of the derivative in the columns.  This matrix</span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a>is called the "Jacobian matrix" of the function $\mybold{f}(\zv)$.</span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a>Note that many authors omit the transpose in the denominator</span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a>of the partial derivative, but I will try to do so to emphasize</span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a>the dimension of the output.</span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a>I will also sometimes write</span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-534"><a href="#cb1-534" aria-hidden="true" tabindex="-1"></a>\frac{\partial \mybold{f}^\trans}{\partial \zv}</span>
<span id="cb1-535"><a href="#cb1-535" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a>\left( </span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a>    \frac{\partial \mybold{f}}{\partial \zv^\trans}</span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a>\right)^\trans.</span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a>When $Q = 1$ and $f$ is a scalar, I will often write</span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial \zv} = </span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a>\frac{\partial}{\partial \zv_1} f(\zv) <span class="sc">\\</span></span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a>    \vdots  <span class="sc">\\</span></span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a>    \frac{\partial}{\partial \zv_P} f(\zv)</span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-555"><a href="#cb1-555" aria-hidden="true" tabindex="-1"></a>Recall a couple rules from vector calculus:</span>
<span id="cb1-556"><a href="#cb1-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a>\frac{\partial}{\partial \zv} \zv^\trans \zv = 2 \zv</span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a>\quad\textrm{and}\quad</span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a>\frac{\partial}{\partial \zv^\trans} \mybold{A} \zv = \mybold{A}</span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a>\quad\textrm{and}\quad</span>
<span id="cb1-563"><a href="#cb1-563" aria-hidden="true" tabindex="-1"></a>\frac{\partial}{\partial \zv} f(\mybold{g}(\zv)) = </span>
<span id="cb1-564"><a href="#cb1-564" aria-hidden="true" tabindex="-1"></a>    \frac{\partial \mybold{g}^\trans}{\partial \zv} \frac{\partial f}{\partial \mybold{g}}</span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a>     \quad</span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a>    \textrm{($f$ is scalar-valued and $\mybold{g}$ is vector-valued)}</span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Exercise'} </span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a>Prove these results above using univariate derivatives and our stacking convention.</span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a>By the chain rule, we then get</span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-580"><a href="#cb1-580" aria-hidden="true" tabindex="-1"></a>\frac{\partial}{\partial \bv} \resv^\trans \resv ={}&amp;</span>
<span id="cb1-581"><a href="#cb1-581" aria-hidden="true" tabindex="-1"></a>2 \frac{\partial \resv^\trans}{\partial \bv}  \resv  <span class="sc">\\</span></span>
<span id="cb1-582"><a href="#cb1-582" aria-hidden="true" tabindex="-1"></a>={}&amp; 2 \frac{\partial (\Y - \X \bv)^\trans}{\partial \bv}  (\Y - \X \bv)  <span class="sc">\\</span></span>
<span id="cb1-583"><a href="#cb1-583" aria-hidden="true" tabindex="-1"></a>={}&amp; -2 \X ^\trans (\Y - \X \bv) <span class="sc">\\</span></span>
<span id="cb1-584"><a href="#cb1-584" aria-hidden="true" tabindex="-1"></a>={}&amp; -2 \X^\trans \Y + 2  \X^\trans \X \bv.</span>
<span id="cb1-585"><a href="#cb1-585" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-586"><a href="#cb1-586" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-587"><a href="#cb1-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-588"><a href="#cb1-588" aria-hidden="true" tabindex="-1"></a>Assuming our estimator $\bhat$ sets these partial derivatives are equal to zero, we then</span>
<span id="cb1-589"><a href="#cb1-589" aria-hidden="true" tabindex="-1"></a>get</span>
<span id="cb1-590"><a href="#cb1-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-591"><a href="#cb1-591" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-592"><a href="#cb1-592" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-593"><a href="#cb1-593" aria-hidden="true" tabindex="-1"></a>\X^\trans \X \bvhat ={}&amp; \X^\trans \Y.</span>
<span id="cb1-594"><a href="#cb1-594" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-595"><a href="#cb1-595" aria-hidden="true" tabindex="-1"></a>$${#eq-ols-esteq}</span>
<span id="cb1-596"><a href="#cb1-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-597"><a href="#cb1-597" aria-hidden="true" tabindex="-1"></a>This is a set of $P$ equations in $P$ unknowns.  If it is not degenerate,</span>
<span id="cb1-598"><a href="#cb1-598" aria-hidden="true" tabindex="-1"></a>one can solve for $\bvhat$.  That is, if the matrix $\X^\trans \X$ is invertible, then we</span>
<span id="cb1-599"><a href="#cb1-599" aria-hidden="true" tabindex="-1"></a>can multiply both sides of @eq-ols-esteq by $(\X^\trans \X)^{-1}$</span>
<span id="cb1-600"><a href="#cb1-600" aria-hidden="true" tabindex="-1"></a>to get</span>
<span id="cb1-601"><a href="#cb1-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-602"><a href="#cb1-602" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-603"><a href="#cb1-603" aria-hidden="true" tabindex="-1"></a>\bvhat = (\X^\trans \X)^{-1} \X^\trans \Y</span>
<span id="cb1-604"><a href="#cb1-604" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-605"><a href="#cb1-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-606"><a href="#cb1-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-607"><a href="#cb1-607" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-608"><a href="#cb1-608" aria-hidden="true" tabindex="-1"></a>I will use $\onev$ to denote a vector full of ones.  Usually it</span>
<span id="cb1-609"><a href="#cb1-609" aria-hidden="true" tabindex="-1"></a>will be a $P$--vector, but sometimes its dimension will just be</span>
<span id="cb1-610"><a href="#cb1-610" aria-hidden="true" tabindex="-1"></a>implicit.  Similarly, $\zerov$ is a vector of zeros.</span>
<span id="cb1-611"><a href="#cb1-611" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-612"><a href="#cb1-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-613"><a href="#cb1-613" aria-hidden="true" tabindex="-1"></a>Indeed, by plugging @eq-simple_matrix into @ols-est-eq, we get</span>
<span id="cb1-614"><a href="#cb1-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-615"><a href="#cb1-615" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-616"><a href="#cb1-616" aria-hidden="true" tabindex="-1"></a>\X^\trans \X =</span>
<span id="cb1-617"><a href="#cb1-617" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-618"><a href="#cb1-618" aria-hidden="true" tabindex="-1"></a>1 &amp; \ldots &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-619"><a href="#cb1-619" aria-hidden="true" tabindex="-1"></a>x_1 &amp; \ldots &amp; x_N <span class="sc">\\</span></span>
<span id="cb1-620"><a href="#cb1-620" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-621"><a href="#cb1-621" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-622"><a href="#cb1-622" aria-hidden="true" tabindex="-1"></a>  1 &amp; \x_{1}<span class="sc">\\</span></span>
<span id="cb1-623"><a href="#cb1-623" aria-hidden="true" tabindex="-1"></a>  \vdots &amp; \vdots<span class="sc">\\</span></span>
<span id="cb1-624"><a href="#cb1-624" aria-hidden="true" tabindex="-1"></a>  1 &amp; \x_{N}<span class="sc">\\</span></span>
<span id="cb1-625"><a href="#cb1-625" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =</span>
<span id="cb1-626"><a href="#cb1-626" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-627"><a href="#cb1-627" aria-hidden="true" tabindex="-1"></a>\onev^\trans \onev  &amp; \onev^\trans \xv <span class="sc">\\</span></span>
<span id="cb1-628"><a href="#cb1-628" aria-hidden="true" tabindex="-1"></a>\xv^\trans \onev  &amp; \xv^\trans \xv <span class="sc">\\</span></span>
<span id="cb1-629"><a href="#cb1-629" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-630"><a href="#cb1-630" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-631"><a href="#cb1-631" aria-hidden="true" tabindex="-1"></a>N</span>
<span id="cb1-632"><a href="#cb1-632" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-633"><a href="#cb1-633" aria-hidden="true" tabindex="-1"></a>1  &amp; \overline{x} <span class="sc">\\</span></span>
<span id="cb1-634"><a href="#cb1-634" aria-hidden="true" tabindex="-1"></a>\overline{x}  &amp; \overline{xx} <span class="sc">\\</span></span>
<span id="cb1-635"><a href="#cb1-635" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} <span class="sc">\\</span></span>
<span id="cb1-636"><a href="#cb1-636" aria-hidden="true" tabindex="-1"></a>%</span>
<span id="cb1-637"><a href="#cb1-637" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-638"><a href="#cb1-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-639"><a href="#cb1-639" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb1-640"><a href="#cb1-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-641"><a href="#cb1-641" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-642"><a href="#cb1-642" aria-hidden="true" tabindex="-1"></a>\X^\trans \Y =</span>
<span id="cb1-643"><a href="#cb1-643" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-644"><a href="#cb1-644" aria-hidden="true" tabindex="-1"></a>1 &amp; \ldots &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-645"><a href="#cb1-645" aria-hidden="true" tabindex="-1"></a>x_1 &amp; \ldots &amp; x_N <span class="sc">\\</span></span>
<span id="cb1-646"><a href="#cb1-646" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-647"><a href="#cb1-647" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-648"><a href="#cb1-648" aria-hidden="true" tabindex="-1"></a>  \y_{1}<span class="sc">\\</span></span>
<span id="cb1-649"><a href="#cb1-649" aria-hidden="true" tabindex="-1"></a>  \vdots<span class="sc">\\</span></span>
<span id="cb1-650"><a href="#cb1-650" aria-hidden="true" tabindex="-1"></a>  \y_{N}<span class="sc">\\</span></span>
<span id="cb1-651"><a href="#cb1-651" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =</span>
<span id="cb1-652"><a href="#cb1-652" aria-hidden="true" tabindex="-1"></a>N</span>
<span id="cb1-653"><a href="#cb1-653" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-654"><a href="#cb1-654" aria-hidden="true" tabindex="-1"></a>\overline{y} <span class="sc">\\</span></span>
<span id="cb1-655"><a href="#cb1-655" aria-hidden="true" tabindex="-1"></a>\overline{xy}</span>
<span id="cb1-656"><a href="#cb1-656" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-657"><a href="#cb1-657" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-658"><a href="#cb1-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-659"><a href="#cb1-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-660"><a href="#cb1-660" aria-hidden="true" tabindex="-1"></a>Canceling $N$ shows that @eq-ols-esteq is the same as @eq-simple-est-as-matrix.</span>
<span id="cb1-661"><a href="#cb1-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-662"><a href="#cb1-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-663"><a href="#cb1-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-664"><a href="#cb1-664" aria-hidden="true" tabindex="-1"></a><span class="fu"># What if the matrix is not invertible?</span></span>
<span id="cb1-665"><a href="#cb1-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-666"><a href="#cb1-666" aria-hidden="true" tabindex="-1"></a>I'll end with a short note on what happens with simmple</span>
<span id="cb1-667"><a href="#cb1-667" aria-hidden="true" tabindex="-1"></a>linear regression if $\overline{xx} - \overline{x}^2 = 0$.</span>
<span id="cb1-668"><a href="#cb1-668" aria-hidden="true" tabindex="-1"></a>Recall that if $\overline{xx} - \overline{x}^2 = 0$</span>
<span id="cb1-669"><a href="#cb1-669" aria-hidden="true" tabindex="-1"></a>then the matrix in @eq-ols-esteq is not invertible.</span>
<span id="cb1-670"><a href="#cb1-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-671"><a href="#cb1-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-672"><a href="#cb1-672" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Exercise'} </span>
<span id="cb1-673"><a href="#cb1-673" aria-hidden="true" tabindex="-1"></a>Prove that $\overline{xx} - \overline{x}^2 = 0$ </span>
<span id="cb1-674"><a href="#cb1-674" aria-hidden="true" tabindex="-1"></a>means $\x_n$ is a constant.  Hint: look at the sample</span>
<span id="cb1-675"><a href="#cb1-675" aria-hidden="true" tabindex="-1"></a>variance of $\x_n$.</span>
<span id="cb1-676"><a href="#cb1-676" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-677"><a href="#cb1-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-678"><a href="#cb1-678" aria-hidden="true" tabindex="-1"></a>For simplicity, let's take $\x_n = 1$.  In that case</span>
<span id="cb1-679"><a href="#cb1-679" aria-hidden="true" tabindex="-1"></a>we can rewrite our estimating equation as</span>
<span id="cb1-680"><a href="#cb1-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-681"><a href="#cb1-681" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-682"><a href="#cb1-682" aria-hidden="true" tabindex="-1"></a>\y_n = \beta_1 + \beta_2 \x_n + \res_n </span>
<span id="cb1-683"><a href="#cb1-683" aria-hidden="true" tabindex="-1"></a>     = (\beta_1 + \beta_2) + \res_n.</span>
<span id="cb1-684"><a href="#cb1-684" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-685"><a href="#cb1-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-686"><a href="#cb1-686" aria-hidden="true" tabindex="-1"></a>Note that there are an infinite number of combinations</span>
<span id="cb1-687"><a href="#cb1-687" aria-hidden="true" tabindex="-1"></a>$\beta_1$ and $\beta_2$ that give exactly the same regression</span>
<span id="cb1-688"><a href="#cb1-688" aria-hidden="true" tabindex="-1"></a>line.  For example, if I take</span>
<span id="cb1-689"><a href="#cb1-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-690"><a href="#cb1-690" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-691"><a href="#cb1-691" aria-hidden="true" tabindex="-1"></a>\beta_1' = \beta_1 - 5 </span>
<span id="cb1-692"><a href="#cb1-692" aria-hidden="true" tabindex="-1"></a>\quad\textrm{and}\quad</span>
<span id="cb1-693"><a href="#cb1-693" aria-hidden="true" tabindex="-1"></a>\beta_2' = \beta_2 + 5 </span>
<span id="cb1-694"><a href="#cb1-694" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-695"><a href="#cb1-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-696"><a href="#cb1-696" aria-hidden="true" tabindex="-1"></a>then</span>
<span id="cb1-697"><a href="#cb1-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-698"><a href="#cb1-698" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-699"><a href="#cb1-699" aria-hidden="true" tabindex="-1"></a>\y_n = (\beta_1 + \beta_2) + \res_n</span>
<span id="cb1-700"><a href="#cb1-700" aria-hidden="true" tabindex="-1"></a>     = (\beta_1' + 5 + \beta_2' - 5) + \res_n</span>
<span id="cb1-701"><a href="#cb1-701" aria-hidden="true" tabindex="-1"></a>     = (\beta_1' + \beta_2') + \res_n,</span>
<span id="cb1-702"><a href="#cb1-702" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-703"><a href="#cb1-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-704"><a href="#cb1-704" aria-hidden="true" tabindex="-1"></a>even though $\beta_1 \ne \beta_1'$ and $\beta_2 \ne \beta_2'$.</span>
<span id="cb1-705"><a href="#cb1-705" aria-hidden="true" tabindex="-1"></a>We cannot possibly hope to find a unique least squares</span>
<span id="cb1-706"><a href="#cb1-706" aria-hidden="true" tabindex="-1"></a>solution --- even though the expressivity of the line</span>
<span id="cb1-707"><a href="#cb1-707" aria-hidden="true" tabindex="-1"></a>we are able to fit is unchanged.</span>
<span id="cb1-708"><a href="#cb1-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-709"><a href="#cb1-709" aria-hidden="true" tabindex="-1"></a>In that case,</span>
<span id="cb1-710"><a href="#cb1-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-711"><a href="#cb1-711" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-712"><a href="#cb1-712" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-713"><a href="#cb1-713" aria-hidden="true" tabindex="-1"></a>\overline{x} ={}&amp; \meann 1 = 1 <span class="sc">\\</span></span>
<span id="cb1-714"><a href="#cb1-714" aria-hidden="true" tabindex="-1"></a>\overline{xx} ={}&amp; \meann 1^2 = 1 <span class="sc">\\</span></span>
<span id="cb1-715"><a href="#cb1-715" aria-hidden="true" tabindex="-1"></a>\overline{xy} ={}&amp; \meann 1 \y_n = \overline{y},</span>
<span id="cb1-716"><a href="#cb1-716" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-717"><a href="#cb1-717" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-718"><a href="#cb1-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-719"><a href="#cb1-719" aria-hidden="true" tabindex="-1"></a>and we see that our system of equations is</span>
<span id="cb1-720"><a href="#cb1-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-721"><a href="#cb1-721" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-722"><a href="#cb1-722" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-723"><a href="#cb1-723" aria-hidden="true" tabindex="-1"></a>1  &amp; \overline{x} <span class="sc">\\</span></span>
<span id="cb1-724"><a href="#cb1-724" aria-hidden="true" tabindex="-1"></a>\overline{x} &amp; \overline{xx} </span>
<span id="cb1-725"><a href="#cb1-725" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} </span>
<span id="cb1-726"><a href="#cb1-726" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-727"><a href="#cb1-727" aria-hidden="true" tabindex="-1"></a>\beta_1 <span class="sc">\\</span> \beta_2</span>
<span id="cb1-728"><a href="#cb1-728" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-729"><a href="#cb1-729" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-730"><a href="#cb1-730" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-731"><a href="#cb1-731" aria-hidden="true" tabindex="-1"></a>1  &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-732"><a href="#cb1-732" aria-hidden="true" tabindex="-1"></a>1 &amp;  1 </span>
<span id="cb1-733"><a href="#cb1-733" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-734"><a href="#cb1-734" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-735"><a href="#cb1-735" aria-hidden="true" tabindex="-1"></a>\beta_1 <span class="sc">\\</span> \beta_2</span>
<span id="cb1-736"><a href="#cb1-736" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-737"><a href="#cb1-737" aria-hidden="true" tabindex="-1"></a> = </span>
<span id="cb1-738"><a href="#cb1-738" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-739"><a href="#cb1-739" aria-hidden="true" tabindex="-1"></a>\overline{y} <span class="sc">\\</span> \overline{y}</span>
<span id="cb1-740"><a href="#cb1-740" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-741"><a href="#cb1-741" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-742"><a href="#cb1-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-743"><a href="#cb1-743" aria-hidden="true" tabindex="-1"></a>The degeneracy we noticed before manifests in linear</span>
<span id="cb1-744"><a href="#cb1-744" aria-hidden="true" tabindex="-1"></a>algebra form by noticing that</span>
<span id="cb1-745"><a href="#cb1-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-746"><a href="#cb1-746" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-747"><a href="#cb1-747" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-748"><a href="#cb1-748" aria-hidden="true" tabindex="-1"></a>1  &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-749"><a href="#cb1-749" aria-hidden="true" tabindex="-1"></a>1 &amp;  1 </span>
<span id="cb1-750"><a href="#cb1-750" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-751"><a href="#cb1-751" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-752"><a href="#cb1-752" aria-hidden="true" tabindex="-1"></a>1 <span class="sc">\\</span> -1</span>
<span id="cb1-753"><a href="#cb1-753" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-754"><a href="#cb1-754" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-755"><a href="#cb1-755" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-756"><a href="#cb1-756" aria-hidden="true" tabindex="-1"></a>1  -1 <span class="sc">\\</span> 1 - 1</span>
<span id="cb1-757"><a href="#cb1-757" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-758"><a href="#cb1-758" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-759"><a href="#cb1-759" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-760"><a href="#cb1-760" aria-hidden="true" tabindex="-1"></a>0 <span class="sc">\\</span> 0</span>
<span id="cb1-761"><a href="#cb1-761" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-762"><a href="#cb1-762" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-763"><a href="#cb1-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-764"><a href="#cb1-764" aria-hidden="true" tabindex="-1"></a>so that</span>
<span id="cb1-765"><a href="#cb1-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-766"><a href="#cb1-766" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-767"><a href="#cb1-767" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-768"><a href="#cb1-768" aria-hidden="true" tabindex="-1"></a>1  &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-769"><a href="#cb1-769" aria-hidden="true" tabindex="-1"></a>1  &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-770"><a href="#cb1-770" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} </span>
<span id="cb1-771"><a href="#cb1-771" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-772"><a href="#cb1-772" aria-hidden="true" tabindex="-1"></a>\beta_1 + 5<span class="sc">\\</span> \beta_2 - 5</span>
<span id="cb1-773"><a href="#cb1-773" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-774"><a href="#cb1-774" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-775"><a href="#cb1-775" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-776"><a href="#cb1-776" aria-hidden="true" tabindex="-1"></a>1  &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-777"><a href="#cb1-777" aria-hidden="true" tabindex="-1"></a>1  &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-778"><a href="#cb1-778" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} </span>
<span id="cb1-779"><a href="#cb1-779" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-780"><a href="#cb1-780" aria-hidden="true" tabindex="-1"></a>\beta_1<span class="sc">\\</span> \beta_2</span>
<span id="cb1-781"><a href="#cb1-781" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-782"><a href="#cb1-782" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>5</span>
<span id="cb1-783"><a href="#cb1-783" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-784"><a href="#cb1-784" aria-hidden="true" tabindex="-1"></a>1  &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-785"><a href="#cb1-785" aria-hidden="true" tabindex="-1"></a>1  &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-786"><a href="#cb1-786" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} </span>
<span id="cb1-787"><a href="#cb1-787" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-788"><a href="#cb1-788" aria-hidden="true" tabindex="-1"></a>1<span class="sc">\\</span> -1</span>
<span id="cb1-789"><a href="#cb1-789" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-790"><a href="#cb1-790" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-791"><a href="#cb1-791" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-792"><a href="#cb1-792" aria-hidden="true" tabindex="-1"></a>1  &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-793"><a href="#cb1-793" aria-hidden="true" tabindex="-1"></a>1  &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-794"><a href="#cb1-794" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} </span>
<span id="cb1-795"><a href="#cb1-795" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-796"><a href="#cb1-796" aria-hidden="true" tabindex="-1"></a>\beta_1<span class="sc">\\</span> \beta_2</span>
<span id="cb1-797"><a href="#cb1-797" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-798"><a href="#cb1-798" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-799"><a href="#cb1-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-800"><a href="#cb1-800" aria-hidden="true" tabindex="-1"></a>In other words, the fact that the matrix $\X^\trans \X$</span>
<span id="cb1-801"><a href="#cb1-801" aria-hidden="true" tabindex="-1"></a>maps a non-zero vector to zero results in the non-existence</span>
<span id="cb1-802"><a href="#cb1-802" aria-hidden="true" tabindex="-1"></a>of the OLS solution.  This will turn out to be very general,</span>
<span id="cb1-803"><a href="#cb1-803" aria-hidden="true" tabindex="-1"></a>and only one of the ways in which the structure of</span>
<span id="cb1-804"><a href="#cb1-804" aria-hidden="true" tabindex="-1"></a>$\X^\trans \X$ will reveal a lot about the behavior</span>
<span id="cb1-805"><a href="#cb1-805" aria-hidden="true" tabindex="-1"></a>of the OLS estimate and fit.</span>
<span id="cb1-806"><a href="#cb1-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-807"><a href="#cb1-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-808"><a href="#cb1-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-809"><a href="#cb1-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-810"><a href="#cb1-810" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>