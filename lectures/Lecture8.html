<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Vector and matrix-valued statistics and limit theorems.</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Vector and matrix-valued statistics and limit theorems.</li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../stat_bear.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/berkeley-stat151a/spring-2024" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course_policies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Policies</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lectures and labs</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments/assignments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../datasets/data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Datasets</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#goals" id="toc-goals" class="nav-link active" data-scroll-target="#goals">Goals</a></li>
  <li><a href="#normal-random-variables" id="toc-normal-random-variables" class="nav-link" data-scroll-target="#normal-random-variables">Normal random variables</a></li>
  <li><a href="#vector-valued-random-variables" id="toc-vector-valued-random-variables" class="nav-link" data-scroll-target="#vector-valued-random-variables">Vector-valued random variables</a>
  <ul class="collapse">
  <li><a href="#the-bivariate-normal-distribution" id="toc-the-bivariate-normal-distribution" class="nav-link" data-scroll-target="#the-bivariate-normal-distribution">The bivariate normal distribution</a></li>
  <li><a href="#the-multivariate-normal-distribution" id="toc-the-multivariate-normal-distribution" class="nav-link" data-scroll-target="#the-multivariate-normal-distribution">The multivariate normal distribution</a></li>
  <li><a href="#generic-random-vectors" id="toc-generic-random-vectors" class="nav-link" data-scroll-target="#generic-random-vectors">Generic random vectors</a></li>
  <li><a href="#the-law-of-large-numbers-for-vectors" id="toc-the-law-of-large-numbers-for-vectors" class="nav-link" data-scroll-target="#the-law-of-large-numbers-for-vectors">The law of large numbers for vectors</a></li>
  <li><a href="#extensions" id="toc-extensions" class="nav-link" data-scroll-target="#extensions">Extensions</a></li>
  <li><a href="#the-central-limit-theorem-for-vectors" id="toc-the-central-limit-theorem-for-vectors" class="nav-link" data-scroll-target="#the-central-limit-theorem-for-vectors">The central limit theorem for vectors</a></li>
  <li><a href="#extensions-1" id="toc-extensions-1" class="nav-link" data-scroll-target="#extensions-1">Extensions</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
$$

\newcommand{\mybold}[1]{\boldsymbol{#1}} 


\newcommand{\trans}{\intercal}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\bbr}{\mathbb{R}}
\newcommand{\bbz}{\mathbb{Z}}
\newcommand{\bbc}{\mathbb{C}}
\newcommand{\gauss}[1]{\mathcal{N}\left(#1\right)}
\newcommand{\chisq}[1]{\mathcal{\chi}^2_{#1}}
\newcommand{\studentt}[1]{\mathrm{StudentT}_{#1}}

\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\,}
\newcommand{\projop}[1]{\underset{#1}{\mathrm{Proj}}\,}
\newcommand{\proj}[1]{\underset{#1}{\mybold{P}}}
\newcommand{\expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\dens}[1]{\mathit{p}\left(#1\right)}
\newcommand{\var}[1]{\mathrm{Var}\left(#1\right)}
\newcommand{\cov}[1]{\mathrm{Cov}\left(#1\right)}
\newcommand{\sumn}{\sum_{n=1}^N}
\newcommand{\meann}{\frac{1}{N} \sumn}

\newcommand{\trace}[1]{\mathrm{trace}\left(#1\right)}
\newcommand{\diag}[1]{\mathrm{Diag}\left(#1\right)}
\newcommand{\grad}[2]{\nabla_{#1} \left. #2 \right.}
\newcommand{\gradat}[3]{\nabla_{#1} \left. #2 \right|_{#3}}
\newcommand{\fracat}[3]{\left. \frac{#1}{#2} \right|_{#3}}


\newcommand{\W}{\mybold{W}}
\newcommand{\w}{w}
\newcommand{\wbar}{\bar{w}}
\newcommand{\wv}{\mybold{w}}

\newcommand{\X}{\mybold{X}}
\newcommand{\x}{x}
\newcommand{\xbar}{\bar{x}}
\newcommand{\xv}{\mybold{x}}
\newcommand{\Xcov}{\Sigmam_{\X}}

\newcommand{\Z}{\mybold{Z}}
\newcommand{\z}{z}
\newcommand{\zv}{\mybold{z}}
\newcommand{\zbar}{\bar{z}}

\newcommand{\Y}{\mybold{Y}}
\newcommand{\Yhat}{\hat{\Y}}
\newcommand{\y}{y}
\newcommand{\yv}{\mybold{y}}
\newcommand{\yhat}{\hat{\y}}
\newcommand{\ybar}{\bar{y}}

\newcommand{\res}{\varepsilon}
\newcommand{\resv}{\mybold{\res}}
\newcommand{\resvhat}{\hat{\mybold{\res}}}
\newcommand{\reshat}{\hat{\res}}

\newcommand{\betav}{\mybold{\beta}}
\newcommand{\betavhat}{\hat{\bv}}
\newcommand{\betahat}{\hat{\beta}}

\newcommand{\bv}{\mybold{\beta}}
\newcommand{\bvhat}{\hat{\bv}}
\newcommand{\bhat}{\hat{\beta}}

\newcommand{\alphav}{\mybold{\alpha}}
\newcommand{\alphavhat}{\hat{\av}}
\newcommand{\alphahat}{\hat{\alpha}}

\newcommand{\gv}{\mybold{\gamma}}
\newcommand{\gvhat}{\hat{\gv}}
\newcommand{\ghat}{\hat{\gamma}}

\newcommand{\hv}{\mybold{\h}}
\newcommand{\hvhat}{\hat{\hv}}
\newcommand{\hhat}{\hat{\h}}

\newcommand{\gammav}{\mybold{\gamma}}
\newcommand{\gammavhat}{\hat{\gammav}}
\newcommand{\gammahat}{\hat{\gamma}}

\newcommand{\new}{\mathrm{new}}
\newcommand{\zerov}{\mybold{0}}
\newcommand{\onev}{\mybold{1}}
\newcommand{\id}{\mybold{I}}

\newcommand{\sigmahat}{\hat{\sigma}}


\newcommand{\etav}{\mybold{\eta}}
\newcommand{\muv}{\mybold{\mu}}
\newcommand{\Sigmam}{\mybold{\Sigma}}

\newcommand{\rdom}[1]{\mathbb{R}^{#1}}

\newcommand{\RV}[1]{\tilde{#1}}



\def\A{\mybold{A}}

\def\A{\mybold{A}}
\def\av{\mybold{a}}
\def\a{a}

\def\B{\mybold{B}}


\def\S{\mybold{S}}
\def\sv{\mybold{s}}
\def\s{s}

\def\R{\mybold{R}}
\def\rv{\mybold{r}}
\def\r{r}

\def\V{\mybold{V}}
\def\vv{\mybold{v}}
\def\v{v}

\def\U{\mybold{U}}
\def\uv{\mybold{u}}
\def\u{u}

\def\tv{\mybold{t}}
\def\t{t}

\def\Sc{\mathcal{S}}
\def\ev{\mybold{e}}

\def\Lammat{\mybold{\Lambda}}


$$

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Vector and matrix-valued statistics and limit theorems.</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><span class="math inline">\(\LaTeX\)</span></p>
<section id="goals" class="level1">
<h1>Goals</h1>
<ul>
<li>Introduce / review multivariate probability
<ul>
<li>The law of large numbers for vectors and matrices</li>
<li>The multivariate normal distribution</li>
<li>The central limit theorem for vectors</li>
</ul></li>
</ul>
</section>
<section id="normal-random-variables" class="level1">
<h1>Normal random variables</h1>
<p>There is a special distribution called a “Normal” or “Gaussian” distribution that plays a special role in statistics largely due to the Central Limit Theorem (which we will discuss shortly). Specifically, <span class="math inline">\(\RV{\z}\)</span> is a “standard normal random variable” if</p>
<ul>
<li><span class="math inline">\(\expect{\RV{\z}}\)</span> = 0</li>
<li><span class="math inline">\(\var{\RV{\z}}\)</span> = 1</li>
<li><span class="math inline">\(\dens{z} = \frac{1}{\sqrt{2\pi}} \exp\left( - \frac{z^2}{2} \right)\)</span>.</li>
</ul>
<p>For this class, the particular form of the density will not be too important, except possibly when doing Bayesian statistics. But importantly, if <span class="math inline">\(\RV{\z}\)</span> is standard normal, then we can accurately calculate the probability <span class="math inline">\(\prob{\RV{\z} \le z}\)</span> for any <span class="math inline">\(z \in \rdom{}\)</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>From <span class="math inline">\(\prob{\RV{\z} \le z}\)</span>, show how to compute <span class="math inline">\(\prob{\RV{\z} \ge z'}\)</span> and <span class="math inline">\(\prob{\abs{\RV{\z}} \le z''}\)</span> for any <span class="math inline">\(z'\)</span> and <span class="math inline">\(z''\)</span>.</p>
</div>
</div>
<p>For any <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, we say that the random variable <span class="math inline">\(\RV{\x} = \mu + \sigma \z\)</span> is also standard normal, but with</p>
<ul>
<li><span class="math inline">\(\expect{\RV{\x}} = \mu\)</span></li>
<li><span class="math inline">\(\var{\RV{\x}} = \sigma^2\)</span></li>
<li><span class="math inline">\(\dens{x} = \frac{1}{\sigma \sqrt{2\pi}} \exp\left( - \frac{(x - \mu)^2}{2\sigma^2} \right)\)</span>.</li>
</ul>
<p>Further, if we have two independent normal random variables, <span class="math inline">\(\RV{u}\)</span> and <span class="math inline">\(\RV{v}\)</span>, the sum <span class="math inline">\(\RV{u} + \RV{v}\)</span> is itself normal, with mean and variance given by the ordinary rules of expectation. Recall that independence means things like:</p>
<ul>
<li><span class="math inline">\(\expect{\RV{u} \RV{v}} = \expect{\RV{u}} \expect{\RV{v}}\)</span></li>
<li><span class="math inline">\(\expect{\RV{u} | \RV{v}} = \expect{\RV{u}}\)</span></li>
<li><span class="math inline">\(\prob{\RV{u} \in A \textrm{ and } \RV{v} \in B} = \prob{\RV{u} \in A} \prob{\RV{v} \in B}\)</span>,</li>
</ul>
<p>and so on.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>I will write <span class="math inline">\(\RV{\x} \sim \gauss{\mu, \sigma^2}\)</span> to mean that <span class="math inline">\(\RV{\x}\)</span> is a normal random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
</div>
</section>
<section id="vector-valued-random-variables" class="level1">
<h1>Vector-valued random variables</h1>
<section id="the-bivariate-normal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="the-bivariate-normal-distribution">The bivariate normal distribution</h2>
<p>Suppose that <span class="math inline">\(\RV{\u}_1\)</span> and <span class="math inline">\(\RV{\u}_2\)</span> are independent standard normal random variables. Suppose we define the new random variables</p>
<p><span class="math display">\[
\begin{aligned}
\RV{\x_1} :={}&amp; a_{11} \RV{\u}_1 + a_{12} \RV{\u}_2 \\
\RV{\x_2} :={}&amp; a_{21} \RV{\u}_1 + a_{22} \RV{\u}_2.
\end{aligned}
\]</span></p>
<p>We can see that <span class="math inline">\(\RV{\x_1}\)</span> and <span class="math inline">\(\RV{x_2}\)</span> are each normal random variables, and we can compute their means and variances:</p>
<p><span class="math display">\[
\begin{aligned}
\expect{\RV{\x_1}} :={}&amp; a_{11} \expect{\RV{\u}_1} + a_{12} \expect{\RV{\u}_2} = 0 \\
\expect{\RV{\x_2}} :={}&amp; a_{21} \expect{\RV{\u}_1} + a_{22} \expect{\RV{\u}_2} = 0 \\
\var{\RV{\x_1}} :={}&amp; a_{11}^2 \var{\RV{\u}_1} + a_{12}^2 \var{\RV{\u}_2} = a_{11}^2 + a_{12}^2 \\
\var{\RV{\x_2}} :={}&amp; a_{21}^2 \var{\RV{\u}_1} + a_{22}^2 \var{\RV{\u}_2} = a_{21}^2 + a_{22}^2.
\end{aligned}
\]</span></p>
<p>But in general <span class="math inline">\(\RV{\x_1}\)</span> and <span class="math inline">\(\RV{x_2}\)</span> are not independent. If they were, we would have <span class="math inline">\(\expect{\RV{\x_1} \RV{\x_2}} = \expect{\RV{\x_1}} \expect{\RV{\x_2}} = 0\)</span>, but in fact we have</p>
<p><span class="math display">\[
\expect{\RV{\x_1} \RV{\x_2}} =
\expect{(a_{11} \RV{\u}_1 + a_{12} \RV{\u}_2)(a_{21} \RV{\u}_1 + a_{22} \RV{\u}_2)} =
a_{11} a_{21} \expect{\RV{\u}_1^2} + a_{12} a_{22} \expect{\RV{\u}_2^2} =
a_{11} a_{21} + a_{12} a_{22}.
\]</span></p>
<p>The variables <span class="math inline">\(\RV{\x_1}\)</span> and <span class="math inline">\(\RV{x_2}\)</span> are instances of the <em>bivariate normal distribution</em>, which is the two-dimensional analogue of the normal distribution. One can define the bivariate normal distribution in many ways. Here, we have defined it by taking linear combinations of independent univariate normal distributions. It turns out this is always possible, and for the purposes of this class, we will see that it is enough to use such a definition.</p>
<p>The properties we derived above can in fact be represented more succinctly in vector notation. We can write</p>
<p><span class="math display">\[
\RV{\xv} =
\begin{pmatrix}
\RV{\x_1} \\
\RV{\x_2} \\
\end{pmatrix} =
\begin{pmatrix}
a_{11} &amp; a_{12} \\
a_{21} &amp; a_{22} \\
\end{pmatrix}
\begin{pmatrix}
\RV{\u}_1 \\
\RV{\u}_2 \\
\end{pmatrix} =:
\A \RV{\uv}.
\]</span></p>
<p>Note that the matrix <span class="math inline">\(\A\)</span> is not random. Defining the expectation of a vector to be the vector of expectations of its entries, we get</p>
<p><span class="math display">\[
\expect{\RV{\xv}} = \A \expect{\RV{\uv}} = \A \zerov = \zerov,
\]</span></p>
<p>just as above.</p>
<p>Defining the “variance” of <span class="math inline">\(\RV{\xv}\)</span> is more subtle. Recall that a normal distribution is fully characterized by its variance. We would like this to be the case for the bivariate normal as well. But for this it is not enough to know the marginal varianecs <span class="math inline">\(\var{\RV{\x_1}}\)</span> and <span class="math inline">\(\var{\RV{\x_2}}\)</span>, since the “covariance” <span class="math inline">\(\cov{\RV{\x_1}, \RV{\x_2}}\)</span> is also very important for the behavior of the random variable.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>When dealing with two scalar– or vector–valued random variables, I will write the covariance as a function of two arguments, e.g., <span class="math inline">\(\cov{\RV{\x_1}, \RV{\x_2}}\)</span>. However, when speaking of only a single random variable I might write <span class="math inline">\(\cov{\RV{\xv}}\)</span> as a shorthand for the covariance of a vector with itself, e.g., <span class="math inline">\(\cov{\RV{\xv}} = \cov{\RV{\xv}, \RV{\xv}}\)</span>. I will try to reserve the variance <span class="math inline">\(\var{\RV{\x}}\)</span> for only the variance of a single scalar random variable.</p>
</div>
</div>
<p>A convenient way to write all the covariances in a single expression is to define</p>
<p><span class="math display">\[
\begin{aligned}
\cov{\RV{\xv}} ={}&amp;
  \expect{\left(\RV{\xv} - \expect{\RV{\xv}} \right)
          \left(\RV{\xv} - \expect{\RV{\xv}} \right)^\trans}
          \\={}&amp;
  \expect{
    \begin{pmatrix}
      (\RV{\x_1} - \expect{\RV{\x_1}})^2 &amp;
        (\RV{\x_1} - \expect{\RV{\x_1}})(\RV{\x_2} - \expect{\RV{\x_2}}) \\
      (\RV{\x_1} - \expect{\RV{\x_1}})(\RV{\x_2} - \expect{\RV{\x_2}}) &amp;
        (\RV{\x_2} - \expect{\RV{\x_2}})^2
    \end{pmatrix}
  }
          \\={}&amp;
  \begin{pmatrix}
  \var{\RV{\x_1}} &amp; \cov{\RV{\x_1}, \RV{\x_2}} \\
  \cov{\RV{\x_1}, \RV{\x_2}} &amp; \var{\RV{\x_2}}
  \end{pmatrix}.
\end{aligned}
\]</span></p>
<p>Note that the dimension of the matrix inside the expectation is <span class="math inline">\(2 \times 2\)</span>, since we take the transpose on the left. By expanding, we see that each entry of this “covariance matrix” has the covariance between two entries of the vector, and the diagonal contains the “marginal” variances.</p>
<p>This expression in fact allows quite convenient calculation of all the covariances above, since</p>
<p><span class="math display">\[
\begin{aligned}
\cov{\RV{\xv}} ={}&amp;
\expect{\A \RV{\uv} \RV{\uv}^\trans \A^\trans}
\\={}&amp;
\A  \expect{\RV{\uv} \RV{\uv}^\trans } \A^\trans
\\={}&amp;
\A  \id \A^\trans
\\={}&amp;
\A \A^\trans.
\end{aligned}
\]</span></p>
<p>You can readily verify that the expression matches the ones derived manually above.</p>
</section>
<section id="the-multivariate-normal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="the-multivariate-normal-distribution">The multivariate normal distribution</h2>
<p>We can readily generalize the previous section to <span class="math inline">\(P\)</span>–dimensional vectors. Let <span class="math inline">\(\RV{\uv}\)</span> denote a vector of <span class="math inline">\(P\)</span> standard normal random variables. Then define</p>
<p><span class="math display">\[
\RV{\xv} := \A \RV{\uv} + \muv.
\]</span></p>
<p>Then we say that <span class="math inline">\(\RV{\xv}\)</span> is a multivariate normal random variable with mean</p>
<p><span class="math display">\[
\expect{\RV{\xv}} = \A \expect{\RV{\uv}} + \muv = \muv
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\cov{\RV{\xv}} =
\expect{(\RV{\xv} - \muv)(\RV{\xv} - \muv)^\trans} =
\expect{(\A \RV{\uv})(\A \RV{\uv})^\trans} =
\A \A^\trans,
\]</span></p>
<p>writing</p>
<p><span class="math display">\[
\RV{\xv} \sim \gauss{\muv, \A \A^\trans}.
\]</span></p>
<p>Suppose we want to design a multivariate normal with a given covariance matrix <span class="math inline">\(\Sigmam\)</span>. If we require that <span class="math inline">\(\Sigmam\)</span> be positive semi-definite (see exercise), the we can take <span class="math inline">\(\A = \Sigmam^{1/2}\)</span>, and use that to construct a multivariate normal with covariance <span class="math inline">\(\Sigmam\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>I will write <span class="math inline">\(\RV{\xv} \sim \gauss{\muv, \Sigmam}\)</span> to mean that <span class="math inline">\(\RV{\xv}\)</span> is a multivariate normal random variable with mean <span class="math inline">\(\muv\)</span> and covariance matrix <span class="math inline">\(\Sigmam\)</span>.</p>
</div>
</div>
<p>Note that we will not typically go through the construction <span class="math inline">\(\RV{\xv} = \Sigmam^{1/2} \RV{\uv}\)</span> — we’ll take for granted that we can do so. The construction in terms of univariate random variables is simply an easy way to define multivariate random variables without having to deal with multivariate densities.</p>
<div class="callout callout-style-default callout-warning callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Show that if you have a vector-valued random variable with a non positive semi-definite covariance matrix, you can construct a univariate random variable with negative variance, which is impossible. It follows that every vector covariance must be postive semi-definite.</p>
</div>
</div>
<p>A few useful properties come out immediately from properties of the univariate normal distribution:</p>
<ul>
<li>The entries of a multivariate normal random vector are independent if an only if the covariance matrix is diagonal.</li>
<li>Any linear combination of a multivariate normal random variable is itself multivariate normal (possibly with different dimension).</li>
<li><span class="math inline">\(\prob{\RV{\xv} \in S} = \prob{\RV{\uv} \in \{\uv: \A \uv \in S\}}\)</span></li>
</ul>
</section>
<section id="generic-random-vectors" class="level2">
<h2 class="anchored" data-anchor-id="generic-random-vectors">Generic random vectors</h2>
<p>In general, we can consider a random vector — or a random matrix — as a collection of potentially non-independent random values.<br>
For such a vector <span class="math inline">\(\RV{\xv} \in \rdom{P}\)</span>, we can speak about its expectation and covariance just as we would for a multivariate normal distribution. Specifically,</p>
<p><span class="math display">\[
\expect{\RV{\xv}} =
\begin{pmatrix}
\expect{\RV{\x}_1} \\
\vdots \\
\expect{\RV{\x}_P} \\
\end{pmatrix}
\quad\textrm{and}\quad
\cov{\RV{\xv}} =
\expect{\left(\RV{\xv} - \expect{\RV{\xv}} \right)
        \left(\RV{\xv} - \expect{\RV{\xv}} \right)^\trans} =
\begin{pmatrix}
\var{\RV{\x_1}} &amp; \cov{\RV{\x_1}, \RV{\x_2}} &amp; \ldots &amp; \cov{\RV{\x_1}, \RV{\x_P}} \\
\cov{\RV{\x_2}, \RV{\x_1}} &amp; \ldots &amp; \ldots &amp; \cov{\RV{\x_2}, \RV{\x_P}} \\
\vdots &amp; &amp; &amp; \vdots \\
\cov{\RV{\x_P}, \RV{\x_1}} &amp; \ldots &amp; \ldots &amp; \var{\RV{\x_P}} \\
\end{pmatrix}.
\]</span></p>
<p>For these expressions to exist, it suffices for <span class="math inline">\(\expect{\RV{\x}_p^2} &lt; \infty\)</span> for all <span class="math inline">\(p \in \{1,\ldots,P\}\)</span>.</p>
<p>We will at times talk about the expectation of a random matrix, <span class="math inline">\(\RV{\X}\)</span>, which is simply the matrix of expectations. The notation for covariances of course doesn’t make sense for matrices, but it won’t be needed. (In fact, in cases where the covariance of a random matrix is needed in statistics, the matrix is typically stacked into a vector first.)</p>
</section>
<section id="the-law-of-large-numbers-for-vectors" class="level2">
<h2 class="anchored" data-anchor-id="the-law-of-large-numbers-for-vectors">The law of large numbers for vectors</h2>
<p>The law of large numbers is particularly simple for vectors — as long as the dimension stays fixed as <span class="math inline">\(N \rightarrow \infty\)</span>, you can simply apply the LLN to each component separately. Suppose you’re given a sequence of vector-valued random variables, <span class="math inline">\(\RV{\xv}_n \in \rdom{P}\)</span>. Write <span class="math inline">\(\expect{\RV{\xv}_n} = \muv_n\)</span>, and <span class="math inline">\(\var{\RV{\xv}_{np}} = \v_{np}\)</span> for each <span class="math inline">\(p \in \{1,\ldots,P\}\)</span>. Assume that <span class="math inline">\(\max_{n,p} \v_{np} &lt; \infty\)</span>, and that <span class="math inline">\(\muv_n \rightarrow \overline{\muv}\)</span>.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Note to future instructors">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note to future instructors
</div>
</div>
<div class="callout-body-container callout-body">
<p>As with the univariate LLN, we really need <span class="math inline">\(\lim_{N\rightarrow\infty} \max_{n=1,\ldots,N, p} \frac{1}{N} \v_{np} = 0\)</span>, which is more convenient when we actually use this for the fixed regressor setting, but which complicates the exhibition here.</p>
</div>
</div>
<p>Then we can apply the LLN to each component, getting</p>
<p><span class="math display">\[
\overline{\RV{\xv}} :=
\meann \RV{\xv}_n \rightarrow \overline{\muv}
\quad\textrm{as }N\rightarrow \infty.
\]</span></p>
<p>You might be worried that we’re combining probabilistic arguments, even though the entries of <span class="math inline">\(\xv_n\)</span> might not be independent.<br>
(If you are worried about that, wonderful!) Since the dimension is fixed, note that we can write</p>
<p><span class="math display">\[
\prob{\abs{\overline{\RV{\xv}}_p - \overline{\muv}_p} &lt; \varepsilon \textrm{ for all }p} =
\prob{\abs{\overline{\RV{\xv}}_1 - \overline{\muv}_1} &lt; \varepsilon \ldots\textrm{ and }\ldots
  \abs{\overline{\RV{\xv}}_P - \overline{\muv}_P} &lt; \varepsilon
} \le
\sum_{p=1}^P
  \prob{\abs{\overline{\RV{\xv}}_p - \overline{\muv}_p} &lt; \varepsilon}
  \rightarrow 0,
\]</span></p>
<p>since each entry in the sum goes to zero.</p>
</section>
<section id="extensions" class="level2">
<h2 class="anchored" data-anchor-id="extensions">Extensions</h2>
<p>Since we see that the shape of the vector doesn’t matter, we can also apply the LLN to matrices. For example, if <span class="math inline">\(\RV{\X}_n\)</span> is a sequence of random matrices, with <span class="math inline">\(\expect{\RV{\X}_n} = \A\)</span>, then</p>
<p><span class="math display">\[
\meann \RV{\X}_n \rightarrow \A.
\]</span></p>
<p>We will also use (without proof) a theorem called the <strong>continuous mapping theorem</strong>, which says that, for a continous function <span class="math inline">\(f(\cdot)\)</span>, then</p>
<p><span class="math display">\[
f\left(\meann \RV{\xv}_n\right) \rightarrow f(\overline{\mu}).
\]</span></p>
<p>Note that the preceding statment is different than saying</p>
<p><span class="math display">\[
\meann  f\left( RV{\xv}_n \right) \rightarrow \expect{f\left( RV{\xv}_n \right)},
\]</span></p>
<p>which may also be true, but which applies the LLN to the random variables <span class="math inline">\(f\left( RV{\xv}_n \right)\)</span>.</p>
</section>
<section id="the-central-limit-theorem-for-vectors" class="level2">
<h2 class="anchored" data-anchor-id="the-central-limit-theorem-for-vectors">The central limit theorem for vectors</h2>
<p>We might hope that we can apply the CLT componentwise just as we did for the LLN, but we are not so lucky. It is true that, assuming (as before) that <span class="math inline">\(\meann \v_{np} \rightarrow \overline{v}_p\)</span> for each <span class="math inline">\(p\)</span>, that</p>
<p><span class="math display">\[
\sqrt{N} (\overline{\RV{\xv}}_p - \overline{\mu}_p) \rightarrow
  \gauss{0, \overline{v}_p}.
\]</span></p>
<p>However, this does not tell us about the <em>joint</em> behavior of the random variables. Here’s a simple example that illustrates the problem. Consider two settings, based on IID standard normal scalar variables <span class="math inline">\(\RV{u}_n\)</span> and <span class="math inline">\(\RV{v}_n\)</span>.</p>
<section id="setting-one" class="level4">
<h4 class="anchored" data-anchor-id="setting-one">Setting one</h4>
<p>Take</p>
<p><span class="math display">\[\RV{\xv}_n =
\begin{pmatrix}
\RV{u}_n\\
\RV{v}_n
\end{pmatrix}.
\]</span></p>
<p>We know that <span class="math inline">\(\frac{1}{\sqrt{N}} \RV{\xv}_n\)</span> is exactly multivariate normal with mean <span class="math inline">\(\zerov\)</span> and covariance matrix <span class="math inline">\(\id{}\)</span> for all <span class="math inline">\(N\)</span>. So, <em>a fortiori</em>,</p>
<p><span class="math display">\[
\frac{1}{\sqrt{N}} \sumn \RV{\xv}_n \rightarrow \gauss{\zerov, \id{}}.
\]</span></p>
</section>
<section id="setting-two" class="level4">
<h4 class="anchored" data-anchor-id="setting-two">Setting two</h4>
<p>Take</p>
<p><span class="math display">\[\RV{\yv}_n =
\begin{pmatrix}
\RV{u}_n\\
\RV{u}_n
\end{pmatrix}.
\]</span></p>
<p>We know that <span class="math inline">\(\frac{1}{\sqrt{N}} \RV{\yv}_n\)</span> is exactly multivariate normal with mean <span class="math inline">\(\zerov\)</span> and covariance matrix <span class="math inline">\(\onev \onev^\trans\)</span> for all <span class="math inline">\(N\)</span>.<br>
So, <em>a fortiori</em>,</p>
<p><span class="math display">\[
\frac{1}{\sqrt{N}} \sumn \RV{\yv}_n \rightarrow \gauss{\zerov, \onev \onev^\trans}.
\]</span></p>
<p>For both <span class="math inline">\(\RV{\xv}\)</span> and <span class="math inline">\(\RV{\yv}\)</span>, each component converges to a standard normal random variable. But for <span class="math inline">\(\RV{\xv}\)</span> the two components were independent, and for <span class="math inline">\(\RV{\yv}\)</span> they were perfectly dependent. The behavior of the marginals cannot tell you about the joint behavior.</p>
<p>Consequently, we have to consider the behavior of the whole vector. In particular, write</p>
<p><span class="math display">\[
\Sigmam_n := \cov{\RV{\xv}_n}
\]</span></p>
<p>and assume that</p>
<p><span class="math display">\[
\meann \Sigmam_n \rightarrow \overline{\Sigmam}
\]</span></p>
<p>for each entry of the matrix, where <span class="math inline">\(\overline{\Sigmam}\)</span> is element-wise finite. (Note that this requires the average of each entry of the diagonal to to converge!) Then</p>
<p><span class="math display">\[
\frac{1}{\sqrt{N}} \left( \sumn \RV{\xv}_n - \expect{\RV{\xv}_n} \right)
\rightarrow \RV{\zv} \quad\textrm{ where }
\RV{\zv} \sim \gauss{\zerov, \overline{\Sigmam}}.
\]</span></p>
</section>
</section>
<section id="extensions-1" class="level2">
<h2 class="anchored" data-anchor-id="extensions-1">Extensions</h2>
<p>Finally, we note that the continuous mapping theorem applies to the CLT as well. That is, if <span class="math inline">\(f(\cdot)\)</span> is continuous, and</p>
<p><span class="math display">\[
\frac{1}{\sqrt{N}} \left( \sumn \RV{\xv}_n - \expect{\RV{\xv}_n} \right)
\rightarrow \RV{\zv}
\quad\textrm{ where }
\RV{\zv} \sim \gauss{\zerov, \overline{\Sigmam}},
\]</span></p>
<p>then</p>
<p><span class="math display">\[
f\left( \frac{1}{\sqrt{N}} \left( \sumn \RV{\xv}_n - \expect{\RV{\xv}_n} \right) \right)
\rightarrow f\left( \RV{\zv} \right).
\]</span></p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Vector and matrix-valued statistics and limit theorems."</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    include-before-body:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">     - file: ../macros.md</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>$\LaTeX$</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu"># Goals</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Introduce / review multivariate probability</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>The law of large numbers for vectors and matrices</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>The multivariate normal distribution</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>The central limit theorem for vectors</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="fu"># Normal random variables</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>There is a special distribution called a "Normal" or "Gaussian" distribution that</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plays a special role in statistics largely due to the Central Limit Theorem</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>(which we will discuss shortly).</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>Specifically, $\RV{\z}$ is a "standard normal random variable" if</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\expect{\RV{\z}}$ = 0</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\var{\RV{\z}}$ = 1</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\dens{z} = \frac{1}{\sqrt{2\pi}} \exp\left( - \frac{z^2}{2} \right)$.</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>For this class, the particular form of the density will not be too important,</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>except possibly when doing Bayesian statistics.  But importantly, </span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>if $\RV{\z}$ is standard normal, then we can accurately calculate the probability</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>$\prob{\RV{\z} \le z}$ for any $z \in \rdom{}$.</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Exercise'} </span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>From $\prob{\RV{\z} \le z}$, show how to compute</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>$\prob{\RV{\z} \ge z'}$ and $\prob{\abs{\RV{\z}} \le z''}$</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>for any $z'$ and $z''$.</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>For any $\mu$ and $\sigma$, we say that the random variable</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>$\RV{\x} = \mu + \sigma \z$ is also standard normal, but with</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\expect{\RV{\x}} = \mu$</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\var{\RV{\x}} = \sigma^2$</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\dens{x} = \frac{1}{\sigma \sqrt{2\pi}} \exp\left( - \frac{(x - \mu)^2}{2\sigma^2} \right)$.</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>Further, if we have two independent normal random variables, $\RV{u}$ and $\RV{v}$,</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>the sum $\RV{u} + \RV{v}$ is itself normal, with mean and variance given by</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>the ordinary rules of expectation. Recall that independence means things like:</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\expect{\RV{u} \RV{v}} = \expect{\RV{u}} \expect{\RV{v}}$</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\expect{\RV{u} | \RV{v}} = \expect{\RV{u}}$</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\prob{\RV{u} \in A \textrm{ and } \RV{v} \in B} = \prob{\RV{u} \in A} \prob{\RV{v} \in B}$,</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>and so on. </span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'} </span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>I will write $\RV{\x} \sim \gauss{\mu, \sigma^2}$ to mean that $\RV{\x}$ is a normal random</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>variable with mean $\mu$ and variance $\sigma^2$.</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="fu"># Vector-valued random variables</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="fu">## The bivariate normal distribution</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>Suppose that $\RV{\u}_1$ and $\RV{\u}_2$ are independent</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>standard normal random variables.   Suppose we define the new random variables</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>\RV{\x_1} :={}&amp; a_{11} \RV{\u}_1 + a_{12} \RV{\u}_2 <span class="sc">\\</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>\RV{\x_2} :={}&amp; a_{21} \RV{\u}_1 + a_{22} \RV{\u}_2.</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>We can see that $\RV{\x_1}$ and $\RV{x_2}$ are each normal random variables,</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>and we can compute their means and variances:</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>\expect{\RV{\x_1}} :={}&amp; a_{11} \expect{\RV{\u}_1} + a_{12} \expect{\RV{\u}_2} = 0 <span class="sc">\\</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>\expect{\RV{\x_2}} :={}&amp; a_{21} \expect{\RV{\u}_1} + a_{22} \expect{\RV{\u}_2} = 0 <span class="sc">\\</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>\var{\RV{\x_1}} :={}&amp; a_{11}^2 \var{\RV{\u}_1} + a_{12}^2 \var{\RV{\u}_2} = a_{11}^2 + a_{12}^2 <span class="sc">\\</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>\var{\RV{\x_2}} :={}&amp; a_{21}^2 \var{\RV{\u}_1} + a_{22}^2 \var{\RV{\u}_2} = a_{21}^2 + a_{22}^2.</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>But in general $\RV{\x_1}$ and $\RV{x_2}$  are not independent.  If they were, we would have </span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>$\expect{\RV{\x_1} \RV{\x_2}} = \expect{\RV{\x_1}} \expect{\RV{\x_2}} = 0$, but</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>in fact we have</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>\expect{\RV{\x_1} \RV{\x_2}} =</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>\expect{(a_{11} \RV{\u}_1 + a_{12} \RV{\u}_2)(a_{21} \RV{\u}_1 + a_{22} \RV{\u}_2)} = </span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>a_{11} a_{21} \expect{\RV{\u}_1^2} + a_{12} a_{22} \expect{\RV{\u}_2^2} = </span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>a_{11} a_{21} + a_{12} a_{22}.</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>The variables $\RV{\x_1}$ and $\RV{x_2}$ are instances of the *bivariate normal distribution*,</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>which is the two-dimensional analogue of the normal distribution.  One can define</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>the bivariate normal distribution in many ways.  Here, we have defined it by taking</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>linear combinations of independent univariate normal distributions.  It turns out</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>this is always possible, and for the purposes of this class, we will see that it is</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>enough to use such a definition.</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>The properties we derived above can in fact be represented more succinctly in vector</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>notation.  We can write</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>\RV{\xv} = </span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>\RV{\x_1} <span class="sc">\\</span></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>\RV{\x_2} <span class="sc">\\</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>a_{11} &amp; a_{12} <span class="sc">\\</span></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>a_{21} &amp; a_{22} <span class="sc">\\</span></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} </span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>\RV{\u}_1 <span class="sc">\\</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>\RV{\u}_2 <span class="sc">\\</span></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =:</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>\A \RV{\uv}.</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>Note that the matrix $\A$ is not random.  Defining the expectation of a vector</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>to be the vector of expectations of its entries, we get</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>\expect{\RV{\xv}} = \A \expect{\RV{\uv}} = \A \zerov = \zerov,</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>just as above.</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>Defining the "variance" of $\RV{\xv}$ is more subtle.  Recall that a normal</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>distribution is fully characterized by its variance.  We would like this</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>to be the case for the bivariate normal as well.  But for this it is not enough</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>to know the marginal varianecs $\var{\RV{\x_1}}$ and $\var{\RV{\x_2}}$, since the</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>"covariance" $\cov{\RV{\x_1}, \RV{\x_2}}$ is also very important for the behavior</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>of the random variable. </span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Notation"}</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>When dealing with two scalar-- or vector--valued random variables, I will</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>write the covariance as a function of two arguments, e.g., </span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>$\cov{\RV{\x_1}, \RV{\x_2}}$.  However, when speaking of only a single</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>random variable I might write $\cov{\RV{\xv}}$ as a shorthand for</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>the covariance of a vector with itself, e.g.,</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>$\cov{\RV{\xv}} = \cov{\RV{\xv}, \RV{\xv}}$.  I will try to reserve</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>the variance $\var{\RV{\x}}$ for only the variance of a single scalar</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>random variable.</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>A convenient way to write all the covariances in a single expression is to define</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>\cov{\RV{\xv}} ={}&amp;</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>  \expect{\left(\RV{\xv} - \expect{\RV{\xv}} \right)</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>          \left(\RV{\xv} - \expect{\RV{\xv}} \right)^\trans}</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>          <span class="sc">\\</span>={}&amp;</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>  \expect{</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>    \begin{pmatrix}</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>      (\RV{\x_1} - \expect{\RV{\x_1}})^2 &amp; </span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>        (\RV{\x_1} - \expect{\RV{\x_1}})(\RV{\x_2} - \expect{\RV{\x_2}}) <span class="sc">\\</span></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>      (\RV{\x_1} - \expect{\RV{\x_1}})(\RV{\x_2} - \expect{\RV{\x_2}}) &amp;</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>        (\RV{\x_2} - \expect{\RV{\x_2}})^2</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>    \end{pmatrix}</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>          <span class="sc">\\</span>={}&amp;</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>  \begin{pmatrix}</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>  \var{\RV{\x_1}} &amp; \cov{\RV{\x_1}, \RV{\x_2}} <span class="sc">\\</span></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>  \cov{\RV{\x_1}, \RV{\x_2}} &amp; \var{\RV{\x_2}}</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>  \end{pmatrix}.</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>Note that the dimension of the matrix inside the expectation is $2 \times 2$, since</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>we take the transpose on the left.  By expanding, we see that each entry of this</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>"covariance matrix" has the covariance between two entries of the vector, and the</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>diagonal contains the "marginal" variances.</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>This expression in fact allows quite convenient calculation of all the covariances</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>above, since</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>\cov{\RV{\xv}} ={}&amp;</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>\expect{\A \RV{\uv} \RV{\uv}^\trans \A^\trans} </span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>={}&amp;</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>\A  \expect{\RV{\uv} \RV{\uv}^\trans } \A^\trans</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>={}&amp;</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>\A  \id \A^\trans</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>={}&amp;</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>\A \A^\trans.</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>You can readily verify that the expression matches the ones derived</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>manually above.</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a><span class="fu">## The multivariate normal distribution</span></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>We can readily generalize the previous section to $P$--dimensional vectors.</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>Let $\RV{\uv}$ denote a vector of $P$ standard normal random variables.</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>Then define</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>\RV{\xv} := \A \RV{\uv} + \muv.</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>Then we say that $\RV{\xv}$ is a multivariate normal random</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>variable with mean</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>\expect{\RV{\xv}} = \A \expect{\RV{\uv}} + \muv = \muv</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>and </span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>\cov{\RV{\xv}} =</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>\expect{(\RV{\xv} - \muv)(\RV{\xv} - \muv)^\trans} =</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>\expect{(\A \RV{\uv})(\A \RV{\uv})^\trans} =</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>\A \A^\trans,</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>writing</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>\RV{\xv} \sim \gauss{\muv, \A \A^\trans}.</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>Suppose we want to design a multivariate normal with</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>a given covariance matrix $\Sigmam$.  If we require that</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>$\Sigmam$ be positive semi-definite (see exercise),</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>the we can take $\A = \Sigmam^{1/2}$, and use</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>that to construct a multivariate normal with </span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>covariance $\Sigmam$.</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'} </span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>I will write $\RV{\xv} \sim \gauss{\muv, \Sigmam}$ to mean that $\RV{\xv}$ is a</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>multivariate normal random variable with mean $\muv$ and covariance matrix</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>$\Sigmam$.  </span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>Note that we will not typically go through the construction</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>$\RV{\xv} = \Sigmam^{1/2} \RV{\uv}$ --- we'll take for granted that</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>we can do so.  The construction in terms of univariate random variables</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>is simply an easy way to define multivariate random variables without</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>having to deal with multivariate densities.</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Exercise'} </span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>Show that if you have a vector-valued random variable</span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>with a non positive semi-definite covariance matrix,</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>you can construct a univariate random variable with</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>negative variance, which is impossible.  It follows</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>that every vector covariance must be postive</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>semi-definite.</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>A few useful properties come out immediately from properties</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>of the univariate normal distribution:</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The entries of a multivariate normal random vector are</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>independent if an only if the covariance matrix is diagonal.</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Any linear combination of a multivariate normal random variable</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>is itself multivariate normal (possibly with different dimension).</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\prob{\RV{\xv} \in S} = \prob{\RV{\uv} \in <span class="sc">\{</span>\uv: \A \uv \in S<span class="sc">\}</span>}$</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a><span class="fu">## Generic random vectors</span></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>In general, we can consider a random vector --- or a random matrix ---</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>as a collection of potentially non-independent random values.  </span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>For such a vector $\RV{\xv} \in \rdom{P}$, we can speak about its expectation</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>and covariance just as we would for a multivariate normal distribution.</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>Specifically,</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>\expect{\RV{\xv}} =</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>\expect{\RV{\x}_1} <span class="sc">\\</span></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>\expect{\RV{\x}_P} <span class="sc">\\</span></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>\quad\textrm{and}\quad</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>\cov{\RV{\xv}} =</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>\expect{\left(\RV{\xv} - \expect{\RV{\xv}} \right) </span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>        \left(\RV{\xv} - \expect{\RV{\xv}} \right)^\trans} =</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>\var{\RV{\x_1}} &amp; \cov{\RV{\x_1}, \RV{\x_2}} &amp; \ldots &amp; \cov{\RV{\x_1}, \RV{\x_P}} <span class="sc">\\</span></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>\cov{\RV{\x_2}, \RV{\x_1}} &amp; \ldots &amp; \ldots &amp; \cov{\RV{\x_2}, \RV{\x_P}} <span class="sc">\\</span></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>\vdots &amp; &amp; &amp; \vdots <span class="sc">\\</span></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>\cov{\RV{\x_P}, \RV{\x_1}} &amp; \ldots &amp; \ldots &amp; \var{\RV{\x_P}} <span class="sc">\\</span></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>For these expressions to exist, it suffices for $\expect{\RV{\x}_p^2} &lt; \infty$ </span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>for all $p \in <span class="sc">\{</span>1,\ldots,P<span class="sc">\}</span>$.</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>We will at times talk about the expectation of a random matrix,</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>$\RV{\X}$, which is simply the matrix of expectations.  The notation</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>for covariances of course doesn't make sense for matrices, but it </span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>won't be needed.  (In fact, in cases where the covariance of a random matrix</span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>is needed in statistics, the matrix is typically stacked into a vector first.)</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a><span class="fu">## The law of large numbers for vectors</span></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>The law of large numbers is particularly simple for vectors --- as </span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>long as the dimension stays fixed as $N \rightarrow \infty$, you can</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>simply apply the LLN to each component separately.  Suppose you're</span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>given a sequence of vector-valued random variables, $\RV{\xv}_n \in \rdom{P}$.</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>Write $\expect{\RV{\xv}_n} = \muv_n$, and</span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>$\var{\RV{\xv}_{np}} = \v_{np}$ for each $p \in <span class="sc">\{</span>1,\ldots,P<span class="sc">\}</span>$. Assume that</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>$\max_{n,p} \v_{np} &lt; \infty$, and that $\muv_n \rightarrow \overline{\muv}$.</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>::: {.callout-important title='Note to future instructors'} </span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>As with the univariate LLN, we really need</span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a>$\lim_{N\rightarrow\infty} \max_{n=1,\ldots,N, p} \frac{1}{N} \v_{np} = 0$,</span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>which is more convenient when we actually use this for the fixed regressor</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a>setting, but which complicates the exhibition here.</span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>Then we can apply the LLN to each component, getting</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>\overline{\RV{\xv}} :=</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>\meann \RV{\xv}_n \rightarrow \overline{\muv}</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a>\quad\textrm{as }N\rightarrow \infty.</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a>You might be worried that we're combining probabilistic arguments,</span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a>even though the entries of $\xv_n$ might not be independent.  </span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a>(If you are worried about that, wonderful!)  Since the dimension</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a>is fixed, note that we can write</span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a>\prob{\abs{\overline{\RV{\xv}}_p - \overline{\muv}_p} &lt; \varepsilon \textrm{ for all }p} =</span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>\prob{\abs{\overline{\RV{\xv}}_1 - \overline{\muv}_1} &lt; \varepsilon \ldots\textrm{ and }\ldots</span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a>  \abs{\overline{\RV{\xv}}_P - \overline{\muv}_P} &lt; \varepsilon </span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a>} \le</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a>\sum_{p=1}^P</span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>  \prob{\abs{\overline{\RV{\xv}}_p - \overline{\muv}_p} &lt; \varepsilon}</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a>  \rightarrow 0,</span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a>since each entry in the sum goes to zero.</span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a><span class="fu">## Extensions</span></span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a>Since we see that the shape of the vector doesn't matter, we</span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a>can also apply the LLN to matrices.  For example, if $\RV{\X}_n$</span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a>is a sequence of random matrices, with $\expect{\RV{\X}_n} = \A$,</span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a>then</span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a>\meann \RV{\X}_n \rightarrow \A.</span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a>We will also use (without proof) a theorem called the **continuous</span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a>mapping theorem**, which says that, for a continous function $f(\cdot)$,</span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>then</span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a>f\left(\meann \RV{\xv}_n\right) \rightarrow f(\overline{\mu}).</span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a>Note that the preceding statment is different than saying</span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a>\meann  f\left( RV{\xv}_n \right) \rightarrow \expect{f\left( RV{\xv}_n \right)},</span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a>which may also be true, but which applies the LLN to the random variables</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a>$f\left( RV{\xv}_n \right)$.</span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a><span class="fu">## The central limit theorem for vectors</span></span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a>We might hope that we can apply the CLT componentwise just as we did</span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a>for the LLN, but we are not so lucky.  It is true that, assuming (as</span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a>before) that $\meann \v_{np} \rightarrow \overline{v}_p$ for each</span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a>$p$, that</span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a>\sqrt{N} (\overline{\RV{\xv}}_p - \overline{\mu}_p) \rightarrow </span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a>  \gauss{0, \overline{v}_p}.</span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a>However, this does not tell us about the *joint* behavior of the</span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a>random variables.  Here's a simple example that illustrates the</span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a>problem.  Consider two settings, based on IID standard normal</span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a>scalar variables $\RV{u}_n$ and $\RV{v}_n$.</span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Setting one</span></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a>Take</span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a>$$\RV{\xv}_n =</span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a>\RV{u}_n<span class="sc">\\</span></span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a>\RV{v}_n</span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a>We know that $\frac{1}{\sqrt{N}} \RV{\xv}_n$ is exactly multivariate normal</span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a>with mean $\zerov$ and covariance matrix $\id{}$ for all $N$.  So, *a fortiori*,</span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sqrt{N}} \sumn \RV{\xv}_n \rightarrow \gauss{\zerov, \id{}}.</span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Setting two</span></span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a>Take</span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a>$$\RV{\yv}_n =</span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a>\RV{u}_n<span class="sc">\\</span></span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a>\RV{u}_n</span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a>We know that $\frac{1}{\sqrt{N}} \RV{\yv}_n$ is exactly multivariate normal</span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a>with mean $\zerov$ and covariance matrix $\onev \onev^\trans$ for all $N$.  </span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a>So, *a fortiori*,</span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sqrt{N}} \sumn \RV{\yv}_n \rightarrow \gauss{\zerov, \onev \onev^\trans}.</span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a>For both $\RV{\xv}$ and $\RV{\yv}$, each component converges to a standard</span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a>normal random variable.  But for $\RV{\xv}$ the two components were</span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a>independent, and for $\RV{\yv}$ they were perfectly dependent.  The behavior</span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a>of the marginals cannot tell you about the joint behavior.</span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a>Consequently, we have to consider the behavior of the whole vector.</span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a>In particular, write</span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a>\Sigmam_n := \cov{\RV{\xv}_n}</span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a>and assume that</span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a>\meann \Sigmam_n \rightarrow \overline{\Sigmam}</span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a>for each entry of the matrix,</span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a>where $\overline{\Sigmam}$ is element-wise finite.  (Note that this</span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a>requires the average of each entry of the diagonal to to converge!)</span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a>Then</span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sqrt{N}} \left( \sumn \RV{\xv}_n - \expect{\RV{\xv}_n} \right)</span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a>\rightarrow \RV{\zv} \quad\textrm{ where }</span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a>\RV{\zv} \sim \gauss{\zerov, \overline{\Sigmam}}.</span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a><span class="fu">## Extensions</span></span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a>Finally, we note that the continuous mapping theorem applies</span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a>to the CLT as well.  That is, if $f(\cdot)$ is continuous, </span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sqrt{N}} \left( \sumn \RV{\xv}_n - \expect{\RV{\xv}_n} \right) </span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a>\rightarrow \RV{\zv}</span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a>\quad\textrm{ where }</span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a>\RV{\zv} \sim \gauss{\zerov, \overline{\Sigmam}},</span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a>then</span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a>f\left( \frac{1}{\sqrt{N}} \left( \sumn \RV{\xv}_n - \expect{\RV{\xv}_n} \right) \right)</span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a>\rightarrow f\left( \RV{\zv} \right).</span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a>$$</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>