[
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar",
    "section": "",
    "text": "We can just embed the iframe html:"
  },
  {
    "objectID": "datasets/data.html",
    "href": "datasets/data.html",
    "title": "Datasets",
    "section": "",
    "text": "The following datasets are used in lectures, homework, and labs."
  },
  {
    "objectID": "datasets/data.html#aluminum-dataset",
    "href": "datasets/data.html#aluminum-dataset",
    "title": "Datasets",
    "section": "Aluminum dataset",
    "text": "Aluminum dataset\nDataset containing stress-strain curves for commercially available aluminum samples at varying tempertures. The data accompanies B.S. Aakash, JohnPatrick Connors, Michael D. Shields, Variability in the thermo-mechanical behavior of structural aluminum, Thin-Walled Structures, Volume 144, 2019, 106122, ISSN 0263-8231. (link)\nPaper abstract: The nominal performance of structural aluminum alloys at elevated temperature has been thoroughly investigated in the past. Although it is well known that the performance of a given material specimen will differ from the nominal behavior, the extent of this variability has not been quantitied to date. This limits the ability to perform reliability and performance-based design and analysis for aluminum structures subjected to high temperatures (e.g. in structural fire engineering). This work presents an experimental investigation of the variability in the stress-strain behavior of AA 6061-T651 (as a model ductile aluminum alloy). We performed steady-state tensile tests on nine different batches of nominally identical material sourced from different suppliers/manufacturers at six different temperatures (20 °C, 100 °C, 150 °C, 200 °C, 250 °C, and 300 °C) under two different geometries to induce uniaxial tension and plane strain stress states in the gauge section. The results are investigated statistically to illustrate variability in the salient features of the stress-strain behavior of the material ranging from nonlinear elastic behavior to strain localization and ductile fracture. Some observations on material performance and its variability are made along the way. Overall, it is illustrated that variations between batches of material can be quite large and – especially as it relates to strain localization, necking, and material failure – variations can be very large even within a fixed batch of material. To encourage data of this nature to be expanded and integrated into research and practice to improve structural design and investigations, the full searchable dataset are publicly available with experimental details published concurrently through Data in Brief.\n\nLink to csv\nSource link (downloaded Dec 2023)"
  },
  {
    "objectID": "datasets/data.html#bodyfat-dataset",
    "href": "datasets/data.html#bodyfat-dataset",
    "title": "Datasets",
    "section": "Bodyfat dataset",
    "text": "Bodyfat dataset\nBodyfat and other physical measurements on a number of individuals.\nMeasurement standards are apparently those listed in Benhke and Wilmore (1974), pp. 45-48 where, for instance, the abdomen 2 circumference is measured “laterally, at the level of the iliac crests, and anteriorly, at the umbilicus”.\nThese data are used to produce the predictive equations for lean body weight given in the abstract “Generalized body composition prediction equation for men using simple measurement techniques”, K.W. Penrose, A.G. Nelson, A.G. Fisher, FACSM, Human Performance Research Center, Brigham Young University, Provo, Utah 84602 as listed in Medicine and Science in Sports and Exercise, vol. 17, no. 2, April 1985, p. 189.\n\nLink to csv\nSource link (downloaded Dec 2023)"
  },
  {
    "objectID": "datasets/data.html#spotify-dataset",
    "href": "datasets/data.html#spotify-dataset",
    "title": "Datasets",
    "section": "Spotify dataset",
    "text": "Spotify dataset\nThis dataset consists of roughly 30,000 Songs from the Spotify API with black-box machine learning quantifications of musical features. No guarantees are made on how the tracks were sampled.\n\nLink to csv\nSource link (downloaded Dec 2023)"
  },
  {
    "objectID": "lectures/lab1_student_version.html",
    "href": "lectures/lab1_student_version.html",
    "title": "Stat151 Coding style",
    "section": "",
    "text": "Suppose you work at Spotify, and a colleague told you that high energy songs are the most popular. You asked for their code, and this is what they sent you.\n\nHow did they attempt to answer this question?\nDid they make any mistakes?\nIs it easy to read and think critically about their analysis?\nWould it be easy to re-run their analysis with a slightly different method or dataset?\nHow would you improve their code while keeping their method the same?\nDo you agree with their high-level conclusion? How would you convey these results?\n\n\ndata_location &lt;- \"/home/rgiordan/Documents/git_repos/stat151a_website/datasets\"\ndf &lt;- read.csv(file.path(data_location, \"spotify_songs.csv\"))\n# run analysis\nxx &lt;- df[,c(4,12,13,14,15,16,17,18,19,20,21,22,23)]; xxx =df[,4]\nz=sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,2]-sum(xx[,2])/nrow(xx)))/t(xx[,2]-sum(xx[,2])/nrow(xx))%*%(xx[,2] - sum(xx[,2])/nrow(xx))\nz2=sum((xx[,1]-sum(xx[,1])/nrow(xx))* (xx[,3]-sum(xx[,3])/nrow(xx)))/t(xx[,3]-sum(xx[,3])/nrow(xx))%*%(xx[,3] - sum(xx[,3])/nrow(xx))\nz3=sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,4]-sum(xx[,4])/nrow(xx)))/t(xx[,4]-sum(xx[,4])/nrow(xx))%*%(xx[,4]-sum(xx[,4])/nrow(xx))\nz4&lt;-sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,5]-sum(xx[,5])/nrow(xx)))/t(xx[,5]-sum(xx[,5])/nrow(xx))%*%(xx[,5]-sum(xx[,5])/nrow(xx))\nz5&lt;-sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,6]-sum(xx[,5])/nrow(xx)))/t(xx[,6]-sum(xx[,6])/nrow(xx))%*%(xx[,6]-sum(xx[,6])/nrow(xx))\nz6&lt;-sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,7]-sum(xx[,7])/nrow(xx)))/t(xx[,7]-sum(xx[,2])/nrow(xx))%*%(xx[,7]- sum(xx[,7])/nrow(xx))\nz7&lt;-sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,8]-sum(xx[,8])/nrow(xx)))/t(xx[,8]-sum(xx[,8])/nrow(xx))%*%(xx[,8]- sum(xx[,8])/nrow(xx))\nz8&lt;-sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,9]-sum(xx[,9])/nrow(xx)))/t(xx[,9]-sum(xx[,9])/nrow(xxx))%*%(xx[,9]- sum(xx[,9])/nrow(xx))\nz9&lt;-sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,10]-sum(xx[,10])/nrow(xx)))/t(xx[,10]-sum(xx[,10])/nrow(xxx))%*%(xx[,10] -sum(xx[,10])/nrow(xx))\nz10 &lt;- sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,11]-sum(xx[,11])/nrow(xx)))/t(xx[,11]-sum(xx[,11])/nrow(xxx))%*%(xx[,11] -sum(xx[,11])/nrow(xx))\nz11 &lt;- sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,12]-sum(xx[,12])/nrow(xx)))/t(xx[,12]-sum(xx[,12])/nrow(xx))%*%(xx[,12] -sum(xx[,12])/nrow(xx))\nyy=(names(df)[c(12,13,14,15,16,17,17,19,20,21,22,23)][order(c(z, z2,z3,z4, z5, z6, z7, z8, z9, z10,z11))][1])\nprint(sprintf(\"The best is %s\",yy))\n\n[1] \"The best is energy\""
  },
  {
    "objectID": "lectures/lab1_student_version.html#a-bad-analysis",
    "href": "lectures/lab1_student_version.html#a-bad-analysis",
    "title": "Stat151 Coding style",
    "section": "",
    "text": "Suppose you work at Spotify, and a colleague told you that high energy songs are the most popular. You asked for their code, and this is what they sent you.\n\nHow did they attempt to answer this question?\nDid they make any mistakes?\nIs it easy to read and think critically about their analysis?\nWould it be easy to re-run their analysis with a slightly different method or dataset?\nHow would you improve their code while keeping their method the same?\nDo you agree with their high-level conclusion? How would you convey these results?\n\n\ndata_location &lt;- \"/home/rgiordan/Documents/git_repos/stat151a_website/datasets\"\ndf &lt;- read.csv(file.path(data_location, \"spotify_songs.csv\"))\n# run analysis\nxx &lt;- df[,c(4,12,13,14,15,16,17,18,19,20,21,22,23)]; xxx =df[,4]\nz=sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,2]-sum(xx[,2])/nrow(xx)))/t(xx[,2]-sum(xx[,2])/nrow(xx))%*%(xx[,2] - sum(xx[,2])/nrow(xx))\nz2=sum((xx[,1]-sum(xx[,1])/nrow(xx))* (xx[,3]-sum(xx[,3])/nrow(xx)))/t(xx[,3]-sum(xx[,3])/nrow(xx))%*%(xx[,3] - sum(xx[,3])/nrow(xx))\nz3=sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,4]-sum(xx[,4])/nrow(xx)))/t(xx[,4]-sum(xx[,4])/nrow(xx))%*%(xx[,4]-sum(xx[,4])/nrow(xx))\nz4&lt;-sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,5]-sum(xx[,5])/nrow(xx)))/t(xx[,5]-sum(xx[,5])/nrow(xx))%*%(xx[,5]-sum(xx[,5])/nrow(xx))\nz5&lt;-sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,6]-sum(xx[,5])/nrow(xx)))/t(xx[,6]-sum(xx[,6])/nrow(xx))%*%(xx[,6]-sum(xx[,6])/nrow(xx))\nz6&lt;-sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,7]-sum(xx[,7])/nrow(xx)))/t(xx[,7]-sum(xx[,2])/nrow(xx))%*%(xx[,7]- sum(xx[,7])/nrow(xx))\nz7&lt;-sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,8]-sum(xx[,8])/nrow(xx)))/t(xx[,8]-sum(xx[,8])/nrow(xx))%*%(xx[,8]- sum(xx[,8])/nrow(xx))\nz8&lt;-sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,9]-sum(xx[,9])/nrow(xx)))/t(xx[,9]-sum(xx[,9])/nrow(xxx))%*%(xx[,9]- sum(xx[,9])/nrow(xx))\nz9&lt;-sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,10]-sum(xx[,10])/nrow(xx)))/t(xx[,10]-sum(xx[,10])/nrow(xxx))%*%(xx[,10] -sum(xx[,10])/nrow(xx))\nz10 &lt;- sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,11]-sum(xx[,11])/nrow(xx)))/t(xx[,11]-sum(xx[,11])/nrow(xxx))%*%(xx[,11] -sum(xx[,11])/nrow(xx))\nz11 &lt;- sum((xx[,1]-sum(xx[,1])/nrow(xx))*(xx[,12]-sum(xx[,12])/nrow(xx)))/t(xx[,12]-sum(xx[,12])/nrow(xx))%*%(xx[,12] -sum(xx[,12])/nrow(xx))\nyy=(names(df)[c(12,13,14,15,16,17,17,19,20,21,22,23)][order(c(z, z2,z3,z4, z5, z6, z7, z8, z9, z10,z11))][1])\nprint(sprintf(\"The best is %s\",yy))\n\n[1] \"The best is energy\""
  },
  {
    "objectID": "lectures/lab1_student_version.html#some-guidelines",
    "href": "lectures/lab1_student_version.html#some-guidelines",
    "title": "Stat151 Coding style",
    "section": "Some guidelines",
    "text": "Some guidelines\nSome guidelines for readable, reliable, and reusable analyses:\n\nDon’t repeat code of any complexity\n\nBad: Copy and paste\nGood: Write a well-documented function\n\nUse descriptive variable names\n\nBad: xx &lt;- df[, c(1,2)]\nGood: df_regressors &lt;- df[, c(1,2)]\n\nUse description function names which are verbs\n\nBad: zzz &lt;- function( ...)\nGood: ComputeRegressionCoefficient &lt;- function( ...)\n\nDocument function inputs, outputs, and purpose\nUse consistent spacing, formatting, and style conventions\nUse comments to describe what you’re doing in ordinary language\n\nBad: mean(x * y)\nPointless: mean(x * y)   # average x and y\nGood: mean(x * y)   # Compute the correlation coefficient\n\nWrite for reusability under small changes in data and task\n\nBad: df[[2]] (fails if the column order changes)\nGood: df[[\"response\"]] (robust to changes in input, also more readable)\n\nSanity check a lot and provide useful error messages\n\nSometimes bad: 1 / y\nCan be better: stopifnot(y != 0); 1 / y\n\nTry not to repeat computation (unless it gets too verbose)\n\nOften bad: (x - mean(x)) * (x - mean(x))\nOften better: x_mean &lt;- mean(x);  (x - x_mean) * (x - x_mean)\n\nBe careful with variable scoping\n\nOften bad: df &lt;- data.frame(...);  ComputeStuff &lt;- function(x) { df[\"x\"] ... }\nOften better: df &lt;- data.frame(...);  ComputeStuff &lt;- function(x, df) { df[\"x\"] ... }\n\nUse tidy data structures when possible\n\n(Note that I don’t recommend combining lines with semicolons, I only did this to fit things in the bullets)\nFor this class it’s not important which style guide you use, but please follow some consistent style.\nGoogle style guide\nHadley style guide\nAWS style guide"
  },
  {
    "objectID": "lectures/Lecture1.html",
    "href": "lectures/Lecture1.html",
    "title": "Course outline",
    "section": "",
    "text": "This is a course about linear models. You probably all know what linear models are already — in short, they are models which fit straight lines through data, possibly high-dimensional data. Every setting we consider in this class will have the following attributes:\n\nA bunch of data points. We’ll index with \\(n = 1, \\ldots, N\\).\nEach datapoint consists of:\n\nA scalar-valued “response” \\(y_n\\)\nA vector-valued “regressor” \\(\\xv_n = (\\x_{n1}, \\ldots, \\x_{nP})\\).\n\n\n\n\n\n\n\n\nNotation\n\n\n\nThroughout the course, I will (almost) always use the letter “x” for regressors and the letter “y” for responses. There will always be \\(N\\) datapoints, and the regressors will be \\(P\\) dimensional. Vectors and matrices will be boldface.\nOf course, I may deviate from this (and any) notation convention by saying so explicitly.\n\n\nWe will be interested in what \\(\\xv_n\\) can tell us about \\(\\y_n\\). This setup is called a “regression problem,” and can be attacked with lots of models, including non-linear models. But we will focus on approaches to this problem that operate via fitting straight lines to the dependence of \\(y_n\\) on \\(\\xv_n\\).\nRelative to a lot of other machine learning or statistical procedures, linear models are relatively easy to analyze and understand. Yet they are also complex enough to exhibit a lot of the strengths and pitfalls of all machine learning and statistics. So really, this is only partly a course about linear models per se. I hope to make it a course about concepts in statistics in machine learning more generally, but viewed within the relatively simple framework of linear models. Some examples that I hope to touch on at least briefly include:\n\nAsymptotics under misspecification\nRegularization\nSparsity\nThe bias / variance tradeoff\nThe influence function\nDomain transfer\nDistributed learning\nConformal inference\nPermutation testing\nBayesian methods\nBenign overfitting\n\nOur results and conclusions will be expressed in formal mathematical statements and in software. For the purpose of this class, I view mathematics and coding as analogous to language, grammar, and style: you need to have a command of these things in order to say something. But the content of this course doesn’t stop and math and conding, just as learning language alone does not give you something to say. Linear models will be a mathematical and computational tool for communicating with and about the real world. Datasets can speak to us in the language of linear models, and we can communicate with other humans through the language of linear models. Learning to communicate effectively in this way is the most important content of this course, and is a skill that will remain relevent whether or not you ever interpret or fit another linear model in your life.\nWhether or not a statistical analysis is “good” cannot be evaluated outside a particular context. Why do we care about the conclusions of this analysis? What will they be used for? Who needs to understand our analysis? What are the consequences of certain kinds of errors? Outside of a classroom, you will probably never encounter a linear model without a real question and context attached to it. I will make a real effort in this class to respect this fact, and always present data in context, to the extent possible within a classroom setting. I hope you will in turn get in the habit of always thinking about the context of a problem, even when going through formal homework and lab exercises. For pedagogical reasons we may have to step into abstract territory at times, but I will make an effort to tie what we learn back to reality, and, in grading we’ll make sure to reward your efforts to do so as well. Just as there is not a “correct” essay in an English class, this will often mean that there are not “correct” analyses for a dataset, even though there are certainly better and worse approaches, as well as unambiguous errors."
  },
  {
    "objectID": "lectures/Lecture1.html#exploratory-data-analysis",
    "href": "lectures/Lecture1.html#exploratory-data-analysis",
    "title": "Course outline",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nSpotify example\nEDA is part of every project (start by plotting your data)\nOften a starting point for more detailed analyses\nAnything goes, but correpsondingly the results may not be that meaningful\nHelpful to have a formal understanding of what regression is doing\nLinear algebra perspective"
  },
  {
    "objectID": "lectures/Lecture1.html#prediction-problems",
    "href": "lectures/Lecture1.html#prediction-problems",
    "title": "Course outline",
    "section": "Prediction problems",
    "text": "Prediction problems\n\nBodyfat example\nHave some pairs of responses and explanatory variables\nGiven new explanatory variables, we want a “best guess” for an unseen response\nWe care about how our model fits the data we have, and how it extrapolates\nThe model itself (i.e., the slopes of the fitted line) doesn’t have much meaning\nCare about uncertainty in and calibration of our prediction\nLoss minimization perspective"
  },
  {
    "objectID": "lectures/Lecture1.html#inference-problems",
    "href": "lectures/Lecture1.html#inference-problems",
    "title": "Course outline",
    "section": "Inference problems",
    "text": "Inference problems\n\nAluminum stress-strain curve example\nWe have a question about the world that can’t be expressed as pure prediction\nSometimes we “reify” our model, even if tentatively\nSometimes has a causal intepretation: if we intervene in some aspect of the world, what will happen?\nWe need some notion of the subjective uncertainty of our estimates\nMaximum likelihood perspective\n\nThese perspectives are highly overlapping, and often a problem doesn’t fit neatly into one or the other. For example, good inference should give good predictions, and inference in a very tentatively reified model is close to exploratory data analysis. Still, I’ll lean on this division to help organize the course conceptually.\nWe can look at these examples in Lecture1_examples.ipynb."
  },
  {
    "objectID": "course_policies.html",
    "href": "course_policies.html",
    "title": "Syllabus and Course Structure",
    "section": "",
    "text": "Objectives\nBy the end of the course, you should be able to\n\nExpress standard regression analyses both mathematically and in R code\nCritically relate the intended use of a regression analysis to its methods and assumptions\nIdentify common practical and conceptual pitfalls of regression analysis, and to improve the analysis when possible\nCommunicate the process and results of a regression analysis simply and clearly for a broad audience, using well-organized prose, reproducible code, and effective data visualizations.\n\n\n\nAssignments, Exams, and Grading\n\nAttendance\nAttendance in lectures will be required and will contribute to the participation portion of the students’ grade. Laptops will not be permitted in lectures, and violation of this policy can constitute an absence for the purpose of the participation grade.\nEach student will be given four lecture absences without losing any participation points, with the expectation that these absences will be used for illness and emergencies.\nAttendance at labs will be optional.\n\n\nGrading.\nThe weighting for the grades will be:\n\nHomework completion (each weighted equally): 16%\nHomework correctness (each weighted equally): 4%\nQuizzes (each weighted equally): 20%\nFinal exam: 20%\nGroup project: 20%\nParticipation (primarily lecture attendance): 10%\n\nGrades will not be curved except where otherwise noted. Letter grades will be assigned according the weighted points earned. A score within [90-92%) will earn an A-, [92-98%) will earn an A, and [99-100%) will earn an A+. Scores in the 80’s will receive B’s, in the 70’s will receive C’s, with the same thresholds for plusses and minuses. Scores below 70% will be considered failing. Grades will be non-negotiable.\n\n\nFinal exam.\nAn in-person pencil-and-paper final exam will be scheduled during the usual final exam week.\n\n\nQuizzes.\nEvery two weeks we will have an twenty-minute in-class quiz, typically on the Tuesday following a homework due date. These quizzes will take the place of a sitdown midterm exam (i.e., there will be no midterm).\n\n\nHomework.\nHomework assignments will be published on Fridays and due two weeks later. Homework will typically consist of a combination of mathematical problems and data analysis in R. All homework will be due as a pdf via Gradescope unless otherwise noted.\nThe purpose of homework is for students to attempt to work through problems on their own, both to advance their own understanding, and to allow the instructors to monitor student learning. Neither of these objectives are served if students are copying answers. For that reason, thoughtful and complete homework answers will receive nearly full credit (80% of the available homework points) even if incorrect. We strongly encourage students to submit their own best efforts, even if imperfect, rather than copy a correct answer.\n\n\nFinal group project.\nStudents will form groups of up to three people to submit a final project consisting of an analysis of a real dataset applying principles and techniques from the course.\n\n\nTurning-in assignments\nYou will be turning in your assignments on a platform called Gradescope. This is also the platform where your assignments will be graded, so you can return there to get feedback on your work. You are welcome to file a regrade request if you notice that we made an error in applying the rubric to your work, but be sure to do so within a week of the grades being posted. We will not accept regrade requests past that point.\nIn order to provide flexibility around emergencies that might arise for you throughout the semester (for example, missing a quiz due to COVID), we will apply for everyone:\n\none emergency drop for quizzes\n\ntwo emergency drops for homework\n\nThis means that we will drop your lowest quiz score (which would be a 0 if you were absent) before computing your quiz average. For homework, we will drop your two lowest. Unless students are excused by official university policies, additional drops will not be given.\nWe strongly recommend that students reserve their emergency drops for real emergencies.\n\n\nLate Work\nLate work will not be accepted. If work is not submitted on time, it will receive a zero.\nIt is entirely the students’ responsibility to turn work in on time.\n\n\n\nPrerequisites\nThis course will assume familiarity with the material in STAT 135 or STAT 102. STAT 135 implies other prerequisite courses (STAT 134 and its prerequisites). In particular, you must have had linear algebra, so you should be familiar with basic matrix operations, vector subspaces and projections, rank and invertibility of matrices, and quadratic forms.\nThis semester of Stat151A will include labs and projects in the R language. Proficiency with R at the level of the is a prerequisite. Students with a strong background in another programming language (e.g. Python) will be permitted to enroll with the understanding that they will learn R on their own prior to the start of the class.\n\n\nMaterials\nUnelss otherwise noted, the primary materials for the course are the lecture notes, which will be posted to the course website in advance of class. The following textbooks are useful supplementary texts and are all freely available online:\n\nLinear Models and Extensions Ding\nRegression and other Stories Gelman, Hill, Vehtari\nAn Introduction to Statistical Learning James, Witten, Hastie, Tibshirani\nR for Data Science, Wickham, Grolemund\n\n\nRStudio\nThe software that we’ll be using for our data analysis is the free and open-source language called R that we’ll be interacting with via software called RStudio. If you have difficulty installing RStudio, please reach out to an instructor.\n\n\nCourse website\nAll of the assignments will be posted to the course website at https://stat151a.berkeley.edu/spring-2024/. This also holds the course notes, the syllabus, and links to Gradescope and RStudio.\n\n\n\nPolicies\n\nCourse Culture\nStudents taking STAT151A come from a wide range of backgrounds. We hope to foster an inclusive and supportive learning environment based on curiosity rather than competition. All members of the course community—the instructor, students, tutors, and readers—are expected to treat each other with courtesy and respect.\nYou will be interacting with course staff and fellow students in several different environments: in class, over the discussion forum, and in office hours. Some of these will be in person, some of them will be online, but the same expectations hold: be kind, be respectful, be professional.\nIf you are concerned about classroom environment issues created by other students or course staff, please come talk to the instructors about it.\n\n\nCollaboration policy\nYou are encouraged to collaborate with your fellow students on problem sets and labs, but the work you turn in should reflect your own understanding and all of your collaborators must be cited. The individual component of quizzes, reading questions, and exams must reflect only your work.\nResearchers don’t use one another’s research without permission; scholars and students always use proper citations in papers; professors may not circulate or publish student papers without the writer’s permission; and students may not circulate or post non-public materials (quizzes, exams, rubrics-any private class materials) from their class without the written permission of the instructor.\nThe general rule: you must not submit assignments that reflect the work of others unless they are a cited collaborator.\nThe following examples of collaboration are allowed and in fact encouraged!\n\nDiscussing how to solve a problem with a classmate.\nShowing your code to a classmate along with an error message or confusing output.\nPosting snippets of your code to the discussion forum when seeking help.\nHelping other students solve questions on the discussion with conceptual pointers or snippets of code that doesn’t whole hog give away the answer.\nGoogling the text of an error message.\nCopying small snippets of code from answers on Stack Overflow.\n\nThe following examples are not allowed:\n\nLeaving a representation of your assignment (the text, a screenshot) where students (current and future) can access it. Examples of this include websites like course hero, on a group text chain, over discord/slack, or in a file passed on to future students.\nAccessing and submitting solutions to assignments from other students distributed as above. This includes copying written answers from other students and slightly modifying the language to differentiate it.\nSearching or using generative AI to produce complete problem solutions.\nWorking on the final exam or individual quizzes in collaboration with other people or resources. These assignments must reflect individual work.\nSubmitting work on an exam that reflects consultation with outside resources or other people. Exams must reflect individual work.\n\nIf you have questions about the boundaries of the policy, please ask. We’re always happy to clarify.\n\n\nViolations of the collaboration policy\nThe integrity of our course depends on our ability to ensure that students do not violate the collaboration policy. We take this responsibility seriously and forward cases of academic misconduct to the Center for Student Conduct.\nStudents determined to have violated the academic misconduct policy by the Center for Student Conduct will receive a grade penalty in the course and a sanction from the university which is generally: (i) First violation: Non-Reportable Warning and educational intervention, (ii) Second violation: Suspension/Disciplinary Probation and educational interventions, (iii) Third violation: Dismissal.\nAnd again, if you have questions about the boundaries of the collaboration policy, please ask!\n\n\nLaptop policy\nLaptops will not be permitted in lecture, but will be required for labs.\nIf you do not have access to a laptop, you can borrow one from the University library. See the UC Berkeley hardware lending program for more details. The Student Technology Equity Program is another good resource. Feel free to contact the instructor if you have concerns about your access to needed technology.\n\n\nCOVID policy\nMaintaining your health and that of the Berkeley community is of primary importance to course staff, so if you are feeling ill or have been exposed to illness, please do not come to class. All of the materials used in class will be posted to the course website. You’re encouraged to reach out to fellow students to discuss the class materials or stop by group tutoring or office hours to chat with a tutor or the instructor.\n\n\nAccomodations for students with disabilities\nStat 151A is a course that is designed to allow all students to succeed. If you have letters of accommodations from the Disabled Students’ Program, please share them with your instructor as soon as possible, and we will work out the necessary arrangements.\n\n\n\n\n\n\nNote\n\n\n\nThese course polices are based on a template and text generously shared by Andrew Bray. Thanks, Andrew!"
  },
  {
    "objectID": "assignments/hw1_math.html",
    "href": "assignments/hw1_math.html",
    "title": "STAT151A Homework 1: Due Jan 26th",
    "section": "",
    "text": "1 Simple regression in matrix form\nConsider the simple linear model \\(y_n = \\beta_0 + \\beta_1 z_n + \\varepsilon_n\\).\nLet \\(\\bar{y}:= \\frac{1}{N} \\sum_{n=1}^Ny_n\\) and \\(\\bar{z}:= \\frac{1}{N} \\sum_{n=1}^Nz_n\\). Recall that the ordinary least squares estimates are given by \\[\n\\begin{aligned}\n    \\hat{\\beta}_1 = \\frac{\\frac{1}{N} \\sum_{n=1}^N(z_n - \\bar{z}) (y- \\bar{y})}{\\frac{1}{N} \\sum_{n=1}^N(z_n - \\bar{z})^2}\n    \\quad\\textrm{and}\\quad\n    \\hat{\\beta}_0 = \\bar{y}- \\hat{\\beta}_1 \\bar{z}.\n\\end{aligned}\n\\]\n\n(a)\nWrite the regression in matrix form, where \\(\\boldsymbol{X}\\) is an \\(N \\times 2\\) matrix, \\(\\boldsymbol{\\beta}= (\\beta_0, \\beta_1)^\\intercal\\), and \\(\\boldsymbol{Y}= \\boldsymbol{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}\\).\n\n\n(b)\nDefine \\[\n\\begin{aligned}\n    m_{z^2} := \\frac{1}{N} \\sum_{n=1}^Nz_n^2\n    \\quad\\textrm{and}\\quad\n    m_{zy} := \\frac{1}{N} \\sum_{n=1}^Nz_n y_n\n\\end{aligned}\n\\]\nWrite an explict expressions for \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\), \\(\\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\), and \\(\\left(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1}\\), all in terms of \\(\\bar{y}\\), \\(\\bar{z}\\), \\(m_{z^2}\\), \\(m_{zy}\\), and \\(N\\). Verify that the inverse is correct by direct multiplication.\n\n\n(c)\nCompute \\((\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\). Show that the first row is equal to \\(\\hat{\\beta}_0\\) and the second row is equal to \\(\\hat{\\beta}_1\\).\n\n\n\n2 Mean zero residuals.\nConsider the model \\(y_n = \\beta z_n + \\varepsilon_n\\).\n\n(a)\nSuppose \\(z_n\\) is not a constant. Is it necessarily the case that \\(\\frac{1}{N} \\sum_{n=1}^N\\hat{\\varepsilon}_n = 0\\)? Prove your answer.\n\n\n(b)\nSuppose \\(z_n\\) is a constant, but \\(z_n \\equiv 5\\) for every \\(n \\in \\{1, \\ldots, N\\}\\). Is it necessarily the case that \\(\\frac{1}{N} \\sum_{n=1}^N\\hat{\\varepsilon}_n = 0\\)? Prove your answer.\n\n\n(c)\nNow the model \\(y_n = \\beta_1 z_{n1} + \\beta_2 z_{n2} + \\varepsilon_n\\). Suppose that \\(z_{n1} = 1\\) is \\(n\\) is even, and is \\(0\\) otherwise. Similarly, suppose that \\(z_{n1} = 1\\) is \\(n\\) is odd, and is \\(0\\) otherwise. Is it necessarily the case that \\(\\frac{1}{N} \\sum_{n=1}^N\\hat{\\varepsilon}_n = 0\\)? Prove your answer.\n\n\n\n3 Inner products and covariances\nLet \\(\\boldsymbol{z}= (z_1, \\ldots, z_N)\\) and \\(\\boldsymbol{y}= (y_1, \\ldots, y_N)\\). Let \\(\\boldsymbol{X}\\) denote an \\(N \\times P\\) matrix whose \\(n\\)–th row is the \\(P\\)-vector \\(\\boldsymbol{x}_n\\).\n\n(a)\nLet \\(\\mathcal{Z}'\\) denote a random variable which is produced by drawing an element with replacement from the set \\(\\{z_1, \\ldots, z_N \\}\\) with equal probability given to each entry. This random variables is called a draw from the “empirical distribution” over the set \\(\\{z_1, \\ldots, z_N \\}\\). Similarly, let \\(\\mathcal{Y}'\\) denote a draw from the empirical distribution over the entries of the vector \\(\\boldsymbol{y}\\).\nProve that \\(\\frac{1}{N} \\boldsymbol{z}^\\intercal\\boldsymbol{y}= \\underset{}{\\mathbb{E}}\\left[\\mathcal{Z}' \\mathcal{Y}'\\right]\\). Then prove that \\(\\frac{1}{N} \\boldsymbol{1}^\\intercal\\boldsymbol{z}= \\underset{}{\\mathbb{E}}\\left[\\mathcal{Z}'\\right]\\) as a special case.\n\n\n(b)\nNow suppose that the entries of \\(\\boldsymbol{z}\\) are independent and identically distributed (IID) realizations of the random variable \\(\\mathcal{Z}\\), and that the entries of \\(\\boldsymbol{y}\\) are similarly IID realizations of a random variable \\(\\mathcal{Y}\\). Assuming that \\(\\underset{}{\\mathbb{E}}\\left[|\\mathcal{Z}|\\right] &lt; \\infty\\) and \\(\\underset{}{\\mathbb{E}}\\left[|\\mathcal{Y}|\\right] &lt; \\infty\\), prove that\n\\[\n\\lim_{N\\rightarrow\\infty} \\frac{1}{N} \\boldsymbol{z}^\\intercal\\boldsymbol{y}\\rightarrow\n    \\underset{}{\\mathbb{E}}\\left[\\mathcal{Z} \\mathcal{Y}\\right],\n\\]\nwhere the limit is with probability one (“almost sure” convergence). (Hint: don’t prove this from scratch, appeal to a probability theorem.)\n\n\n(c)\nUsing only inner products involving \\(\\boldsymbol{y}\\), \\(\\boldsymbol{z}\\), and \\(\\boldsymbol{1}\\), write an expression for \\(\\underset{}{\\mathrm{Cov}}\\left(\\mathcal{Y}', \\mathcal{Z}'\\right)\\). Prove that the expression converges with probability one to \\(\\underset{}{\\mathrm{Cov}}\\left(\\mathcal{Y}, \\mathcal{Z}\\right)\\). (Hint: again, use your previous results and a theorem from probability.)\n\n\n(d)\nNow, let \\(\\mathcal{X}'\\) denote a draw from the empirical distribution over the rows of \\(\\boldsymbol{X}\\). Prove that\n\\[\n\\begin{align*}\n\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}= \\underset{}{\\mathbb{E}}\\left[\\mathcal{X}' \\mathcal{X}'^\\intercal\\right]\n\\quad\\textrm{and}\\quad\n\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}= \\underset{}{\\mathbb{E}}\\left[\\mathcal{X}' \\mathcal{Y}'\\right].\n\\end{align*}\n\\]\n\n\n(e)\nNow, suppose that rows of \\(\\boldsymbol{X}\\) are IID realizations of the random \\(P\\)–vector \\(\\mathcal{X}\\), and that \\(\\underset{}{\\mathbb{E}}\\left[|\\mathcal{X}_p|\\right] &lt; \\infty\\) for each \\(p \\in \\{ 1, \\ldots, P \\}\\). Assume, as above, that \\(\\underset{}{\\mathbb{E}}\\left[|\\mathcal{Y}|\\right] &lt; \\infty\\).\nProve that\n\\[\n\\lim_{N\\rightarrow\\infty} \\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\rightarrow\n    \\underset{}{\\mathbb{E}}\\left[\\mathcal{X} \\mathcal{X}^\\intercal\\right]\n\\quad\\textrm{and}\\quad\n\\lim_{N\\rightarrow\\infty} \\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\rightarrow\n    \\underset{}{\\mathbb{E}}\\left[\\mathcal{X} \\mathcal{Y}\\right],\n\\]\nwhere both limits are with probability one.\n\n\n(f)\nNow assume that, for each \\(p \\in \\{1, \\ldots, P\\}\\) and \\(q \\in \\{1, \\ldots, P\\}\\), \\(\\underset{}{\\mathbb{E}}\\left[\\left|\\mathcal{X}'_p\\right| \\left|\\mathcal{X}'_q\\right| \\mathcal{Y}^2\\right] &lt; \\infty\\). Prove that\n\\[\n\\lim_{N\\rightarrow\\infty} \\frac{1}{\\sqrt{N}}\n\\left( \\boldsymbol{X}^\\intercal\\boldsymbol{Y}- \\underset{}{\\mathbb{E}}\\left[\\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\right] \\right) \\rightarrow \\mathcal{Z},\n\\]\nwhere \\(\\mathcal{Z}\\) is a multivariate normal random variable, and the limit is in distribution. What is the covariance of \\(\\mathcal{Z}\\)? (Hint: again, appeal to a probability theorem.)"
  },
  {
    "objectID": "assignments/hw1_code.html",
    "href": "assignments/hw1_code.html",
    "title": "STAT151A Lab 1: Due January 26th",
    "section": "",
    "text": "For all questions below, provide answers in complete sentences, and include correct and readable code to support your answers.\n\n1 Spotify dataset\n\n(a)\nFind another variable (other than danceability) that is associated with popularity according to linear regression.\n\n\n(b)\nHow does this association change if you remove low-popularity tracks?\n\n\n(c)\nIdentify a song that defies the relationship you found. (For example, having found a positive relationship between danceability and popularity, I might find a song that is highly popular but not ``danceable.’’)\nListen to the song on Spotify and comment on whether the result makes sense.\n\n\n\n2 Bodyfat dataset\n\n(a)\nChoose two variables (other than bodyfat). Use lm to regress bodyfat on these two variables.\n\n\n(b)\nFor the regression in the previous example, construct your own \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Y}\\) matrices by hand (don’t use the output of lm). Using these, compute your own estimate \\(\\hat{\\beta}\\) and confirm that it matches the output of lm.\n\n\n(c)\nWrite a function in R that computes \\(\\hat{\\beta}\\) from \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Y}\\). Document the function’s inputs and outputs. As an example, you might follow the Function Documentation section of the Amazon R style guide.\n\n\n\n3 Aluminum dataset\n\n(a)\nRun the regression from Lecture 1 using all three specimens, both with and without an intercept term. Plot the results.\nFor convenience, here is a filter function to limit to the right set of data:\nfilter(Strain &lt; 0.0035, Strain &gt; 0.0001,\n       loading_type == \"T\", temp == 20, lot == \"A\")\nComment on whether an intercept should be included and why. When the intercept is estimated, how can it be interpreted?"
  },
  {
    "objectID": "assignments/assignments.html",
    "href": "assignments/assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Due Jan 26th:\n\nHomework 1 (math component)\nHomework 1 (code component)"
  },
  {
    "objectID": "lectures/macros.html",
    "href": "lectures/macros.html",
    "title": "",
    "section": "",
    "text": "$$\n% Note that bm and boldface do not work.\n$$"
  },
  {
    "objectID": "lectures/lectures.html",
    "href": "lectures/lectures.html",
    "title": "Lectures and Labs",
    "section": "",
    "text": "Lecture Jan 16th:\n\nLecture\nCode examples\n\nLab Jan 17th:\n\nStyle problem"
  },
  {
    "objectID": "lectures/Lecture1_examples.html",
    "href": "lectures/Lecture1_examples.html",
    "title": "Three example datasets for linear modeling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gridExtra)\nlibrary(car)\n\noptions(repr.plot.width = 15, repr.plot.height = 8, repr.plot.res = 300)\n\n── Attaching core tidyverse packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: ‘gridExtra’\n\n\nThe following object is masked from ‘package:dplyr’:\n\n    combine\n\n\nLoading required package: carData\n\n\nAttaching package: ‘car’\n\n\nThe following object is masked from ‘package:dplyr’:\n\n    recode\n\n\nThe following object is masked from ‘package:purrr’:\n\n    some"
  },
  {
    "objectID": "lectures/Lecture1_examples.html#motivation",
    "href": "lectures/Lecture1_examples.html#motivation",
    "title": "Three example datasets for linear modeling",
    "section": "Motivation",
    "text": "Motivation\nMeasuring bodyfat precisely is hard, and proxies are very useful. Can we predict the outcome of an expensive, inconvenient procedure using only easy-to-gather measurements?\n\n\n\nHydrostatic weighing\n\n\nImage from https://www.topendsports.com/testing/tests/underwater.htm\n\nbodyfat_df &lt;- read.csv(\"datasets/bodyfat.csv\")\nhead(bodyfat_df)\nnrow(bodyfat_df)\n\n\nA data.frame: 6 × 15\n\n\n\nDensity\nbodyfat\nAge\nWeight\nHeight\nNeck\nChest\nAbdomen\nHip\nThigh\nKnee\nAnkle\nBiceps\nForearm\nWrist\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1.0708\n12.3\n23\n154.25\n67.75\n36.2\n93.1\n85.2\n94.5\n59.0\n37.3\n21.9\n32.0\n27.4\n17.1\n\n\n2\n1.0853\n6.1\n22\n173.25\n72.25\n38.5\n93.6\n83.0\n98.7\n58.7\n37.3\n23.4\n30.5\n28.9\n18.2\n\n\n3\n1.0414\n25.3\n22\n154.00\n66.25\n34.0\n95.8\n87.9\n99.2\n59.6\n38.9\n24.0\n28.8\n25.2\n16.6\n\n\n4\n1.0751\n10.4\n26\n184.75\n72.25\n37.4\n101.8\n86.4\n101.2\n60.1\n37.3\n22.8\n32.4\n29.4\n18.2\n\n\n5\n1.0340\n28.7\n24\n184.25\n71.25\n34.4\n97.3\n100.0\n101.9\n63.2\n42.2\n24.0\n32.2\n27.7\n17.7\n\n\n6\n1.0502\n20.9\n24\n210.25\n74.75\n39.0\n104.5\n94.4\n107.8\n66.0\n42.0\n25.6\n35.7\n30.6\n18.8\n\n\n\n\n\n252\n\n\n\nvars &lt;- c(\"Weight\", \"Height\", \"Neck\", \"Chest\", \"Abdomen\", \"Hip\", \n          \"Thigh\", \"Knee\", \"Ankle\", \"Biceps\", \"Forearm\", \"Wrist\")\nstopifnot(all(vars %in% names(bodyfat_df)))\n\ngrid.arrange(\n    ggplot(bodyfat_df) + geom_point(aes(x=Abdomen, y=bodyfat)),\n    ggplot(bodyfat_df) + geom_point(aes(x=Height, y=bodyfat)),\n    ggplot(bodyfat_df) + geom_point(aes(x=Weight, y=bodyfat)),\n    ncol=3)\n\n\n\n\n\nregressors &lt;- paste(vars, collapse=\" + \")\nreg_form &lt;- formula(paste0(\"bodyfat ~ 1 + \", regressors))\nreg &lt;- lm(reg_form, bodyfat_df)\nsummary(reg)\n\nggplot() +\n    geom_point(aes(x=bodyfat_df$bodyfat, y=reg$fitted.values)) +\n    geom_abline(aes(slope=1, intercept=0)) +\n    xlab(\"Actual bodyfat\") + ylab(\"Predicted bodyfat\")\n\n\nCall:\nlm(formula = reg_form, data = bodyfat_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4341  -3.0551  -0.0158   3.1951   9.7682 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -19.54803   17.43111  -1.121   0.2632    \nWeight       -0.10974    0.05266  -2.084   0.0382 *  \nHeight       -0.09410    0.09569  -0.983   0.3264    \nNeck         -0.42995    0.23280  -1.847   0.0660 .  \nChest        -0.01728    0.09964  -0.173   0.8625    \nAbdomen       1.02953    0.07761  13.266   &lt;2e-16 ***\nHip          -0.22995    0.14626  -1.572   0.1172    \nThigh         0.13476    0.13510   0.997   0.3195    \nKnee          0.13187    0.23554   0.560   0.5761    \nAnkle         0.12974    0.22150   0.586   0.5586    \nBiceps        0.20538    0.17163   1.197   0.2326    \nForearm       0.38964    0.19756   1.972   0.0497 *  \nWrist        -1.27227    0.50602  -2.514   0.0126 *  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.329 on 239 degrees of freedom\nMultiple R-squared:  0.7452,    Adjusted R-squared:  0.7324 \nF-statistic: 58.24 on 12 and 239 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 151A: Linear Models",
    "section": "",
    "text": "Gradescope\n  RStudio\n  BCourses\n\nNo matching items"
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Statistics 151A: Linear Models",
    "section": "Instructors",
    "text": "Instructors\n\n\nInstructor: Ryan Giordano  Office: 389 Evans Hall Office hours: 9:30am-11am Mondays (subject to change) rgiordano@berkeley.edu pronouns: He / him\n\n\nGSI: Dohyeong Ki  Office: TBD Evans Hall Office hours: 3:30pm-5:30pm Thursdays (subject to change) dohyeong_ki@berkeley.edu pronouns: He / him\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDue to illness, Jan 16th’s class will be held on Zoom. For the link, look for an email from rgiordano@berkeley.edu."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Statistics 151A: Linear Models",
    "section": "Schedule",
    "text": "Schedule\nLectures will be held Jan 16 2024 – May 03 2024 on Tuesday and Thursday, 9:30 am – 10:59 am, in Etcheverry 3108.\nLabs will be held on Wednesdays from 9:00 am – 11:00 am and 1:00pm – 3:00 pm in Evans 344.\n(Link to official course calendar.)\nThe following subject schedule is tentative and subject to change.\n\n\n   Week 1\n   \n   \n   \n   \n           \n           Jan 16:\n           Lecture 1 Why study linear models?  Class goals and organization.\n                \n           \n           \n           Jan 17:\n           Lab 1 R basics.  Loading data, making plots, style best practices\n                \n           \n           \n           Jan 18:\n           Lecture 2 Simple regression as EDA, prediction, and inference\n                \n           \n   \n   \n   Week 2\n   \n   \n   \n   \n           \n           Jan 23:\n           Lecture 3 Linear algebra review.  Matrix inverses, square roots, projections\n                \n           \n           \n           Jan 24:\n           Lab 2 Simulating data.  Matrix square roots.\n                \n           \n           \n           Jan 25:\n           Lecture 4 Probability review.  Consistency, unbiasedness, CLT\n                \n           \n   \n   \n   Week 3\n   \n   \n   \n   \n           \n           Jan 30:\n           Lecture 5 Multivariate regression as empirical risk minimization.  \n                \n           \n           \n           Jan 31:\n           Lab 3 Train and test error as number of regressors increases\n                \n           \n           \n           Feb 1:\n           Lecture 6 Train / test split and out of sample error\n                \n           \n   \n   \n   Week 4\n   \n   \n   \n   \n           \n           Feb 6:\n           Lecture 7 Multivariate regression as maximum likelihood estimation\n                \n           \n           \n           Feb 7:\n           Lab 4 Optimziation\n                \n           \n           \n           Feb 8:\n           Lecture 8 Analysis of variance\n                \n           \n   \n   \n   Week 5\n   \n   \n   \n   \n           \n           Feb 13:\n           Lecture 9 Frequentist inference.  Level, power, testing, and confidence intervals\n                \n           \n           \n           Feb 14:\n           Lab 5 Bootstrapping\n                \n           \n           \n           Feb 15:\n           Lecture 10 Grouping data in the bootstrap\n                \n           \n   \n   \n   Week 6\n   \n   \n   \n   \n           \n           Feb 20:\n           Lecture 11 Asymptotics of empirical risk minimization.  Sandwich covariance\n                \n           \n           \n           Feb 21:\n           Lab 6 Conformal inference\n                \n           \n           \n           Feb 22:\n           Lecture 12 Calibration and conformal inference for prediction\n                \n           \n   \n   \n   Week 7\n   \n   \n   \n   \n           \n           Feb 27:\n           Lecture 13 How regression fails: Omitted variables, endogeneity\n                \n           \n           \n           Feb 28:\n           Lab 7 TBD\n                \n           \n           \n           Feb 29:\n           Lecture 14 How regression fails: Domain changes, Simpson’s paradox\n                \n           \n   \n   \n   Week 8\n   \n   \n   \n   \n           \n           Mar 5:\n           Lecture 15 How regression fails: Outliers and leverage\n                \n           \n           \n           Mar 6:\n           Lab 8 TBD\n                \n           \n           \n           Mar 7:\n           Lecture 16 How regression fails: Model reification\n                \n           \n   \n   \n   Week 9\n   \n   \n   \n   \n           \n           Mar 12:\n           Lecture 17 Binary regression for prediction\n                \n           \n           \n           Mar 13:\n           Lab 9 Fitting regressions with rstanarm\n                \n           \n           \n           Mar 14:\n           Lecture 18 Binary regression for inference\n                \n           \n   \n   \n   Week 10\n   \n   \n   \n   \n           \n           Mar 19:\n           Lecture 19 Bayesian inference: multivariate normal\n                \n           \n           \n           Mar 20:\n           Lab 10 Inference failure for variable selection\n                \n           \n           \n           Mar 21:\n           Lecture 20 Bayesian inference: regression and shrinkage\n                \n           \n   \n   \n   Week 11\n   \n   \n   \n   \n           \n           Mar 25:\n           Holiday  Spring break\n                \n           \n   \n   \n   Week 12\n   \n   \n   \n   \n           \n           Apr 2:\n           Lecture 21 Variable selection: collinearity and shrinkage\n                \n           \n           \n           Apr 3:\n           Lab 11 Using lme4\n                \n           \n           \n           Apr 4:\n           Lecture 22 Variable selection: LASSO and post-selection inference\n                \n           \n   \n   \n   Week 13\n   \n   \n   \n   \n           \n           Apr 9:\n           Lecture 23 Random effects and hierarchical modeling\n                \n           \n           \n           Apr 10:\n           Lab 12 Project office hours\n                \n           \n           \n           Apr 11:\n           Lecture 24 Random effects and hierarchical modeling\n                \n           \n   \n   \n   Week 14\n   \n   \n   \n   \n           \n           Apr 16:\n           Lecture 25 Basis expansions: Bias and variance\n                \n           \n           \n           Apr 17:\n           Lab 13 TBD\n                \n           \n           \n           Apr 18:\n           Lecture 26 Basis expansions: Interpolation and double descent\n                \n           \n   \n   \n\nNo matching items"
  }
]