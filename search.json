[
  {
    "objectID": "lectures/Lecture6_code.html",
    "href": "lectures/Lecture6_code.html",
    "title": "Body fat linear combinations",
    "section": "",
    "text": "Measuring bodyfat precisely is hard, and proxies are very useful. Can we predict the outcome of an expensive, inconvenient procedure using only easy-to-gather measurements?\n\n\n\nHydrostatic weighing\n\n\nImage from https://www.topendsports.com/testing/tests/underwater.htm\n\nall_vars &lt;- c(\"Weight\", \"Height\", \"Neck\", \"Chest\", \"Abdomen\", \"Hip\", \n              \"Thigh\", \"Knee\", \"Ankle\", \"Biceps\", \"Forearm\", \"Wrist\")\nstopifnot(all(all_vars %in% names(bodyfat_df)))\n\n\ngrid.arrange(\n    ggplot(bodyfat_df) + geom_point(aes(x=Abdomen, y=bodyfat)),\n    ggplot(bodyfat_df) + geom_point(aes(x=Height, y=bodyfat)),\n    ggplot(bodyfat_df) + geom_point(aes(x=Weight, y=bodyfat)),\n    ncol=3)\n\n\n\n\n\n\n\n\n\nPlotFit &lt;- function(reg) {\n    ggplot() +\n        geom_point(aes(x=reg$model$bodyfat, y=reg$fitted.values)) +\n        geom_abline(aes(slope=1, intercept=0)) +\n        xlab(\"Actual bodyfat\") + ylab(\"Predicted bodyfat\")\n}\n\n\n\nreg &lt;- lm(bodyfat ~ Abdomen + Height + Weight, bodyfat_df)\nprint(sprintf(\"Error: %f\", mean(reg$residuals^2)))\n#PlotFit(reg)\n\n[1] \"Error: 19.456161\"\n\n\n\n\n\n\n\n\n\n\nprint(reg$coefficients)\n\n(Intercept)     Abdomen      Height      Weight \n-36.6147193   0.9515631  -0.1270307  -0.1307606 \n\n\n\nbodyfat_df &lt;- bodyfat_df %&gt;%\n    mutate(height_norm=(Height - mean(Height)) / sd(Height),\n           weight_norm=(Weight - mean(Weight)) / sd(Weight),\n           hw_diff=height_norm - weight_norm)\n\nreg &lt;- lm(bodyfat ~ Abdomen + Height + Weight, bodyfat_df)\nreg_norm &lt;- lm(bodyfat ~ Abdomen + height_norm + weight_norm, bodyfat_df)\nreg_diff &lt;- lm(bodyfat ~ Abdomen + height_norm + weight_norm + hw_diff, bodyfat_df)\n\ncat(sprintf(\"Original error:\\t\\t\\t\\t%f\\n\", mean(reg$residuals^2)))\ncat(sprintf(\"Normalized regression error:\\t\\t%f\\n\", mean(reg_norm$residuals^2)))\ncat(sprintf(\"Normalized regression difference error:\\t%f\\n\", mean(reg_norm$residuals^2)))\n\nOriginal error:             19.456161\nNormalized regression error:        19.456161\nNormalized regression difference error: 19.456161\n\n\n\nsummary(reg_norm)\n\n\nCall:\nlm(formula = bodyfat ~ Abdomen + height_norm + weight_norm, data = bodyfat_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.4481  -3.1699  -0.0009   3.0830  10.1897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -68.92203    5.79408 -11.895  &lt; 2e-16 ***\nAbdomen       0.95156    0.06253  15.218  &lt; 2e-16 ***\nheight_norm  -0.46530    0.32593  -1.428    0.155    \nweight_norm  -3.84294    0.70602  -5.443 1.26e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.446 on 248 degrees of freedom\nMultiple R-squared:  0.7211,    Adjusted R-squared:  0.7177 \nF-statistic: 213.7 on 3 and 248 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(reg_diff)\n\n\nCall:\nlm(formula = bodyfat ~ Abdomen + height_norm + weight_norm + \n    hw_diff, data = bodyfat_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.4481  -3.1699  -0.0009   3.0830  10.1897 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -68.92203    5.79408 -11.895  &lt; 2e-16 ***\nAbdomen       0.95156    0.06253  15.218  &lt; 2e-16 ***\nheight_norm  -0.46530    0.32593  -1.428    0.155    \nweight_norm  -3.84294    0.70602  -5.443 1.26e-07 ***\nhw_diff            NA         NA      NA       NA    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.446 on 248 degrees of freedom\nMultiple R-squared:  0.7211,    Adjusted R-squared:  0.7177 \nF-statistic: 213.7 on 3 and 248 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/Lecture6_code.html#motivation",
    "href": "lectures/Lecture6_code.html#motivation",
    "title": "Body fat linear combinations",
    "section": "",
    "text": "Measuring bodyfat precisely is hard, and proxies are very useful. Can we predict the outcome of an expensive, inconvenient procedure using only easy-to-gather measurements?\n\n\n\nHydrostatic weighing\n\n\nImage from https://www.topendsports.com/testing/tests/underwater.htm\n\nall_vars &lt;- c(\"Weight\", \"Height\", \"Neck\", \"Chest\", \"Abdomen\", \"Hip\", \n              \"Thigh\", \"Knee\", \"Ankle\", \"Biceps\", \"Forearm\", \"Wrist\")\nstopifnot(all(all_vars %in% names(bodyfat_df)))\n\n\ngrid.arrange(\n    ggplot(bodyfat_df) + geom_point(aes(x=Abdomen, y=bodyfat)),\n    ggplot(bodyfat_df) + geom_point(aes(x=Height, y=bodyfat)),\n    ggplot(bodyfat_df) + geom_point(aes(x=Weight, y=bodyfat)),\n    ncol=3)\n\n\n\n\n\n\n\n\n\nPlotFit &lt;- function(reg) {\n    ggplot() +\n        geom_point(aes(x=reg$model$bodyfat, y=reg$fitted.values)) +\n        geom_abline(aes(slope=1, intercept=0)) +\n        xlab(\"Actual bodyfat\") + ylab(\"Predicted bodyfat\")\n}\n\n\n\nreg &lt;- lm(bodyfat ~ Abdomen + Height + Weight, bodyfat_df)\nprint(sprintf(\"Error: %f\", mean(reg$residuals^2)))\n#PlotFit(reg)\n\n[1] \"Error: 19.456161\"\n\n\n\n\n\n\n\n\n\n\nprint(reg$coefficients)\n\n(Intercept)     Abdomen      Height      Weight \n-36.6147193   0.9515631  -0.1270307  -0.1307606 \n\n\n\nbodyfat_df &lt;- bodyfat_df %&gt;%\n    mutate(height_norm=(Height - mean(Height)) / sd(Height),\n           weight_norm=(Weight - mean(Weight)) / sd(Weight),\n           hw_diff=height_norm - weight_norm)\n\nreg &lt;- lm(bodyfat ~ Abdomen + Height + Weight, bodyfat_df)\nreg_norm &lt;- lm(bodyfat ~ Abdomen + height_norm + weight_norm, bodyfat_df)\nreg_diff &lt;- lm(bodyfat ~ Abdomen + height_norm + weight_norm + hw_diff, bodyfat_df)\n\ncat(sprintf(\"Original error:\\t\\t\\t\\t%f\\n\", mean(reg$residuals^2)))\ncat(sprintf(\"Normalized regression error:\\t\\t%f\\n\", mean(reg_norm$residuals^2)))\ncat(sprintf(\"Normalized regression difference error:\\t%f\\n\", mean(reg_norm$residuals^2)))\n\nOriginal error:             19.456161\nNormalized regression error:        19.456161\nNormalized regression difference error: 19.456161\n\n\n\nsummary(reg_norm)\n\n\nCall:\nlm(formula = bodyfat ~ Abdomen + height_norm + weight_norm, data = bodyfat_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.4481  -3.1699  -0.0009   3.0830  10.1897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -68.92203    5.79408 -11.895  &lt; 2e-16 ***\nAbdomen       0.95156    0.06253  15.218  &lt; 2e-16 ***\nheight_norm  -0.46530    0.32593  -1.428    0.155    \nweight_norm  -3.84294    0.70602  -5.443 1.26e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.446 on 248 degrees of freedom\nMultiple R-squared:  0.7211,    Adjusted R-squared:  0.7177 \nF-statistic: 213.7 on 3 and 248 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(reg_diff)\n\n\nCall:\nlm(formula = bodyfat ~ Abdomen + height_norm + weight_norm + \n    hw_diff, data = bodyfat_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.4481  -3.1699  -0.0009   3.0830  10.1897 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -68.92203    5.79408 -11.895  &lt; 2e-16 ***\nAbdomen       0.95156    0.06253  15.218  &lt; 2e-16 ***\nheight_norm  -0.46530    0.32593  -1.428    0.155    \nweight_norm  -3.84294    0.70602  -5.443 1.26e-07 ***\nhw_diff            NA         NA      NA       NA    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.446 on 248 degrees of freedom\nMultiple R-squared:  0.7211,    Adjusted R-squared:  0.7177 \nF-statistic: 213.7 on 3 and 248 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/Lecture2_examples.html",
    "href": "lectures/Lecture2_examples.html",
    "title": "Three example datasets for linear modeling",
    "section": "",
    "text": "In Lecture XXI of Wittgenstein’s “Lectures on the Foundations of Mathematics” we find this diagram and quote:\nIn the same passage, he goes on to say,\nWhen we run a regression, we must have a point. Otherwise we are just looking at goose entrails.\nlibrary(tidyverse)\nlibrary(gridExtra)\nlibrary(car)\n\noptions(repr.plot.width = 15, repr.plot.height = 8, repr.plot.res = 300)\n\n── Attaching core tidyverse packages ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: ‘gridExtra’\n\n\nThe following object is masked from ‘package:dplyr’:\n\n    combine\n\n\nLoading required package: carData\n\n\nAttaching package: ‘car’\n\n\nThe following object is masked from ‘package:dplyr’:\n\n    recode\n\n\nThe following object is masked from ‘package:purrr’:\n\n    some"
  },
  {
    "objectID": "lectures/Lecture2_examples.html#motivation",
    "href": "lectures/Lecture2_examples.html#motivation",
    "title": "Three example datasets for linear modeling",
    "section": "Motivation",
    "text": "Motivation\nMeasuring bodyfat precisely is hard, and proxies are very useful. Can we predict the outcome of an expensive, inconvenient procedure using only easy-to-gather measurements?\n\n\n\nHydrostatic weighing\n\n\nImage from https://www.topendsports.com/testing/tests/underwater.htm\n\nbodyfat_df &lt;- read.csv(\"../datasets/bodyfat.csv\")\nhead(bodyfat_df)\nnrow(bodyfat_df)\n\n\nA data.frame: 6 × 15\n\n\n\nDensity\nbodyfat\nAge\nWeight\nHeight\nNeck\nChest\nAbdomen\nHip\nThigh\nKnee\nAnkle\nBiceps\nForearm\nWrist\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1.0708\n12.3\n23\n154.25\n67.75\n36.2\n93.1\n85.2\n94.5\n59.0\n37.3\n21.9\n32.0\n27.4\n17.1\n\n\n2\n1.0853\n6.1\n22\n173.25\n72.25\n38.5\n93.6\n83.0\n98.7\n58.7\n37.3\n23.4\n30.5\n28.9\n18.2\n\n\n3\n1.0414\n25.3\n22\n154.00\n66.25\n34.0\n95.8\n87.9\n99.2\n59.6\n38.9\n24.0\n28.8\n25.2\n16.6\n\n\n4\n1.0751\n10.4\n26\n184.75\n72.25\n37.4\n101.8\n86.4\n101.2\n60.1\n37.3\n22.8\n32.4\n29.4\n18.2\n\n\n5\n1.0340\n28.7\n24\n184.25\n71.25\n34.4\n97.3\n100.0\n101.9\n63.2\n42.2\n24.0\n32.2\n27.7\n17.7\n\n\n6\n1.0502\n20.9\n24\n210.25\n74.75\n39.0\n104.5\n94.4\n107.8\n66.0\n42.0\n25.6\n35.7\n30.6\n18.8\n\n\n\n\n\n252\n\n\n\nvars &lt;- c(\"Weight\", \"Height\", \"Neck\", \"Chest\", \"Abdomen\", \"Hip\", \n          \"Thigh\", \"Knee\", \"Ankle\", \"Biceps\", \"Forearm\", \"Wrist\")\nstopifnot(all(vars %in% names(bodyfat_df)))\n\ngrid.arrange(\n    ggplot(bodyfat_df) + geom_point(aes(x=Abdomen, y=bodyfat)),\n    ggplot(bodyfat_df) + geom_point(aes(x=Height, y=bodyfat)),\n    ggplot(bodyfat_df) + geom_point(aes(x=Weight, y=bodyfat)),\n    ncol=3)\n\n\n\n\n\n\n\n\n\nregressors &lt;- paste(vars, collapse=\" + \")\nreg_form &lt;- formula(paste0(\"bodyfat ~ 1 + \", regressors))\nreg &lt;- lm(reg_form, bodyfat_df)\nsummary(reg)\n\nggplot() +\n    geom_point(aes(x=bodyfat_df$bodyfat, y=reg$fitted.values)) +\n    geom_abline(aes(slope=1, intercept=0)) +\n    xlab(\"Actual bodyfat\") + ylab(\"Predicted bodyfat\")\n\n\nCall:\nlm(formula = reg_form, data = bodyfat_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4341  -3.0551  -0.0158   3.1951   9.7682 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -19.54803   17.43111  -1.121   0.2632    \nWeight       -0.10974    0.05266  -2.084   0.0382 *  \nHeight       -0.09410    0.09569  -0.983   0.3264    \nNeck         -0.42995    0.23280  -1.847   0.0660 .  \nChest        -0.01728    0.09964  -0.173   0.8625    \nAbdomen       1.02953    0.07761  13.266   &lt;2e-16 ***\nHip          -0.22995    0.14626  -1.572   0.1172    \nThigh         0.13476    0.13510   0.997   0.3195    \nKnee          0.13187    0.23554   0.560   0.5761    \nAnkle         0.12974    0.22150   0.586   0.5586    \nBiceps        0.20538    0.17163   1.197   0.2326    \nForearm       0.38964    0.19756   1.972   0.0497 *  \nWrist        -1.27227    0.50602  -2.514   0.0126 *  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.329 on 239 degrees of freedom\nMultiple R-squared:  0.7452,    Adjusted R-squared:  0.7324 \nF-statistic: 58.24 on 12 and 239 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/Lecture1_examples.html",
    "href": "lectures/Lecture1_examples.html",
    "title": "Three example datasets for linear modeling",
    "section": "",
    "text": "In Lecture XXI of Wittgenstein’s “Lectures on the Foundations of Mathematics” we find this diagram and quote:\nIn the same passage, he goes on to say,\nWhen we run a regression, we must have a point. Otherwise we are just looking at goose entrails.\nlibrary(tidyverse)\nlibrary(gridExtra)\nlibrary(car)\n\noptions(repr.plot.width = 15, repr.plot.height = 8, repr.plot.res = 300)\n\n── Attaching core tidyverse packages ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: ‘gridExtra’\n\n\nThe following object is masked from ‘package:dplyr’:\n\n    combine\n\n\nLoading required package: carData\n\n\nAttaching package: ‘car’\n\n\nThe following object is masked from ‘package:dplyr’:\n\n    recode\n\n\nThe following object is masked from ‘package:purrr’:\n\n    some"
  },
  {
    "objectID": "lectures/Lecture1_examples.html#motivation",
    "href": "lectures/Lecture1_examples.html#motivation",
    "title": "Three example datasets for linear modeling",
    "section": "Motivation",
    "text": "Motivation\nMeasuring bodyfat precisely is hard, and proxies are very useful. Can we predict the outcome of an expensive, inconvenient procedure using only easy-to-gather measurements?\n\n\n\nHydrostatic weighing\n\n\nImage from https://www.topendsports.com/testing/tests/underwater.htm\n\nbodyfat_df &lt;- read.csv(\"datasets/bodyfat.csv\")\nhead(bodyfat_df)\nnrow(bodyfat_df)\n\n\nA data.frame: 6 × 15\n\n\n\nDensity\nbodyfat\nAge\nWeight\nHeight\nNeck\nChest\nAbdomen\nHip\nThigh\nKnee\nAnkle\nBiceps\nForearm\nWrist\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1.0708\n12.3\n23\n154.25\n67.75\n36.2\n93.1\n85.2\n94.5\n59.0\n37.3\n21.9\n32.0\n27.4\n17.1\n\n\n2\n1.0853\n6.1\n22\n173.25\n72.25\n38.5\n93.6\n83.0\n98.7\n58.7\n37.3\n23.4\n30.5\n28.9\n18.2\n\n\n3\n1.0414\n25.3\n22\n154.00\n66.25\n34.0\n95.8\n87.9\n99.2\n59.6\n38.9\n24.0\n28.8\n25.2\n16.6\n\n\n4\n1.0751\n10.4\n26\n184.75\n72.25\n37.4\n101.8\n86.4\n101.2\n60.1\n37.3\n22.8\n32.4\n29.4\n18.2\n\n\n5\n1.0340\n28.7\n24\n184.25\n71.25\n34.4\n97.3\n100.0\n101.9\n63.2\n42.2\n24.0\n32.2\n27.7\n17.7\n\n\n6\n1.0502\n20.9\n24\n210.25\n74.75\n39.0\n104.5\n94.4\n107.8\n66.0\n42.0\n25.6\n35.7\n30.6\n18.8\n\n\n\n\n\n252\n\n\n\nvars &lt;- c(\"Weight\", \"Height\", \"Neck\", \"Chest\", \"Abdomen\", \"Hip\", \n          \"Thigh\", \"Knee\", \"Ankle\", \"Biceps\", \"Forearm\", \"Wrist\")\nstopifnot(all(vars %in% names(bodyfat_df)))\n\ngrid.arrange(\n    ggplot(bodyfat_df) + geom_point(aes(x=Abdomen, y=bodyfat)),\n    ggplot(bodyfat_df) + geom_point(aes(x=Height, y=bodyfat)),\n    ggplot(bodyfat_df) + geom_point(aes(x=Weight, y=bodyfat)),\n    ncol=3)\n\n\n\n\n\n\n\n\n\nregressors &lt;- paste(vars, collapse=\" + \")\nreg_form &lt;- formula(paste0(\"bodyfat ~ 1 + \", regressors))\nreg &lt;- lm(reg_form, bodyfat_df)\nsummary(reg)\n\nggplot() +\n    geom_point(aes(x=bodyfat_df$bodyfat, y=reg$fitted.values)) +\n    geom_abline(aes(slope=1, intercept=0)) +\n    xlab(\"Actual bodyfat\") + ylab(\"Predicted bodyfat\")\n\n\nCall:\nlm(formula = reg_form, data = bodyfat_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4341  -3.0551  -0.0158   3.1951   9.7682 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -19.54803   17.43111  -1.121   0.2632    \nWeight       -0.10974    0.05266  -2.084   0.0382 *  \nHeight       -0.09410    0.09569  -0.983   0.3264    \nNeck         -0.42995    0.23280  -1.847   0.0660 .  \nChest        -0.01728    0.09964  -0.173   0.8625    \nAbdomen       1.02953    0.07761  13.266   &lt;2e-16 ***\nHip          -0.22995    0.14626  -1.572   0.1172    \nThigh         0.13476    0.13510   0.997   0.3195    \nKnee          0.13187    0.23554   0.560   0.5761    \nAnkle         0.12974    0.22150   0.586   0.5586    \nBiceps        0.20538    0.17163   1.197   0.2326    \nForearm       0.38964    0.19756   1.972   0.0497 *  \nWrist        -1.27227    0.50602  -2.514   0.0126 *  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.329 on 239 degrees of freedom\nMultiple R-squared:  0.7452,    Adjusted R-squared:  0.7324 \nF-statistic: 58.24 on 12 and 239 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/Lecture17_code.html",
    "href": "lectures/Lecture17_code.html",
    "title": "Interpreting the coefficients and R output",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(sandwich)\n\ntheme_update(text = element_text(size=24))\noptions(repr.plot.width=12, repr.plot.height=6)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ninstall &lt;- FALSE\n\n# https://avehtari.github.io/ROS-Examples/examples.html\nif (install) {\n    install.packages(\"rprojroot\")\n    remotes::install_github(\"avehtari/ROS-Examples\", subdir=\"rpackage\")    \n}\n\nlibrary(rosdata)\ndata(kidiq)\n\n\nhead(kidiq)\n\n\nA data.frame: 6 × 5\n\n\n\nkid_score\nmom_hs\nmom_iq\nmom_work\nmom_age\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n65\n1\n121.11753\n4\n27\n\n\n2\n98\n1\n89.36188\n4\n25\n\n\n3\n85\n1\n115.44316\n4\n27\n\n\n4\n83\n1\n99.44964\n3\n25\n\n\n5\n115\n1\n92.74571\n4\n27\n\n\n6\n98\n0\n107.90184\n1\n18\n\n\n\n\n\nLet’s regress the kid’s IQ score on a constant and on whether or not the mom went to high school. This should be interepreted as a comparison between groups, not the effect of “going to high school.”\n\nfit1 &lt;- lm(kid_score ~ mom_hs, kidiq)\nprint(fit1)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs, data = kidiq)\n\nCoefficients:\n(Intercept)       mom_hs  \n      77.55        11.77  \n\n\n\n\nkidiq %&gt;% mutate(pred=fit1$fitted.values) %&gt;%\nggplot() +\n    geom_point(aes(x=mom_hs, y=kid_score)) +\n    geom_point(aes(x=mom_hs, y=pred), color=\"red\", size=5)\n\n\n\n\n\n\n\n\n\n# There are many ways to get estimates out of lm.\nprint(summary(fit1))\n\ncat(\"\\n----------------\\n\")\nprint(summary(fit1)$coefficients)\n\ncat(\"\\n----------------\\n\")\nprint(coefficients(fit1))\n\n\nCall:\nlm(formula = kid_score ~ mom_hs, data = kidiq)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.55 -13.32   2.68  14.68  58.45 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   77.548      2.059  37.670  &lt; 2e-16 ***\nmom_hs        11.771      2.322   5.069 5.96e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 19.85 on 432 degrees of freedom\nMultiple R-squared:  0.05613,   Adjusted R-squared:  0.05394 \nF-statistic: 25.69 on 1 and 432 DF,  p-value: 5.957e-07\n\n\n----------------\n            Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept) 77.54839   2.058612 37.670231 1.392224e-138\nmom_hs      11.77126   2.322427  5.068516  5.956524e-07\n\n----------------\n(Intercept)      mom_hs \n   77.54839    11.77126 \n\n\nWe can interpret the coefficients as the means within samples. Arguably the easiest way to see this is to re-write our regressors. The problem uses \\(x_n^\\trans = (1, i_n)\\) where \\(i_n\\) is \\(1\\) if the mom went to high school and \\(0\\) otherwise. It is somewhat easier to see what the coefficients are estimating if you write the regression in the form of the transformed variable\n\\[\n\\zv_n =\n\\begin{pmatrix}\n1 & -1 \\\\\n0 & 1\n\\end{pmatrix}\n\\xv_n =\n\\begin{pmatrix}\n1 - i_n \\\\\ni_n\n\\end{pmatrix},\n\\]\nsince then\n\\[\n\\Z^\\trans \\Z =\n\\begin{pmatrix}\nN_0 & 0 \\\\\n0 & N_1 \\\\\n\\end{pmatrix},\n\\]\nwhere \\(N_0\\) and \\(N_1\\) are the respective counts of rows of moms without and with high school education. From this it follows that, regressing \\(\\y_n \\sim \\zv_n^\\trans \\gamma\\), that\n\\[\n\\gammahat = \\begin{pmatrix}\\ybar_0 \\\\ \\ybar_1 \\end{pmatrix},\n\\]\nthe means of the response within the two groups. Using the fact that \\(\\zv_n\\) is an invertible transformation of \\(\\xv_n\\), and so the least squares fit is the same at each possible regressor value, we get\n\\[\n\\zv_n^\\trans \\gammahat = \\xv_n^\\trans \\begin{pmatrix}\n1 & 0 \\\\\n-1 & 1\n\\end{pmatrix}\n\\gammahat = \\xv_n^\\trans \\betahat \\quad\\textrm{for all }\\xv_n\\quad \\Rightarrow\n\\]\n\\[\n\\betahat =\n\\begin{pmatrix}\n1 & 0 \\\\\n-1 & 1\n\\end{pmatrix}\\gammahat\n=\n\\begin{pmatrix}\n1 & 0 \\\\\n-1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\\ybar_0 \\\\ \\ybar_1 \\end{pmatrix}\n= \\begin{pmatrix}\n\\ybar_0 \\\\\n\\ybar_1 - \\ybar_0\n\\end{pmatrix}.\n\\]\nSo the least square estimates are just comparing means within groups in this case.\n\n\n\n\n\n\nExercise\n\n\n\nProve that, if \\(\\xv_n\\) takes one of \\(P\\) distinct values, then \\(\\betahat\\) is always a linear combination of the sample means of the response within observations taking the distinct values. Find the linear combination, and prove the present result as a special case.\n\n\n\nmean0 &lt;- mean(kidiq %&gt;% filter(mom_hs == 0) %&gt;% pull(kid_score))\nmean1 &lt;- mean(kidiq %&gt;% filter(mom_hs == 1) %&gt;% pull(kid_score))\ncbind(c(mean0, mean1 - mean0), coefficients(fit1))\n\n\nA matrix: 2 × 2 of type dbl\n\n\n(Intercept)\n77.54839\n77.54839\n\n\nmom_hs\n11.77126\n11.77126\n\n\n\n\n\nLet’s go through and confirm that lm is calculating exactly the same quantites we’ve been looking at in class.\nCheck that the coefficient estimates are given by \\(\\betavhat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\\).\n\nxmat &lt;- model.matrix(fit1)\nyvec &lt;- kidiq$kid_score\n\nxtx &lt;- t(xmat) %*% xmat\nbetahat &lt;- solve(xtx, t(xmat) %*% yvec)\n\ncat(\"\\nThe betahat is the same in lm:\\n\")\nprint(cbind(betahat, coefficients(fit1)))\n\n\nThe betahat is the same in lm:\n                [,1]     [,2]\n(Intercept) 77.54839 77.54839\nmom_hs      11.77126 11.77126\n\n\nCheck that the residuals are given by \\(\\Y - \\X \\betavhat\\).\n\nreshat &lt;- yvec - xmat %*% betahat\ncat(\"\\nThe estimated residuals are the same in lm:\\n\")\nprint(max(abs(reshat - fit1$residuals)))\n\n\nThe estimated residuals are the same in lm:\n[1] 4.902745e-13\n\n\nThe residual estimated variances is given by \\(\\sigmahat^2 = \\frac{1}{N - P} \\sumn \\reshat_n\\).\n\nsigmahat2 &lt;- sum(reshat^2) / (nrow(xmat) - ncol(xmat))\ncat(\"\\nThe residual variance estimates are the same in lm:\\n\")\nprint(sigmahat2)\nprint(sigma(fit1)^2)\n\n\nThe residual variance estimates are the same in lm:\n[1] 394.1231\n[1] 394.1231\n\n\nThe estimated covariance \\(\\cov{\\betahat - \\beta}\\) is given by \\(\\sigmahat^2 (\\X^\\trans\\X)^{-1}\\), the homoskedastic covariance estimate.\n\nbetahat_cov &lt;- sigmahat2 * solve(xtx)\n\ncat(\"\\nThe betahat covariance are the same in lm:\\n\")\nprint(betahat_cov)\nprint(vcov(fit1))\n\n\nThe betahat covariance are the same in lm:\n            (Intercept)    mom_hs\n(Intercept)    4.237883 -4.237883\nmom_hs        -4.237883  5.393669\n            (Intercept)    mom_hs\n(Intercept)    4.237883 -4.237883\nmom_hs        -4.237883  5.393669\n\n\nThe sandwich covariance matrix can also be used, though it’s not the default for lm. Note that the sandwich package has lots of variants, read the documentation for more information.\n\ncat(\"\\nThe sandwich covariances match:\\n\")\nscore_cov &lt;- t(xmat) %*% (diag(as.numeric(reshat)^2) %*% xmat)\nbetahat_sandwich_cov &lt;- solve(xtx, score_cov) %*% solve(xtx)\nprint(betahat_sandwich_cov)\nprint(sandwich::vcovCL(fit1, type=\"HC0\", cadjust=FALSE))\n\n\nThe sandwich covariances match:\n            (Intercept)    mom_hs\n(Intercept)    5.420399 -5.420399\nmom_hs        -5.420399  6.481451\n            (Intercept)    mom_hs\n(Intercept)    5.420399 -5.420399\nmom_hs        -5.420399  6.481451\n\n\nThe reported “standard errors” are the square root of the diagonal of the covariance matrix.\n\ncat(\"\\nThe betahat standard errors are the same in lm:\\n\")\nbetahat_sd &lt;- sqrt(diag(betahat_cov))\nprint(cbind(betahat_sd, summary(fit1)$coefficients[,\"Std. Error\"]))\n\n\nThe betahat standard errors are the same in lm:\n            betahat_sd         \n(Intercept)   2.058612 2.058612\nmom_hs        2.322427 2.322427\n\n\nRecall that, if \\(\\betahat_k - \\beta_k \\sim \\gauss{0, \\sigma^2 v_k}\\), then we have shown that, under the normal assumption,\n\\[\nt_k := \\frac{\\betahat_k - \\beta_k}{\\sigmahat} \\sim \\studentt{N-P}.\n\\]\nIt follows that, if \\(\\prob{t_k \\le -z_\\alpha} = \\alpha / 2\\), then\n\\[\n\\prob{\\beta \\in (\\betahat \\pm z_\\alpha \\sigmahat v_k} = 1 - \\alpha\n\\]\nHere \\(\\sigmahat v_k\\) would just be the “standard error” from above, that is, the square root of the \\(k\\)–th entry of the diagonal of the estimated covariance matrix.\n\nalpha = 0.05\nzalpha &lt;- -1 * qt(alpha / 2, df=nrow(xmat) - ncol(xmat))\nci &lt;- betahat[2] +  betahat_sd[2] * c(-zalpha, zalpha)\n\ncat(\"\\nThe confidence values are the same as lm:\\n\")\nprint(ci)\nprint(confint(fit1, \"mom_hs\", level=1 - alpha))\n\n\nThe confidence values are the same as lm:\n[1]  7.206598 16.335924\n          2.5 %   97.5 %\nmom_hs 7.206598 16.33592\n\n\nThe t-values and p-values are about hypothesis tests for the hypothesis that \\(\\betav = 0\\) using the student-t distribution.\nFirst, the t-values is the statistic \\(t_k\\) defined above for \\(\\beta_k = 0\\). This is how far away from zero the t-statistic would have to be had \\(\\beta_k\\) been equal to \\(0\\).\nThe p-value is then the \\(\\alpha\\) for which you would have included \\(0\\) in your confidence interval. Equivalently, it is the probability of observing \\(t_k\\) as large or larger than what you actually observed, had \\(\\beta_k\\) actually been equal to zero.\n\ncat(\"\\nThe t values are the same in lm:\\n\")\nt_value &lt;- betahat / betahat_sd\nprint(cbind(t_value, summary(fit1)$coefficients[, \"t value\"]))\n\ncat(\"\\nThe p values are the same in lm:\\n\")\ndf &lt;- nrow(xmat) - ncol(xmat)\np_value &lt;- pt(- abs(t_value), df=df) * 2\nprint(cbind(p_value, summary(fit1)$coefficients[, \"Pr(&gt;|t|)\"]))\n\n\nThe t values are the same in lm:\n                 [,1]      [,2]\n(Intercept) 37.670231 37.670231\nmom_hs       5.068516  5.068516\n\nThe p values are the same in lm:\n                     [,1]          [,2]\n(Intercept) 1.392224e-138 1.392224e-138\nmom_hs       5.956524e-07  5.956524e-07"
  },
  {
    "objectID": "lectures/Lecture13_code.html",
    "href": "lectures/Lecture13_code.html",
    "title": "Training set variability",
    "section": "",
    "text": "library(tidyverse)\n\ntheme_update(text = element_text(size=24))\noptions(repr.plot.width=12, repr.plot.height=6)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n# fage: Father's age in years.\n# mage: Mother's age in years.\n# mature: Maturity status of mother.\n# weeks: Length of pregnancy in weeks.\n# premie: Whether the birth was classified as premature (premie) or full-term.\n# visits: Number of hospital visits during pregnancy.\n# gained: Weight gained by mother during pregnancy in pounds.\n# weight: Weight of the baby at birth in pounds.\n# lowbirthweight: Whether baby was classified as low birthweight (low) or not (not low).\n# sex: Sex of the baby, female or male.\n# habit: Status of the mother as a nonsmoker or a smoker.\n# marital: Whether mother is married or not married at birth.\n# whitemom: Whether mom is white or not white.\n\ndf_raw &lt;- read.csv(\"../datasets/births14.csv\")\n\n\n# Drop missing data\ndf_raw %&gt;% summarise_all(~ sum(is.na(.)))\ndf &lt;- df_raw %&gt;% drop_na(everything())\ncat(\"Dropped \", nrow(df_raw) - nrow(df), \" rows \\n\")\nprint(nrow(df))\nhead(df)\n\n# Meaning of truncated variables\ndf %&gt;% group_by(mature) %&gt;% summarize(min(mage), max(mage))\ndf %&gt;% group_by(premie) %&gt;% summarize(min(weeks), max(weeks))\ndf %&gt;% group_by(lowbirthweight) %&gt;% summarize(min(weight), max(weight))\n\n\nA data.frame: 1 × 13\n\n\nfage\nmage\nmature\nweeks\npremie\nvisits\ngained\nweight\nlowbirthweight\nsex\nhabit\nmarital\nwhitemom\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n114\n0\n0\n0\n0\n56\n42\n0\n0\n0\n19\n0\n0\n\n\n\n\n\nDropped  206  rows \n[1] 794\n\n\n\nA data.frame: 6 × 13\n\n\n\nfage\nmage\nmature\nweeks\npremie\nvisits\ngained\nweight\nlowbirthweight\nsex\nhabit\nmarital\nwhitemom\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\n34\n34\nyounger mom\n37\nfull term\n14\n28\n6.96\nnot low\nmale\nnonsmoker\nmarried\nwhite\n\n\n2\n36\n31\nyounger mom\n41\nfull term\n12\n41\n8.86\nnot low\nfemale\nnonsmoker\nmarried\nwhite\n\n\n3\n37\n36\nmature mom\n37\nfull term\n10\n28\n7.51\nnot low\nfemale\nnonsmoker\nmarried\nnot white\n\n\n4\n32\n31\nyounger mom\n36\npremie\n12\n48\n6.75\nnot low\nfemale\nnonsmoker\nmarried\nwhite\n\n\n5\n32\n26\nyounger mom\n39\nfull term\n14\n45\n6.69\nnot low\nfemale\nnonsmoker\nmarried\nwhite\n\n\n6\n37\n36\nmature mom\n36\npremie\n10\n20\n6.13\nnot low\nfemale\nnonsmoker\nmarried\nwhite\n\n\n\n\n\n\nA tibble: 2 × 3\n\n\nmature\nmin(mage)\nmax(mage)\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nmature mom\n35\n47\n\n\nyounger mom\n14\n34\n\n\n\n\n\n\nA tibble: 2 × 3\n\n\npremie\nmin(weeks)\nmax(weeks)\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nfull term\n37\n46\n\n\npremie\n23\n36\n\n\n\n\n\n\nA tibble: 2 × 3\n\n\nlowbirthweight\nmin(weight)\nmax(weight)\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nlow\n1.12\n5.50\n\n\nnot low\n5.50\n10.42\n\n\n\n\n\n\ny_col &lt;- \"weight\"\n# We don't want to deal with collinear regressors in small samples\nx_cols &lt;- c(\"mage\", \"fage\", \"sex\")\nreg_form &lt;- sprintf(\"%s ~ 1 + %s\", y_col, paste(x_cols, collapse=\" + \"))\nx_all &lt;- model.matrix(formula(reg_form), df)\ny_all &lt;- df[[y_col]]\n\nn_all &lt;- length(y_all)\nstopifnot(nrow(x_all) == n_all)\n\n\n# Let's look at a single test point\ntest_ind &lt;- sample(n_all, 1) # Choose a single test point\nx_test &lt;- x_all[test_ind, ]\ny_test &lt;- y_all[test_ind]\nprint(x_test)\nprint(y_test)\n\n(Intercept)        mage        fage     sexmale \n          1          30          27           0 \n[1] 7.87\n\n\n\n# Sanity check that lm() is doing what we expect\nn_obs &lt;- 500\ntrain_inds &lt;- sample(setdiff(1:n_all, test_ind), n_obs, replace=FALSE)\nx &lt;- x_all[train_inds, ]\ny &lt;- y_all[train_inds]\n\nprint(dim(x))\n\n[1] 500   4\n\n\n\nprint(reg_form)\nreg_fit &lt;- lm(formula(reg_form), df[train_inds, ])\nbetahat &lt;- solve(t(x) %*% x, t(x) %*% y)\n\nbind_cols(\n  coefficients(reg_fit),\n  betahat)\n\nres &lt;- y - x %*% betahat\nmax(abs(res - summary(reg_fit)$residual))\n\nsigmasqhat &lt;- sum((y - x %*% betahat)^2) / (nrow(x) - ncol(x))\nc(sigmasqhat, summary(reg_fit)$sigma^2)\n\n[1] \"weight ~ 1 + mage + fage + sex\"\n\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n\n\n\nA tibble: 4 × 2\n\n\n...1\n...2\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n6.65717500\n6.65717500\n\n\n0.02924946\n0.02924946\n\n\n-0.01330772\n-0.01330772\n\n\n0.35398727\n0.35398727\n\n\n\n\n\n1.34267597040605e-13\n\n\n\n1.608225127332961.60822512733295\n\n\n\ntrain_inds &lt;- sample(setdiff(1:n_all, test_ind), n_obs, replace=FALSE)\n\nreg_fit &lt;- lm(formula(reg_form), df[train_inds, ])\nbetahat &lt;- coefficients(reg_fit)\nsigmahat &lt;- summary(reg_fit)$sigma\n\nmiscoverage_prob &lt;- 0.2\nza &lt;- qnorm(1 - miscoverage_prob / 2)\n\ny_pred &lt;- sum(betahat * x_test)\nwidth_pred &lt;- za * sigmahat\n\nintervals_df &lt;-\n    data.frame(sim_ind=1, \n             y_pred=y_pred, \n             width_pred=width_pred, \n             y_test=y_test,\n             n_obs=n_obs)\n\n\nsim_ind &lt;- 1\nggplot(intervals_df) +\n  geom_point(aes(x=sim_ind, y=y_test), color=\"red\", size=3) +\n  geom_point(aes(x=sim_ind, y=y_pred), color=\"black\", size=3) +\n  geom_errorbar(aes(x=sim_ind, ymin=y_pred - width_pred, ymax=y_pred + width_pred),\n                width=0.1) +\n  xlab(\"Simulation number (order doesn't matter)\") +\n  ylab(\"Weight at birth (pounds)\\nPredictive intervals and (unobserved) truth\")\n   \n\n\n\n\n\n\n\n\n\nn_obs &lt;- 50\nnum_sims &lt;- 20\nintervals_df &lt;- data.frame()\n\nfor (n_obs in c(30, 50, 100, 200, 500)) {\n  stopifnot(n_obs &lt; n_all - 1)\n  \n  for (sim_ind in 1:num_sims) {\n    train_inds &lt;- sample(setdiff(n_all, test_ind), n_obs, replace=FALSE)\n    reg_fit &lt;- lm(formula(reg_form), df[train_inds, ])\n    betahat &lt;- coefficients(reg_fit)\n    sigmahat &lt;- summary(reg_fit)$sigma\n    \n    miscoverage_prob &lt;- 0.2\n    za &lt;- qnorm(1 - miscoverage_prob / 2)\n    \n    y_pred &lt;- sum(betahat * x_test)\n    width_pred &lt;- za * sigmahat\n    \n    intervals_df &lt;- bind_rows(\n      intervals_df,\n      data.frame(sim_ind=sim_ind, \n                 y_pred=y_pred, \n                 width_pred=width_pred, \n                 y_test=y_test,\n                 n_obs=n_obs)\n    )\n  }\n}\n\n\nggplot(intervals_df) +\n  geom_point(aes(x=sim_ind, y=y_test), color=\"red\", size=3) +\n  geom_point(aes(x=sim_ind, y=y_pred), color=\"black\", size=3) +\n  geom_errorbar(aes(x=sim_ind, ymin=y_pred - width_pred, ymax=y_pred + width_pred),\n                width=0.1) +\n  xlab(\"Simulation number (order doesn't matter)\") +\n  ylab(\"Weight at birth (pounds)\\nPredictive intervals and (unobserved) truth\") +\n  facet_grid(.~n_obs)"
  },
  {
    "objectID": "quizzes/quiz4_retake.html",
    "href": "quizzes/quiz4_retake.html",
    "title": "STAT151A Quiz 4 retake (Due 9pm Mar 22nd)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[1in]\\]\nThis question will take the residuals of the training data to be random, and will consider variablity under sampling of the training data. The regressors for both the training data and test data will be taken as fixed.\nThe inverse of a 2x2 matrix is given by \\[\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}^{-1} =\n\\frac{1}{ad - bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nThe OLS estimator is given by \\(\\betahat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\\).\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\n\n\n(a)\nFor this quiz, we will assume that \\(\\y_n = \\beta^\\trans \\xv_n + \\res_n\\) for some \\(\\beta\\), and that the residuals \\(\\res_n\\) are IID with \\(\\expect{\\res_n} = 0\\) and \\(\\expect{\\res_n^2} = 1\\), but not necessarily normal.\nLet \\(\\xv_n  = (1, \\z_{n})^\\trans\\), where \\(\\meann \\z_n = 0\\) and \\(\\meann \\z_n^2 = \\delta &gt; 0\\). That is, assume we are regressing on a constant and a single mean-zero regressor. For this question, take the regressors to be fixed (not random).\nFind the limiting distribution of \\(\\sqrt{N}(\\betahat - \\beta)\\) as \\(N \\rightarrow \\infty\\).\n\n\n\n(b)\nDefine the expected prediction error \\[\n\\begin{aligned}\n\\y_\\new :={} \\beta^\\trans \\xv_{\\new} + \\res_\\new\n\\quad\\textrm{and}\\quad\n\\yhat_\\new :={}  \\betahat^\\trans \\xv_\\new.\n\\end{aligned}\n\\]\nUnder the conditions given in part (a), find the limiting distribution of\n\\[\n\\sqrt{N} (\\yhat_\\new - \\expect{\\y_\\new} )\n\\]\nas \\(N \\rightarrow \\infty\\), as a function of \\(\\xv_\\new\\). That is, the limiting distribution will depend on \\(\\xv_\\new\\), so please make the dependence explicit.\nYou may use your answer from part (a).\n\n\n\n(c)\nAssume the conditions and definitions given in (a) and (b). Assume that \\(\\delta \\ll 1\\) (that is, \\(\\delta\\) is much smaller than \\(1\\).)\nFind the limiting distribution of \\(\\sqrt{N} (\\yhat_\\new - \\expect{\\y_\\new} )\\) when\n\n\\(\\xv_\\new = (1, 0)\\) and\n\\(\\xv_\\new = (1, 1)\\).\n\nWhich of the two is larger?\nYou may use your answer from parts (a) and (b)."
  },
  {
    "objectID": "quizzes/40100a05c0473d483bb078019ade4aee.html",
    "href": "quizzes/40100a05c0473d483bb078019ade4aee.html",
    "title": "STAT151A Quiz 3 (Feb 27th)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[1in]\\]\nFor this quiz, assume that you have access to a function\n\\[\n\\Phi(z) = \\prob{\\RV{\\z} \\le z},\n\\]\nas well as its inverse,\n\\[\n\\Phi^{-1}(p) = z \\textrm{ such that }\\prob{\\RV{\\z} \\le z} = p,\n\\textrm{ for }p \\in [0,1],\n\\]\nwhere \\(\\RV{\\z} \\sim \\gauss{0,1}\\) denote a scalar-valued standard normal random variable.\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\n\n\n(a)\nSuppose that \\(\\RV{\\y} \\sim \\gauss{\\mu, \\sigma^2}\\), where \\(\\mu\\) and \\(\\sigma\\) are known. Using only the functions \\(\\Phi(\\cdot)\\), \\(\\Phi^{-1}(\\cdot)\\), and the known quantities \\(\\mu\\), and \\(\\sigma\\), find find a quantity \\(\\a\\) such that\n\\[\n\\prob{\\RV{\\y} \\le \\a} = 0.90.\n\\]\nNote that \\(\\Phi(\\cdot)\\) is only for a standard normal random variable. Please do not assume that you have direct access to the distribution and quantile functions of generic normal random variables.\nPlease justify your answer carefully.\n\n\n\n(b)\nIn the same setting as (a), please find \\(b_\\ell\\) and \\(b_u\\) such that\n\\[\n\\prob{b_\\ell \\le \\RV{\\y} \\le b_u} = 0.80.\n\\]\nNote the change from \\(0.90\\) in (a) to \\(0.80\\) on the right-hand side in the current problem.\nPlease justify your answer carefully. You may use your result from part (a).\n\n\n\n(c)\nNow suppose that \\(\\xv_\\new \\in \\rdom{P}\\), \\(\\betav\\in \\rdom{P}\\), and \\(\\sigma \\in \\rdom{}\\) all denote known, non-random quantites.\nSuppose that you also know that \\(\\y_\\new = \\betav^\\trans \\xv_\\new + \\res_n\\), where \\(\\res_n \\sim \\gauss{0, \\sigma^2}\\).\nUsing only these known quantities and the functions \\(\\Phi(\\cdot)\\), \\(\\Phi^{-1}(\\cdot)\\), find \\(c_\\ell\\) and \\(c_u\\) such that\n\\[\n\\prob{c_\\ell \\le \\y_\\new \\le c_u} = 0.80.\n\\]\nPlease justify your answer carefully. You may use your results from parts (a) and (b)."
  },
  {
    "objectID": "quizzes/cd882921d17da95eab20d193b962fcae.html",
    "href": "quizzes/cd882921d17da95eab20d193b962fcae.html",
    "title": "STAT151A Quiz 2.5 (Feb 22nd)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[1in]\\]\nFor this quiz, we’ll consider the linear models\n\\[\n\\begin{aligned}\ny_n ={} \\betav^\\trans \\xv_n + \\res_n\n&\\quad\\textrm{and}\\quad\ny_n ={} \\gammav^\\trans \\zv_n + \\eta_n\n\\end{aligned}\n\\]\nwith\n\\[\n\\begin{aligned}\n\\xv_n ={} (\\x_{n1}, \\x_{n2})^\\trans\n&\\quad\\textrm{and}\\quad\n\\zv_n ={} (\\z_{n1}, \\z_{n2})^\\trans \\textrm{ where}\n\\\\\n\\z_{n1} :={} \\x_{n1} - \\x_{n2}\n&\\quad\\textrm{and}\\quad\n\\z_{n2} :={} \\x_{n1} + \\x_{n2}\n\\end{aligned}\n\\]\nLet \\(\\X\\) denote the \\(N \\times 2\\) matrix whose \\(n\\)–th row is \\(\\xv_n^\\trans\\), and \\(\\Z\\) denote the \\(N \\times 2\\) matrix whose \\(n\\)–th row is \\(\\zv_n^\\trans\\).\nAssume that the matrix \\(\\X\\) is full rank.\nRecall that the inverse of a 2x2 matrix is given by\n\\[\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}^{-1} =\n\\frac{1}{ad - bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\n\n\n(a)\nUsing the definitions given on the first page, find a \\(2 \\times 2\\) matrix \\(\\A\\) such that \\(\\Z = \\X \\A\\).\n\n\n\n(b)\nUse the definitions given on the first page of the quiz. Suppose I tell you that the OLS estimate of \\(\\beta\\) is given by \\(\\betahat = (2, 4)\\). What is the value of \\(\\gammahat\\), the OLS estimate of \\(\\gamma\\)?\n\n\n\n(c)\nUse the definitions given on the first page of the quiz. Consider the residual variance estimators for the two regressions:\n\\[\n\\sigmahat^2_\\x := \\meann \\hat\\res_n^2 = \\meann (\\y_n - \\xv_n^\\trans \\betahat)^2\n\\quad\\textrm{and}\\quad\n\\sigmahat^2_\\z := \\meann \\hat\\eta_n^2 = \\meann (\\y_n - \\zv_n^\\trans \\gammahat)^2.\n\\]\nPlease select which of (a), (b), (c), or (d) is correct:\n\nIt is always the case that \\(\\sigmahat^2_\\x &gt; \\sigmahat^2_\\z\\)\nIt is always the case that \\(\\sigmahat^2_\\x &lt; \\sigmahat^2_\\z\\)\nIt is always the case that \\(\\sigmahat^2_\\x = \\sigmahat^2_\\z\\)\nIn general, we cannot determine the relationship between \\(\\sigmahat^2_\\x\\) and \\(\\sigmahat^2_\\z\\) using the information provided.\n\nBriefly justify your answer."
  },
  {
    "objectID": "lectures/lectures.html",
    "href": "lectures/lectures.html",
    "title": "Lectures and Labs",
    "section": "",
    "text": "Lecture 1: Jan 16th:\n\nClass organization\n\nLab Jan 17th:\n\nStyle problem\n\nLecture 2: Jan 18th:\n\nDifferent kinds of questions\nNote on linear and affine functions\n\n\n\n\nLecture 3: Jan 23rd:\n\nMultilinear regression as loss minimization\n\nLab Jan 17th:\n\nLinear algebra in R\n\nLecture 4: Jan 25th:\n\nMultilinear regression as projection\n\n\n\n\nLecture 5: Jan 29th:\n\nQuiz 1\nAlternative ways to draw lines through points\n\nLab Jan 30th:\n\nHomework 1 review\n\nLecture 6: Feb 1st:\n\nTransformations of regressors: Some payoffs from the linear algebra perspective\n\n\n\n\nLecture 7: Feb 6th:\n\nUnivariate statistics and limit theorems\n\nLab Feb 7th:\n\nHomework consultation\n\nLecture 8: Feb 8th:\n\nVector and matrix-valued statistics and limit theorems\n\n\n\n\nLecture 9: Feb 13th:\n\nThe Gaussian assumption\n\nLab Feb 14th:\n\nHomework 2 review\n\nLecture 10: Feb 15th:\n\nSimulations and the law of large numbers\n\n\n\n\nLecture 11: Feb 20th:\n\nConsistency of OLS and the residual variance under the Gaussian assumptions\n\nLab Feb 21st:\n\nHomework 3 consultation\n\nSpecial session: Feb 22nd:\n\nLinear algebra review and Quiz 2 retake\n\n\n\n\nLecture 12: Feb 27th:\n\nResidual distribution under normality\n\nLab Feb 28th:\n\nHomework 3 solutions\n\nLecture 13: Feb 29th:\n\nSampling variability of the coefficients\nSampling variability of the coefficients (notebook)\n\n\n\n\nLecture 14: Mar 5th:\n\nImplications of Gaussianity (and deviations from it)\n\nLab Mar 6th:\n\nHomework 4 consultation\n\nLecture 15: Mar 7th:\n\nHeteroskedasticity and the sandwich covariance matrix\n\n\n\n\nLecture 16: Mar 12th:\n\nSampling variability of the coefficients under normality\n\nLab Mar 13th:\n\nHomework 4 solutions\n\nLecture 17: Mar 14th:\n\nInterpreting the regression coefficients\nInterpreting the regression coefficients (notebook)\n\n\n\n\nLecture 18: Mar 19th:\n\nThe role of regressors in the coefficient variance\nThe role of regressors in the coefficient variance (notebook)\n\nLab Mar 20th:\n\nHomework 5 consultation\n\nLecture 19: Mar 21st:\n\nConfidence intervals and hypothesis testing\n\n\n\n\nLecture 20: April 2nd:\n\nTesting groups of coefficients with tht F-test\n\nLab Mar 20th:\n\nProject consultation\n\nLecture 21: April 4th\n\nCross-validation and information criteria",
    "crumbs": [
      "Lectures and labs"
    ]
  },
  {
    "objectID": "lectures/Lecture9.html",
    "href": "lectures/Lecture9.html",
    "title": "The Gaussian assumption",
    "section": "",
    "text": "\\(\\LaTeX\\)"
  },
  {
    "objectID": "lectures/Lecture9.html#prediction-error-with-the-gaussian-assumption",
    "href": "lectures/Lecture9.html#prediction-error-with-the-gaussian-assumption",
    "title": "The Gaussian assumption",
    "section": "Prediction error with the Gaussian assumption",
    "text": "Prediction error with the Gaussian assumption\nUnder our Gaussian assumption with fixed regressors we can write\n\\[\n\\begin{aligned}\n\\y_\\new ={}& \\xv_\\new^\\trans \\betav + \\res_\\new\\\\\n\\yhat_\\new ={}& \\xv_\\new^\\trans \\betavhat \\Rightarrow \\\\\n\\y_\\new - \\yhat_\\new ={}& \\xv_\\new^\\trans (\\betav - \\betavhat) + \\res_\\new.\n\\end{aligned}\n\\]\nFrom this we can see that the error \\(\\y_\\new - \\yhat_\\new\\) is normally distributed. In particular, if we condition on (fix) the training data, and so fix \\(\\betavhat\\), then\n\\[\n\\y_\\new - \\yhat_\\new | \\X, \\Y \\sim \\gauss{\\xv_\\new^\\trans (\\betav - \\betavhat), \\sigma^2}.\n\\]\nUnfortunately we do not know \\(\\betav\\) nor \\(\\res_\\new\\). In fact, we don’t even know \\(\\sigma^2\\), the variance of \\(\\res_\\new\\).\nHowever, under our assumptions, we can show that \\(\\betavhat \\rightarrow \\beta\\). That will mean that \\(\\xv_\\new^\\trans (\\betav - \\betavhat) \\approx 0\\) for large \\(N\\). Even more, this fact will allow us to form an estimate of \\(\\sigma\\) which is accurate for large \\(N\\).\nWe’ll prove \\(\\betavhat \\rightarrow \\beta\\) a few different ways. In each case, the key will be to use the formula\n\\[\n\\betahat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y,\n\\]\ntogether with\n\\[\n\\Y = \\X \\beta + \\resv,\n\\]\nwhere the Gaussian assumption means that\n\\[\n\\resv \\sim \\gauss{\\zerov, \\sigma^2 \\id{}_N},\n\\]\nwhich is an \\(N\\)–dimensional Gaussian random vector. Note that this is equivalent to\n\\[\n\\Y \\sim \\gauss{\\X\\beta, \\sigma^2 \\id{}_N}.\n\\]"
  },
  {
    "objectID": "lectures/Lecture7.html",
    "href": "lectures/Lecture7.html",
    "title": "Univariate statistics and limit theorems",
    "section": "",
    "text": "\\(\\LaTeX\\)"
  },
  {
    "objectID": "lectures/Lecture7.html#limit-theorems",
    "href": "lectures/Lecture7.html#limit-theorems",
    "title": "Univariate statistics and limit theorems",
    "section": "Limit theorems",
    "text": "Limit theorems\nThere are two important limit theorems that we will use: laws of large numbers (LLN) and central limit theorems (CLT). In this class I will not worry too much about getting the strongest possible results (e.g., weakest possible assumptions for a particular convergence guarantee). So I’ll choose conditions that are simple to work with.\n\nLLN\nFirst, let’s state and prove a LLN. Suppose \\(\\RV{\\x}_n\\) are independent random variables for \\(n =1, \\ldots, \\infty\\) such that \\(\\max_n \\var{\\RV{\\x}_n} &lt; \\infty\\). That is, every variance is finite and the variances do not diverge.\n\n\n\n\n\n\nNote to future instructors\n\n\n\nActually, all we need is \\(\\lim_{N\\rightarrow\\infty} \\max_{n=1,\\ldots,N} \\frac{1}{N} \\var{\\RV{\\x}_n} = 0\\), which is more convenient when we actually use this for the fixed regressor setting, but which complicates the exhibition here.\n\n\nWrite \\(\\expect{\\RV{\\x}_n} = \\mu_n\\) and assume that \\(\\meann \\mu_n \\rightarrow \\overline{\\mu} &lt; \\infty\\). Then consider the random variable \\[\n\\overline{x} := \\frac{1}{N} \\sumn \\RV{\\x}_n.\n\\]\nWe can see that\n\\[\n\\begin{aligned}\n\\expect{\\overline{x}} ={}& \\meann \\mu_n \\\\\n\\var{\\overline{x}} ={}& \\\n  \\expect{\\left(\\overline{x} - \\meann \\mu_n\\right)^2} \\\\\n={}&  \\expect{\\left(\\meann (\\RV{\\x}_n - \\mu_n) \\right)^2} \\\\\n={}&  \\frac{1}{N^2} \\sumn \\expect{(\\RV{\\x}_n - \\mu_n)^2} \\\\\n\\le{}&  \\frac{1}{N^2}  N \\max_n \\var{\\RV{\\x}_n} \\rightarrow 0\n\\quad\\textrm{as }N\\rightarrow \\infty.\n\\end{aligned}\n\\]\nA random variable with zero variance is just a constant, so we have shown that \\(\\overline{x}\\) converges to the constant \\(\\overline{\\mu}\\), which is the limit of \\(\\meann \\mu_n\\). (Markov’s theorem gives a formal proof of convergence in probability, but I think that’s beyond the scope of this class.) \\[\n\\overline{x} \\rightarrow \\overline{\\mu} \\quad\n\\textrm{ as }N\\rightarrow \\infty.\n\\]\nNote that in the preceding statement, \\(\\overline{x}\\) is a random variable, but \\(\\overline{\\mu}\\) is a constant.\nIf \\(\\mu_n = \\mu = \\overline{\\mu}\\), then this gives the familiar LLN from introductory probability, but this more general form will be useful in regression.\n\n\nCLT\nThe normal distribution is special because it is the limit of a rescaled average. Take the setting of the previous example, and recall that, as \\(N \\rightarrow \\infty\\),\n\\[\n\\overline{x} - \\overline{\\mu} = \\meann (\\RV{\\x}_n - \\mu_n) \\rightarrow 0.\n\\]\nOne might note that, by the same argument of the previous section,\n\\[\n\\var{N (\\overline{x} - \\overline{\\mu})} \\rightarrow \\infty.\n\\]\nThat is, if you scale \\(\\overline{x} - \\overline{\\mu}\\) by \\(N\\), for large \\(N\\), you get a random variable that takes arbitrarily large values with nonzero probability. However,\n\\[\n\\begin{aligned}\n\\var{\\sqrt{N} \\overline{x}} ={}& \\\n  \\expect{\\left(\\sqrt{N} \\overline{x} - \\sqrt{N} \\meann \\mu_n\\right)^2} \\\\\n={}&  \\expect{\\left(\\sqrt{N} \\meann (\\RV{\\x}_n - \\mu_n) \\right)^2} \\\\\n={}&  \\frac{1}{N} \\sumn \\expect{(\\RV{\\x}_n - \\mu_n)^2} \\\\\n={}&  \\meann \\var{\\RV{\\x}_n},\n\\end{aligned}\n\\]\na quantity that might reasonably converge to a constant, giving \\(\\sqrt{N} (\\overline{x} - \\overline{\\mu})\\) converging to a nondegenerate, but also nondivergent, random variable, as long as \\[\n\\meann \\var{\\RV{\\x}_n} \\rightarrow \\overline{v}.\n\\]\n(Note that the limit must be finite by assumption, but we do need to additionally assume that the limit exists.)\nIn fact, the CLT says something stronger. Not only does \\(\\sqrt{N} (\\overline{x} - \\overline{\\mu})\\) converge to a nicely behaved random variable, but the variable it converges to is normally distributed:\n\\[\n\\sqrt{N} (\\overline{x} - \\overline{\\mu}) =\n\\frac{1}{\\sqrt{N}} \\sumn (\\RV{\\x}_n - \\mu_n) \\rightarrow \\RV{\\z}\n\\quad\\textrm{where }\\RV{\\z} \\sim \\gauss{0, \\overline{v}}.\n\\]\nI won’t prove this here, but you can find lots of references that do prove it. As above, taking \\(\\mu_n = \\mu = \\overline{\\mu}\\) and \\(\\var{\\RV{\\x}_n} = \\overline{v}\\) gives the familiar IID result."
  },
  {
    "objectID": "lectures/Lecture4.html",
    "href": "lectures/Lecture4.html",
    "title": "Least squares as a projection.",
    "section": "",
    "text": "Review / introduce some linear algebra\n\nOrthogonal bases\nEigendecompositions\nMatrix square roots\n\nDerive the OLS estimator from a geometric perspective\n\nDerive the form of projection operators without using vector calculus\nWrite the OLS estimator as a projection operator\nBriefly discuss some consequences of OLS as a projection"
  },
  {
    "objectID": "lectures/Lecture4.html#projection-basis-invariance",
    "href": "lectures/Lecture4.html#projection-basis-invariance",
    "title": "Least squares as a projection.",
    "section": "Projection basis invariance",
    "text": "Projection basis invariance\nSomething suspicious has happened, though — we started with a subspace, and got an answer that depends on the basis we chose. What if we had chosen a different orthonormal basis, \\(\\V\\), which spans \\(\\Sc\\)? Would we have gotten a different answer?\nIt turns out, no. To see this, write each \\(\\uv_p = \\V \\av_p\\) for some \\(\\av_p\\). Stacking the \\(\\av_p\\), we can write \\(\\U = \\V \\A\\).\nWe can then write\n\\[\n\\begin{align*}\n\\V^\\trans \\U ={}& \\V^\\trans \\V \\A = \\A \\\\\n\\U^\\trans \\U ={}& \\id = \\U^\\trans \\V \\A \\Rightarrow \\\\\n\\A^{-1} ={}& \\U^\\trans \\V = \\left(\\V^\\trans \\U \\right)^\\trans = \\A^\\trans.\n\\end{align*}\n\\]\nSo \\(\\A\\) itself is orthonormal. Combining these facts,\n\\[\n\\U \\U^\\trans = \\V \\A \\A^\\trans \\V^\\trans = \\V \\V^\\trans,\n\\]\nand the projection operator does not depend on the orthonormal basis we choose.\nWe can thus safely define the “projection matrix”\n\\[\n\\proj{\\Sc} := \\U \\U^\\trans,\n\\]\nsuch that\n\\[\n\\projop{Sc} \\vv = \\proj{\\Sc} \\vv,\n\\]\nand where \\(\\U\\) is a matrix whose columns are any orthonormal basis of \\(\\Sc\\)."
  },
  {
    "objectID": "lectures/Lecture4.html#some-properties-of-projection-matrices",
    "href": "lectures/Lecture4.html#some-properties-of-projection-matrices",
    "title": "Least squares as a projection.",
    "section": "Some properties of projection matrices",
    "text": "Some properties of projection matrices\nThe projection matrix has some special properties that are worth mentioning.\n\nFirst, if \\(\\vv \\in \\Sc\\), then \\(\\proj{\\Sc}\\vv = \\vv\\) Similarly, \\(\\proj{\\Sc} \\vv = 0\\) is \\(\\vv \\in \\Sc^\\perp\\).\nSecond, \\(\\proj{\\Sc}\\) is symmetric, since \\(\\U \\U^\\trans = (\\U \\U^\\trans)^\\trans\\).\nThird, \\(\\proj{\\Sc}\\) is “idempotent”, meaning \\(\\proj{\\Sc} = \\proj{\\Sc} \\proj{\\Sc}\\).\n\n\n\n\n\n\n\nExercise\n\n\n\nProve these results.\n\n\nThese properties say that \\(\\proj{\\Sc}\\) has a special eigenstructure.\nConsider a square, symmetric matrix \\(\\A\\). The vector \\(\\vv\\) is an eigenvector of \\(\\A\\) with eigenvalue \\(\\lambda\\) if\n\\[\n\\A \\vv = \\lambda \\vv.\n\\]\nThat is, \\(\\A\\) maps \\(\\vv\\) onto a scaled version of itself. The final property constrains the possible values of the eigenvalues of \\(\\proj{\\Sc}\\). Let \\(\\vv\\) and \\(\\lambda\\) denote an eigenvector and eigenvalue of \\(\\proj{\\Sc}\\), then\n\\[\n\\lambda \\vv = \\proj{\\Sc} \\vv =  \\proj{\\Sc} \\ldots \\proj{\\Sc} \\vv = \\lambda^k \\vv.\n\\]\nTherefore we need \\(\\lambda^k = \\lambda\\) for any \\(k\\)! Only two values are possible: \\(\\lambda \\in \\{ 0, 1\\}\\). And we know that, for any basis vectors \\(\\av \\in \\Sc\\) and \\(\\av^\\perp \\in \\Sc^\\perp\\),\n\\[\n\\proj{\\Sc} \\av = \\av \\quad\\textrm{and}\\quad \\proj{\\Sc} \\av^\\perp = 0,\n\\]\nso the eigenvectors are precisely any vectors lying entirely in either \\(\\Sc\\) or \\(\\Sc^\\perp\\).\nFinally, given a projection matrix, we can form the orthogonal projection matrix by simply subtracting from the identity:\n\\[\n\\proj{\\Sc^\\perp} = \\id - \\proj{\\Sc}.\n\\]"
  },
  {
    "objectID": "lectures/Lecture4.html#square-roots-and-inverses-via-eigendecomposition",
    "href": "lectures/Lecture4.html#square-roots-and-inverses-via-eigendecomposition",
    "title": "Least squares as a projection.",
    "section": "Square roots and inverses via eigendecomposition",
    "text": "Square roots and inverses via eigendecomposition\nThe projection matrix was in terms of orthogonal bases. However, we would like it for spaces spanned by a generic, non-orthonormal set of vectors to solve our least squares problem. To bridge the gap, we can construct a linear operator that converts a set of vectors to an orthogonal basis.\nFirst, recall that a square, symmetric \\(P \\times P\\) matrix \\(\\A\\) can always be written in the form\n\\[\n\\begin{align*}\n\\A &= \\U \\Lambda \\U^\\trans\n\\quad\\textrm{where}\\quad \\\\\n\\U^\\trans\\U &= \\id \\textrm{ ($\\U$ is orthogonal) and }\\\\\n\\Lammat &=\n\\begin{pmatrix}\n\\lambda_1 & 0  & \\ldots & 0 \\\\\n0 & \\lambda_2 & 0\\ldots & 0 \\\\\n\\vdots &  & \\ddots & \\vdots \\\\\n0 & 0 & \\ldots 0 & \\lambda_P  \\\\\n\\end{pmatrix}\n\\textrm{ is diagonal.}\n\\end{align*}\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nProve that the columns of \\(\\U\\) are the eigenvectors, and the \\(\\lambda_p\\) are the corresponding eigenvalues.\n\n\nA matrix is called “positive definite” if the eigenvalues are all strictly positive, and positive semi-definite if they are all non-zero.\n\n\n\n\n\n\nExercise\n\n\n\nProve that positive definiteness is equivalent to \\(\\vv^\\trans \\A \\vv &gt; 0\\), and semi-definiteness to \\(\\vv^\\trans \\A \\vv \\ge 0\\).\n\n\n\n\n\n\n\n\nExercise\n\n\n\nProve that if \\(\\A\\) is of the form \\(\\A = \\Q^\\trans \\Q\\) for some \\(\\Q\\), then \\(\\A\\) is positive semi-definite.\n\n\nNote that \\(\\A\\) is invertible if and only if its eigenvalues are non-zero. To see that nonzero eigenvalues are sufficient, we can write \\(\\A^{-1} = \\U \\Lambda^{-1} \\U^\\trans\\).\n\n\n\n\n\n\nExercise\n\n\n\nVerify that \\(\\A^{-1}\\) so defined is an inverse.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nProve that, if some \\(\\lambda_p = 0\\), then \\(\\A\\) is not invertible.\n\n\nFrom the eigendecomposition, one can define the square root of a symmetric positive semit-definite matrix \\(\\A\\): that is, a matrix \\(\\A^{1/2}\\) such that \\((\\A^{1/2})^\\trans \\A^{1/2} = \\A\\). First, define\n\\[\n\\Lammat^{1/2} :=\n\\begin{pmatrix}\n\\sqrt{\\lambda_1} & 0  & \\ldots & 0 \\\\\n0 & \\sqrt{\\lambda_2} & 0\\ldots & 0 \\\\\n\\vdots &  & \\ddots & \\vdots \\\\\n0 & 0 & \\ldots 0 & \\sqrt{\\lambda_P}  \\\\\n\\end{pmatrix}.\n\\]\nThen \\(\\U \\Lammat^{1/2} \\U^\\trans\\) is a square root of \\(\\A\\). Matrix square roots are not unique, but that will not matter — all that matters is that symmetric positive semi-definite matrices always have a square root. However, when I write \\(\\A^{1/2}\\), I will refer to the symmetric square root given in the formula above.\nFurther \\(\\A^{1/2}\\) itself is invertible if and only if \\(\\A\\) is invertible.\n\n\n\n\n\n\nExercise\n\n\n\nProve that \\(\\A^{1/2}\\) so defined is a square root.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFind a different matrix than the one given above that is also a square root.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nProve that any square root \\(\\A^{1/2}\\) is invertible."
  },
  {
    "objectID": "lectures/Lecture4.html#matrix-square-roots-for-orthonormalization",
    "href": "lectures/Lecture4.html#matrix-square-roots-for-orthonormalization",
    "title": "Least squares as a projection.",
    "section": "Matrix square roots for orthonormalization",
    "text": "Matrix square roots for orthonormalization\nIn our case, recall that we want to find an orthonormal basis of the column span of \\(\\X\\). Let us assume that \\(\\X\\) has full column rank. Observe that\n\n\\(\\X^\\trans \\X\\) is symmetric and positive definite.\n\\(\\Rightarrow\\) We can form the inverse square root \\((\\X^\\trans \\X)^{-1/2}\\).\n\\(\\Rightarrow\\) We can define \\(\\U := \\X (\\X^\\trans \\X)^{-1/2}\\).\n\nThen\n\n\\(\\U\\) is orthonormal and\n\\(\\U\\) has the same column span as \\(\\X\\).\n\nEffectively we have produced an orthonormal basis for \\(\\X\\).\n\n\n\n\n\n\nExercise\n\n\n\nProve that \\(\\U\\) is orthonormal and \\(\\U\\) has the same column span as \\(\\X\\).\n\n\nNote that right multiplying \\(\\X\\) effectively transforms each row of \\(\\X\\) — that is, the regressor \\(\\x_n\\) — into a new regressor \\(\\z_n = (\\X^\\trans \\X)^{-1/2} \\z_n\\) which has an identity covariance matrix.\nUsing our orthonormal basis, we can write the projection matrix as\n\\[\n\\proj{\\Sc_\\X} = \\U \\U^\\trans =\n\\X (\\X^\\trans \\X)^{-1/2} (\\X^\\trans \\X)^{-1/2} \\X^\\trans =\n\\X (\\X^\\trans \\X)^{-1} \\X^\\trans.\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nCheck directly that any vector that can be written in the form \\(\\vv = \\X \\beta\\) satisfies \\(\\vv = \\proj{\\Sc_\\X} \\vv\\), and that any \\(\\vv \\in \\Sc_\\X^\\perp\\) satisfies \\(\\proj{\\Sc_\\X} \\vv = \\zerov\\).\nOne might simply begin with the above formula and verify that it acts as a projection directly. However, doing so may seem mysterious or somehow lucky. Bulding the projection up as we have done shows where it comes from.\n\n\nFinally, we have derived the projection operator onto the span of \\(\\X\\):\n\\[\n\\begin{align*}\n\\proj{\\Sc_\\X} \\Y ={}& \\X (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\n     ={} \\X \\betahat \\quad\\textrm{where}\\\\\n     \\betahat ={}& (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y.\n\\end{align*}\n\\]\nWe give this fitted vector a special name:\n\\[\n\\Yhat := \\proj{\\Sc_\\X} \\Y = \\X \\betahat.\n\\]\nAdditionally, we have that the fitted residuals are given by\n\\[\n\\resvhat = \\Y - \\X \\betahat = \\id \\Y - \\proj{\\Sc_\\X} \\Y\n  = (\\id - \\proj{\\Sc_\\X})\\Y = \\proj{\\Sc_\\X^\\perp} \\Y.\n\\]\nNote that \\(\\Y = \\Yhat + \\resvhat = (\\proj{\\Sc_\\X}  + \\proj{\\Sc_\\X^\\perp}) \\Y\\). Also, our squared error loss is given by \\(\\norm{\\resvhat}_2^2 = \\norm{\\proj{\\Sc_\\X^\\perp} \\Y}_2^2\\), the squared magnitude of the component of \\(\\Y\\) not lying in \\(\\X\\)."
  },
  {
    "objectID": "lectures/Lecture4.html#some-facts-that-follow-from-this",
    "href": "lectures/Lecture4.html#some-facts-that-follow-from-this",
    "title": "Least squares as a projection.",
    "section": "Some facts that follow from this",
    "text": "Some facts that follow from this\nFrom the projection form of least squares, we can immediately see some facts that may not be obvious from the minimization perspective.\n\nThe residuals sum to zero if and only if \\(\\onev \\in \\Sc_\\X\\).\nIt is often thought that the residuals of OLS have zero mean, but this can actually be seen to follow from including a constant term. In particular,\n\\[\n\\meann \\reshat_n = \\frac{1}{N} \\resvhat^\\trans \\onev = \\frac{1}{N} \\proj{\\Sc_\\X^\\perp} \\onev.\n\\]\nAnd \\(\\onev \\in \\Sc_\\X\\) if and only if \\(\\proj{\\Sc_\\X^\\perp} \\onev = 0\\). Of course it suffices to have \\(\\onev\\) as a column of \\(\\X\\), but \\(\\onev \\in \\Sc_\\X\\) whenever any linear combination of regressors is a constant.\nA common example is the one-hot encoding for a class. Suppose each observation \\(n\\) is in exactly one of \\(K\\) groups, and we set \\(\\x_{nk} = 1\\) if observation \\(n\\) is in group \\(k\\), and \\(\\x_{nk} = 0\\) otherwise. Since each observation is in exactly one group, \\(\\sum_{k=1}^K \\x_{nk} = 1\\), and \\(\\onev \\in \\Sc_\\X\\), and \\(\\meann \\res_n = 0\\).\n\n\nInvertible linear transformations leave the fit unchanged\nSuppose someone suggests trying to get a better least squares fit by regression on \\(\\z_n = \\A \\x_n\\) rather than \\(\\x_n\\), where \\(\\A\\) is invertible. Since the columns of \\(\\Z = \\X \\A^\\trans\\) are simply linear combinations of the columns of \\(\\X\\), the column span is the same, and both \\(\\Yhat\\) and \\(\\resvhat\\) are unchanged. (Note that the coefficients will change, however, since we are forming linear combinations of a different basis!)\n\n\nMore regressors can only make the sum of squares smaller\nWhen we include an additional regressor, we can only increase the column span of \\(\\Sc_\\X\\), and so only decrease the squared error \\(\\norm{\\resvhat}_2^2\\).\n(This fact is also easy to see from the optimization perspective, since minimizing over a more expressive class can never increase the optimal error.)"
  },
  {
    "objectID": "lectures/Lecture2_linearity_note.html",
    "href": "lectures/Lecture2_linearity_note.html",
    "title": "Note on linear and affine functions",
    "section": "",
    "text": "This course is called “linear models,” so it’s worth being clear about what makes a function linear. As we will see, in some sense it might be better to call the course “affine models.”\nSuppose we have a function \\(f(\\cdot)\\) from one space to another. For example, the input space can be \\(\\mathbb{R}^2\\) (2-vectors) and the output can be scalars, \\(\\mathbb{R}\\). We’ll write the input space as \\(\\mathbb{I}\\) and the output as \\(\\mathbb{O}\\). We assume that addition and scalar multiplication make sense in both \\(\\mathbb{I}\\) and \\(\\mathbb{O}\\).\nLet \\(\\z\\) and \\(\\z'\\) be inputs in \\(\\mathbb{I}\\), and let \\(\\alpha \\in \\mathbb{R}\\). In some generality, a function \\(f(\\cdot)\\) from one space to another is called linear if it satsfies, for all \\(\\alpha\\), \\(\\z\\), and \\(\\z'\\):\n\\[\nf(cdot)\\textrm{is a linear function if and only if }\\quad\nf(\\alpha \\z) = \\alpha f(\\z)\n\\quad\\textrm{and}\\quad\nf(\\z + \\z') = f(\\z) + f(\\z').\n\\]\nFor example, fix \\(\\beta \\in \\mathbb{R}^2\\), and let \\(f(\\z) = \\beta^\\trans \\z\\). Then \\(\\mathbb{I} = \\mathbb{R}^2\\), \\(\\mathbb{O} = \\mathbb{R}\\). Then \\(f(\\cdot)\\) is a linear function.\nBy this defintion, the regression function \\[\nf(\\z) =  \\beta_0 + \\beta_1 \\z_n + \\res_n\n\\]\nis not linear in \\(\\z_n\\). It’s not even linear if we take \\(\\x = (1, \\z_n)\\) and write \\(\\y_n = g(\\x) = \\beta^\\trans \\x_n + \\res_n\\), because of the residual.\nOf course, in non-formal language, we might describe them as “linear” simply because their graph is a straight line. This is one justification for the class name “linear models”.\nFormally, both \\(f(\\cdot)\\) and \\(g(\\cot)\\) are “affine function,” which means they are linear with an offset.\nFor the purposes of this class, we can define affine functions as:1\n\\[\ng(\\cdot)\n\\textrm{ is an affine function if and only if }\n\\textrm{there exists a }b \\in \\mathbb{O}\n\\textrm{ such that }f(\\cdot) - b\n\\textrm{ is linear.}\n\\]\nGiven this, we can see that the relationship \\(\\y_n = \\beta_0 + \\beta_1 \\z_n + \\res_n\\) is affine as maps from\n\\[\n\\begin{align*}\n\\beta &\\mapsto \\y_n\\\\\n\\z_n &\\mapsto \\y_n\\\\\n\\res_n &\\mapsto \\y_n.\n\\end{align*}\n\\]\nMaybe the course should be called “affine models”!\nA final note: it is likely that the models are called “linear models” because the expectation of \\(\\y_n\\) is, in fact, linear in both the regressors and coefficient, under the assumption that the residuals have mean zero:\n\\[\n\\expect{}{\\y_n} = \\beta^\\trans \\x_n.\n\\]\nHowever, keeping with the spirit of the class, I prefer not to bake such an assumption into the defintion of a linear model, reserving stochastisicity for concrete situations."
  },
  {
    "objectID": "lectures/Lecture2_linearity_note.html#footnotes",
    "href": "lectures/Lecture2_linearity_note.html#footnotes",
    "title": "Note on linear and affine functions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs I did in lecture, one could instead write the linear transforms in a way that depends on \\(b\\), which is then asserted to exist. But that is a bit clumsy, and equivalent to the given definition. Wikipedia defines affine transformations in terms of invariance relations, whereas Wolfram appears to limit to \\(\\mathbb{R}^d\\). I’m not sure whether my definition here is official, but I think it strikes a nice balance.↩︎"
  },
  {
    "objectID": "lectures/Lecture19.html",
    "href": "lectures/Lecture19.html",
    "title": "Confidence intervals and hypothesis testing",
    "section": "",
    "text": "\\(\\,\\)"
  },
  {
    "objectID": "lectures/Lecture19.html#t-statistics",
    "href": "lectures/Lecture19.html#t-statistics",
    "title": "Confidence intervals and hypothesis testing",
    "section": "T-statistics",
    "text": "T-statistics\nSuppose we’re interested in the value \\(\\beta_k\\), the \\(k\\)–th entry of \\(\\betav\\) in for some regression \\(\\y_n \\sim \\betav^\\trans \\xv_n\\). Recall that we have been finding \\(\\v\\) such that\n\\[\n\\sqrt{N} (\\beta_k - \\beta) \\rightarrow \\gauss{0, \\v}.\n\\]\nFor example, under homoskedastic assumptions with \\(\\y_n = \\xv_n^\\trans \\beta + \\res_n\\), we have\n\\[\n\\begin{aligned}\n\\v =& \\sigma^2 (\\Xcov^{-1})_{kk} \\textrm{ where } \\\\\n\\Xcov =& \\lim_{N \\rightarrow \\infty} \\frac{1}{N} \\X^\\trans \\X \\textrm{ and } \\\\\n\\sigma^2 =& \\var{\\res_n}.\n\\end{aligned}\n\\]\nTypically we don’t know \\(\\v\\), but have \\(\\hat\\v\\) such that \\(\\hat\\v \\rightarrow \\v\\) as \\(N \\rightarrow \\infty\\). Again, under homoeskedastic assumptions,\n\\[\n\\begin{aligned}\n\\hat\\v =& \\hat\\sigma^2 \\left(\\frac{1}{N} \\X^\\trans \\X \\right)_{kk} \\textrm{ where } \\\\\n\\hat\\sigma^2 =& \\frac{1}{N-P} \\sumn \\reshat_n^2.\n\\end{aligned}\n\\]\nPutting all this together, the quantity\n\\[\n\\t = \\frac{\\sqrt{N} (\\betahat_k - \\beta_k)}{\\sqrt{\\hat\\v}} = \\frac{\\betahat_k - \\beta_k}{\\sqrt{\\hat\\v / N}}\n\\]\nhas an approximately standard normal distribution for large \\(N\\).\nQuantities of this form are called “T–statistics,” since, under our normal assumptions, we have shown that\n\\[\n\\t \\sim \\studentt{N-P},\n\\]\nexactly for all \\(N\\). Despite it’s name, it’s worth remembering that a T–statistic is actually not Student T distributed in general; it is asymptotically normal. Recall that for large \\(N\\), the Student T and standard normal distributions coincide."
  },
  {
    "objectID": "lectures/Lecture19.html#plugging-in-values-for-beta_k",
    "href": "lectures/Lecture19.html#plugging-in-values-for-beta_k",
    "title": "Confidence intervals and hypothesis testing",
    "section": "Plugging in values for \\(\\beta_k\\)",
    "text": "Plugging in values for \\(\\beta_k\\)\nHowever, there’s something funny about a “T-statistic” — as written, you cannot compute it, because you don’t know \\(\\beta_k\\). In fact, finding what values \\(\\beta_k\\) might plausibly take is the whole point of statistical inference.\nSo what good is a T–statistic? Informally, one way to reason about it is as follows. Let’s take some concrete values for an example. Suppose guess that \\(\\beta_k^0\\) is the value, and compute\n\\[\n\\betahat_k = 2 \\quad\\textrm{and}\\quad \\sqrt{\\hat\\v / N} = 3\n\\quad\\textrm{so}\\quad \\t = \\frac{2 - \\beta_k^0}{3}.\n\\]\nWe use the superscript \\(0\\) to indicate that \\(\\beta_k^0\\) is our guess, not necessarily the true value.\nSuppose we plug in some particular value, such as \\(\\beta_k^0 = 32\\). Using this value, we compute our T–statistic, and find that it’s very large — in our example, we would have \\(\\t = (2 - 32) / 3 = -30\\). It’s very unlikely to get a standard normal (or Student T) draw this large. Therefore, either:\n\nWe got a very (very very very very) unusual draw of our standard normal or\nWe guessed wrong, i.e. \\(\\beta_k \\ne \\beta_k^0 = 32\\).\n\nIn this way, we might consider it plausible to “reject” the hypothesis that \\(\\beta_k = 32\\).\nThere’s a subtle problem with the preceding reasoning, however. Suppose we do the same calculation with \\(\\beta_k^0 = 1\\). Then \\(\\t = (2 - 1) / 3 = 1/3\\). This is a much more typical value for a standard normal distribution. However, the probability of getting exactly \\(1/3\\) — or, indeed, any particular value — is zero, since the normal distribution is continuous valued. (This problem is easiest to see with continuous random variables, but the same basic problem will occur when the distribution is discrete but spread over a large number of possible values.)"
  },
  {
    "objectID": "lectures/Lecture19.html#rejection-regions",
    "href": "lectures/Lecture19.html#rejection-regions",
    "title": "Confidence intervals and hypothesis testing",
    "section": "Rejection regions",
    "text": "Rejection regions\nTo resolve this problem, we can specify regions that we consider implausible. That is, suppose we take a region \\(R\\) such that, if \\(\\t\\) is standard normal (or Student-T), then\n\\[\n\\prob{\\t \\in R} \\le \\alpha \\quad\\textrm{form some small }\\alpha.\n\\]\nFor example, we might take \\(\\Phi^{-1}(\\cdot)\\) to be the inverse CDF of \\(\\t\\) if \\(\\beta_k = \\beta_k^0\\). Then we can take\n\\[\nR_{ts} = \\{\\t: \\abs{t} \\ge q \\} \\quad\\textrm{where } q = \\Phi^{-1}(\\alpha / 2)\\\\\n\\]\nwhere \\(q\\) is an \\(\\alpha / 2\\) quantile of the distribution of \\(\\t\\). But there are other choices, such as\n\\[\n\\begin{aligned}\nR_{u} ={}& \\{\\t: \\t \\ge q \\} \\quad\\textrm{where } q = \\Phi^{-1}(1 - \\alpha) \\\\\nR_{l} ={}& \\{\\t: \\t \\le q \\} \\quad\\textrm{where } q = \\Phi^{-1}(\\alpha)  \\\\\nR_{m} ={}& \\{\\t: \\abs{\\t} \\le q \\} \\quad\\textrm{where } q = \\Phi^{-1}(0.5 + \\alpha / 2) \\quad\\textrm{(!!!)}\\\\\nR_{\\infty} ={}& \\begin{cases}\n\\emptyset & \\textrm{ with independent probability } \\alpha  \\\\\n(-\\infty,\\infty) & \\textrm{ with independent probability } 1 - \\alpha  \\\\\n\\end{cases} \\quad\\textrm{(!!!)}\n\\end{aligned}\n\\]\nThe last two may seem silly, but they are still rejection regions into which \\(\\t\\) is unlikely to fall if it has a standard normal distribution.\nHow can we think about \\(\\alpha\\), and about the choice of the region? Recall that\n\nIf \\(\\t \\in R\\), we “reject” the proposed value of \\(\\beta_k^0\\)\nIf \\(\\t \\notin R\\), we “fail to reject” the given value of \\(\\beta_k^0\\).\n\nOf course, we don’t “accept” the value of \\(\\beta_k^0\\) in the sense of believing that \\(\\beta_k^0 = \\beta_k\\) — if nothing else, there will always be multiple values of \\(\\beta_k^0\\) that we do not reject, and \\(\\beta_k\\) cannot be equal to all of them.\nSo there are two ways to make an error:\n\nType I error: We are correct and \\(\\beta_k = \\beta_k^0\\), but \\(\\t \\in R\\) and we reject\nType II error: We are incorrect and \\(\\beta_k \\ne \\beta_k^0\\), but \\(\\t \\notin R\\) and we fail to reject\n\nBy definition of the region \\(R\\), we have that\n\\[\n\\prob{\\textrm{Type I error}} \\le \\alpha.\n\\]\nThis is true for all the regions above, including the silly ones!\nWhat about the Type II error? It must depend on the “true” value of \\(\\beta_k\\), and on the shape of the rejection region we choose. Note that\n\\[\n\\t = \\frac{\\betahat_k - \\beta_k^0}{\\sqrt{\\hat\\v / N}} =\n\\frac{\\betahat_k - \\beta_k}{\\sqrt{\\hat\\v / N}} + \\frac{\\beta_k - \\beta_k^0}{\\sqrt{\\hat\\v / N}}\n\\]\nSo if the true value \\(\\beta_k \\gg \\beta_k^0\\), then our \\(\\t\\) statistic is too large, and so on.\nFor example:\n\nSuppose \\(\\beta_k \\gg \\beta_k^0\\).\n\nThen \\(\\t\\) is too large and positive.\n\n\\(R_u\\) and \\(R_{ts}\\) will reject, but \\(R_l\\) will not.\nThe Type II error of \\(R_u\\) will be lowest, then \\(R_{ts}\\), then \\(R_l\\).\n\\(R_l\\) actually has greater Type II error than the silly regions, \\(R_\\infty\\) and \\(R_m\\).\n\nSuppose \\(\\beta_k \\ll \\beta_k^0\\).\n\nThen \\(\\t\\) is too large and negative.\n\n\\(R_l\\) and \\(R_{ts}\\) will reject, but \\(R_u\\) will not.\nThe Type II error of \\(R_l\\) will be lowest, then \\(R_{ts}\\), then \\(R_u\\).\n\\(R_u\\) actually has greater Type II error than the silly regions, \\(R_\\infty\\) and \\(R_m\\).\n\nSuppose \\(\\beta_k = \\beta_k^0 + \\delta\\) for some very small \\(\\delta\\).\n\nThen \\(\\t\\) has about the same distribution as when \\(\\beta_k^0 = \\beta_k\\).\nAll the regions reject just about as often as we commit a Type I error, that is, a proportion \\(\\alpha\\) of the time.\n\n\nThus the shape of the region determines which alternatives you are able to reject. The probability of “rejecting” under a particular alternative is called the “power” of a test; the power is one minus the Type II error rate."
  },
  {
    "objectID": "lectures/Lecture19.html#the-null-and-alternative",
    "href": "lectures/Lecture19.html#the-null-and-alternative",
    "title": "Confidence intervals and hypothesis testing",
    "section": "The null and alternative",
    "text": "The null and alternative\nStatistics has some formal language to distinguish between the “guess” \\(\\beta_k^0\\) and other values.\n\nThe guess \\(\\beta_k^0\\) is called the “null hypothesis”\n\nFalsely rejecting the null hypothesis is called a Type I error\nBy construction, Type I errors occurs with probability at most \\(\\alpha\\)\n\nThe class of potential other values of \\(\\beta_k\\) is called the “alternative hypothesis.”\n\nFalsely failling to reject the null hypothesis is called a Type II error\nType II errors’ probability depends on the alternative(s) and the rejection region shape.\n\n\nThe choice of a test statistic (here, \\(\\t\\)), together with a rejection region (here, \\(R\\)) constitute a “test” of the null hypothesis. In general, one can imagine constructing many different tests, with different theoretical guarantees and power."
  },
  {
    "objectID": "lectures/Lecture17.html",
    "href": "lectures/Lecture17.html",
    "title": "Interpreting the coefficients and R output",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\n\\(\\,\\)\nThe reading for this section will be sections 10.1 – 10.4 of “Regression and Other Stories” by Gelman, Hill, and Vehtari. The book is freely available here as a pdf, which can also be accessed, along with other materials, at the book webpage.\nThe book takes a Baysian perspective, and our class is taking a frequentist perspective, but that will not matter for the purposes of this discussion. (I am happy to talk about the differences and commonalities between the two approaches, but I don’t plan to make Bayesian statistics a central part of the course content.)"
  },
  {
    "objectID": "lectures/Lecture15.html",
    "href": "lectures/Lecture15.html",
    "title": "Machine learning assumptions and the sandwich covariance matrix",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\n\\(\\,\\)\n\nGoals\n\nStudy what changes for the prediction model under the heteroskedastic and machine learning assumptions\n\nThe sandwich covariance matrix\nThe loss function perspective on prediction\n\n\n\n\nA hierarchy of assumptions\nHere is the hierarchy of assumptions we introduced earlier.\n\nGaussian assumption: Assume that \\(\\y_n = \\x_n^\\trans \\betav + \\res_n\\) for some \\(\\beta\\), where \\(\\res_n \\sim \\gauss{0, \\sigma^2}\\), and \\(\\res_n\\) is independent of \\(\\x_n\\).\nHomeskedastic assumption: Assume that \\(\\y_n = \\x_n^\\trans \\betav + \\res_n\\) for some \\(\\beta\\), where \\(\\expect{\\res_n} = 0\\), \\(\\var{\\res_n} = \\sigma^2 &lt; \\infty\\), and \\(\\res_n\\) is independent of \\(\\x_n\\).\nHeteroskedastic assumption: Assume that \\(\\y_n = \\x_n^\\trans \\betav + \\res_n\\) for some \\(\\beta\\), where \\(\\expect{\\res_n | \\xv_n} = 0\\) and \\(\\var{\\res_n \\vert \\xv_n} = \\sigma_n^2 &lt; \\infty\\).\nMachine learning assumption: Assume that \\(\\y_n = f(\\x_n) + \\res_n\\) for some \\(f\\), where \\(\\expect{\\res_n | \\xv_n} = 0\\) and \\(\\var{\\res_n \\vert \\xv_n} = \\sigma_n^2 &lt; \\infty\\).\n\n\n\n\n\n\n\nDefinition\n\n\n\nTo save space, from now on I’ll write \\(\\Xcovhat := \\meann \\xv_n\\xv_n^\\trans, = \\frac{1}{N} \\X^\\trans \\X\\). In this class, we’ll typically be operating under the assumption that \\(\\Xcovhat \\rightarrow \\Xcov\\) as \\(N \\rightarrow \\infty\\).\n\n\n\n\nHeteroskedastic assumption\nUnder the heteroskedastic assumption, we keep \\(\\y_n = \\x_n^\\trans \\betav + \\res_n\\), with \\(\\expect{\\res_n} = 0\\), but the variance of \\(\\res_n\\) might depend on \\(\\xv_n\\): \\(\\var{\\res_n \\vert \\xv_n} = \\sigma_n^2 &lt; \\infty\\). Importantly, the residuals are no longer independent of \\(\\xv_n\\). Furthermore, even if we knew \\(\\betav\\), the distribution of \\(\\y_new - \\betav^\\trans \\xv_\\new\\) now depends on \\(\\xv_n\\). Estimating this distribution is complicated. For example, even if you assume that \\(\\res_\\new\\) is normally distributed, you would need to model and estimate the dependence \\(\\xv_\\new \\mapsto \\var{\\res_\\new | \\xv_\\new}\\). This is beyond the scope of this class.\nHowever, maybe surprisingly, we can still study the limiting distribution of \\(\\betahat - \\betav\\), which will be particularly useful for inference. Recall that the first step of deriving the limiting distribution of \\(\\betahat - \\beta\\) using the CLT was to write\n\\[\n\\begin{aligned}\n\\sqrt{N} (\\betahat - \\beta) = \\Xcovhat^{-1} \\cltn \\xv_n \\res_n.\n\\end{aligned}\n\\]\nWe then applied the CLT to \\(\\cltn \\xv_n \\res_n\\), using the fact that \\(\\cov{\\xv_n \\res_n} = \\sigma^2 \\Xcov\\) under the homoskedastic assumptions. Under heteroskedastic assumptions, we now have\n\\[\n\\cov{\\xv_n \\res_n} = \\expect{\\xv_n \\xv_n^\\trans \\res_n^2} =\n\\expect{\\xv_n \\xv_n^\\trans \\var{\\res_n | \\xv_n}},\n\\]\nwhich is complicated. However, it can be consistently estimated by the simple expression\n\\[\n\\meann \\xv_n \\xv_n^\\trans \\res_n^2 \\rightarrow \\cov{\\xv_n \\res_n},\n\\]\nas long as we can apply the LLN to each entry of the corresponding matrix.\n\n\n\n\n\n\nExercise\n\n\n\nState precisely an assumption on the moments of \\(\\xv_n\\) and \\(\\res_n\\) that allows the application of the LLN in the previous expression.\n\n\nBy the continuous mapping theorem, we thus see that, under the heteroskedastic assumptions, \\[\n\\begin{aligned}\n\\sqrt{N} (\\betahat - \\beta) \\rightarrow{}& \\gauss{\\zerov, \\Covsand}\n  \\quad\\textrm{where}\\\\\n\\Covsand :={}& \\Xcov^{-1}  \\cov{\\xv_n \\res_n} \\Xcov^{-1}\n    \\quad\\textrm{and}\\\\\n\\Covsandhat :={}& \\Xcovhat^{-1}  \\left( \\meann \\xv_n \\xv_n^\\trans \\res_n^2 \\right) \\Xcovhat^{-1}\n  \\rightarrow \\Covsand.\n\\end{aligned}\n\\]\nThe matrix \\(\\Covsand\\) is often called the “sandwich covariance matrix” and \\(\\Covsandhat\\) the “sandwich covariance estimator,”” since the \\(\\Xcov\\) term is like the “bread” and the \\(\\cov{\\xv_n \\res_n}\\) is like the “meat”. It’s also known as the “Huber-White covariance estimator” (after some of its first promoters), or the “heteroskedasticity-robust” covariance estimator, since it will give correct asymptotic intervals for the OLS coefficients even in the presence of heteroskedasticity.\nSandwich covariance estimators can be computed in R using the sandwich package.\nIt is often — but not necessarily — the case that sandwich covariance estimates are larger than the “standard” homoskedastic estimates. You therefore pay a price for robustness. It is also the case that sandwich covariance matrices can be more unstable for small \\(N\\).\n\n\n\n\n\n\nExercise\n\n\n\nWhen \\(P = 1\\) (so \\(\\xv_n\\) is a scalar), prove that it is possible for \\(\\Covsandhat &lt; \\sigmahat^2 \\Xcovhat^{-1}\\).\n\n\n\n\nMachine learning assumption\nWhat can we say about the OLS coefficients when we do not assume that \\(\\y_n = \\beta^\\trans \\xv_n + \\res_n\\) for any \\(\\beta\\)? In that case we have no model to compare to. However, we can still try to find the best linear estimator in terms of squared loss.\nSuppose we simply want to have a good fit to training data. That is, given some \\(\\y_\\new\\), we want to find a best guess \\(f(\\xv_\\new)\\) for the value of \\(\\y_\\new\\): specifically, we want to find the function that minimizes the “loss”\n\\[\n\\hat{f}(\\cdot) := \\argmin{f} \\expect{\\left( \\y_\\new - f(\\xv_\\new) \\right)^2}.\n\\]\nThis is a functional optimization problem over an infinite dimensional space! You can solve this using some fancy math, but you can also prove directly that the best choice is\n\\[\n\\hat{f}(\\xv_\\new)  = \\expect{\\y_\\new | \\xv_\\new}.\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nProve that any other \\(f(\\cdot)\\) results in larger loss.\n\n\nOf course, we don’t know the functional form of \\(\\expect{\\y_\\new | \\xv_\\new}\\). But suppose we approximate it with\n\\[\n\\expect{\\y_\\new | \\xv_\\new} \\approx \\betav^\\trans \\xv_\\new.\n\\]\nNote that we are not assuming this is true, so this is different from our “correct specification” assumption before that \\(\\expect{\\y_\\new} = \\betav^\\trans \\xv_\\new\\). Rather, we are finding the best approximation to the unknown \\(\\expect{\\y_\\new | \\xv_\\new}\\) amongst the class of functions of the form \\(\\betav^\\trans \\xv_\\new\\).\nUnder this approximation, the problem becomes \\[\n\\begin{aligned}\n\\betastar :={}& \\argmin{\\beta} \\expect{\\left( \\y_\\new - \\betav^\\trans\\xv_\\new \\right)^2} \\\\\n={}& \\argmin{\\beta} \\left(\\expect{\\y_\\new^2} - 2 \\betav^\\trans \\expect{\\y_\\new \\xv_\\new} +\n  \\betav^\\trans \\expect{\\xv_\\new \\xv_\\new^\\trans} \\betav  \\right).\n\\end{aligned}\n\\]\nDifferentiating with respect to \\(\\beta\\) and solving gives\n\\[\n\\betastar = \\expect{\\xv_\\new \\xv_\\new^\\trans} ^{-1} \\expect{\\y_\\new \\xv_\\new}.\n\\]\nThis might look familiar. In fact, recall our first “useless” application of the LLN to \\(\\betahat\\):\n\\[\n\\betahat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y \\rightarrow \\expect{\\xv_\\new \\xv_\\new^\\trans} ^{-1} \\expect{\\y_\\new \\xv_\\new} = \\betastar.\n\\]\nFinally, we have an interpretation — our \\(\\betahat\\) converges to the minimizer of the (intractable) expected loss. Why is this? We can’t compute the expectated loss, since we don’t know the joint distribution of \\(\\xv_\\new\\) and \\(\\y_\\new\\). However, if we have a training set, we can approximate this expectation:\n\\[\n\\betahat :=\n\\argmin{\\beta} \\meann \\left( \\y_n - \\betav^\\trans\\xv_n \\right)^2\n\\approx\n\\argmin{\\beta} \\expect{\\left( \\y_\\new - \\betav^\\trans\\xv_\\new \\right)^2}\n= \\betastar.\n\\]\nHere, we are not positing any true model — we are simply assuming that the draws \\((\\y_n, \\xv_n)\\) are IID, and using an approximate loss. Still, we can analyze \\(\\betahat\\)’s asymptotic behavior, using the fact that it minimizes the empirical loss.\nDefine the gradient of the loss function: \\[\nG(\\beta) := \\frac{\\partial}{\\partial \\beta} \\meann \\left( \\y_n - \\betav^\\trans\\xv_n \\right)^2\n= -2 \\meann \\left( \\y_n - \\betav^\\trans\\xv_n \\right) \\xv_n.\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nProve that \\(G(\\betahat) = \\zerov\\) and \\(\\expect{G(\\betastar)} = 0\\).\n\n\nNoting that \\(G(\\beta)\\) is linear in \\(\\beta\\) (so the second derivative with respect to \\(\\beta\\) is zero), we can Taylor expand \\(G(\\cdot)\\) around \\(\\betahat\\), giving\n\\[\nG(\\betastar) = G(\\betahat) + \\frac{\\partial G}{\\partial \\beta^\\trans} (\\betahat) (\\betastar - \\betahat).\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nDerive the previous equation without using a Taylor series using \\[\n\\begin{aligned}\n\\meann \\left( \\y_n - \\betavhat^\\trans\\xv_n \\right) \\xv_n ={}& \\zerov \\Rightarrow \\\\\n\\meann \\left( \\y_n - \\betavhat^\\trans\\xv_n \\right) \\xv_n  -\n  \\meann \\left( \\y_n - \\betastar^\\trans\\xv_n \\right) \\xv_n\n  ={}& - \\meann \\left( \\y_n - \\betastar^\\trans\\xv_n \\right) \\xv_n,\n\\end{aligned}\n\\] and collecting terms. The Taylor series version is more general, since the analysis here can also be applied to non-linear gradients as long as \\(\\betavhat - \\betastar\\) is small and the second derivative bounded, so that the first-order series expansion becomes accurate to leading order as \\(N\\) grows.\n\n\nUsing the fact that \\(G(\\betahat) = \\zerov\\), we can solve\n\\[\n\\sqrt{N} (\\betahat - \\betastar) = - \\left( \\frac{\\partial G}{\\partial \\beta^\\trans}  (\\betahat) \\right)^{-1} \\sqrt{N} G(\\betastar).\n\\]\nPlugging in our particular loss gradient,\n\\[\n- \\frac{\\partial G}{\\partial \\beta^\\trans} = \\meann \\xv_n \\xv_n^\\trans\n\\quad\\textrm{and}\\quad\n\\sqrt{N} G(\\betastar) =\n\\cltn (\\y_n - \\betastar^\\trans \\xv_n) \\xv_n.\n\\]\nBecause \\(\\expect{(\\y_n - \\betastar \\xv_n)^\\trans \\xv_n} = \\zerov\\), we can apply the CLT to get\n\\[\n\\sqrt{N} G(\\betastar) \\rightarrow \\gauss{\\zerov, \\cov{(\\y_n - \\betastar^\\trans \\xv_n) \\xv_n}}.\n\\]\nFinally, using the fact that \\(\\betahat \\rightarrow \\betastar\\) , we find that we can consistently estimate the covariance as\n\\[\n\\begin{aligned}\n\\meann (\\y_n - \\betahat^\\trans \\xv_n) \\xv_n ={}&\n\\meann (\\y_n - \\betastar^\\trans \\xv_n + (\\betastar - \\betahat)^\\trans \\xv_n)  \\xv_n \\\\={}&\n\\meann (\\y_n - \\betastar^\\trans \\xv_n) \\xv_n + \\left(  \\meann \\xv_n  \\xv_n^\\trans \\right) (\\betastar - \\betahat)\n\\\\\\rightarrow{}&\n\\cov{(\\y_n - \\betastar \\xv_n)^\\trans \\xv_n}.\n\\end{aligned}\n\\]\nIf we call \\(\\reshat_n := \\y_n - \\betahat^\\trans \\xv_n\\), we see that the estimator of the limiting covariance of \\(\\sqrt{N}( \\betahat- \\betastar)\\) is precisely the sandwich covariance.\nNote that we derived this result without any assumptions on the relationship between \\(\\y_n\\) and \\(\\xv_n\\), other than moment conditions allowing us to apply the LLN and CLT."
  },
  {
    "objectID": "lectures/Lecture13.html",
    "href": "lectures/Lecture13.html",
    "title": "Sampling variability of the coefficients",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\n\\(\\,\\)\n\nGoals\n\nIncorporate training set randomness into our predictions\n\nShow the traning set variability with real data\nUse the normal assumptions to derive the distribution under training sampling\nTransform to a pivotal statistic to get intervals that take training varibility into account\n\n\n\n\nSetup\nUp to now, we’ve been studying the predictive interval for \\(\\y_\\new\\) when the training data is fixed. Under our normal assumptions, we’ve shown that\n\\[\n\\y_\\new - \\yhat_\\new = \\xv_\\new(\\beta - \\betahat) + \\res_n\n  \\sim \\gauss{\\xv_\\new(\\beta - \\betahat), \\sigma^2}.\n\\]\nWe then argued that, for large \\(N\\), \\(\\beta \\approx \\betahat\\) and \\(\\sigmahat \\approx \\sigma\\). We then plugged these values in to find an interval\n\\[\nI_\\new = \\xv_\\new^\\trans \\betahat \\pm \\z_\\alpha \\sigmahat\n\\]\nsuch that \\(\\prob{\\y_\\new \\in I_\\new} \\approx 1 - \\alpha\\), where we choose \\(\\z_\\alpha\\) using the normal distribution and error rate \\(\\alpha\\).\nHere, since we’re treating our training set as fixed, it would be more precise to write\n\\[\n\\y_\\new - \\yhat_\\new \\vert \\X,\\Y\n  \\sim \\gauss{\\xv_\\new(\\beta - \\betahat), \\sigma^2},\n\\]\nsince we’re conditioning on \\(\\X\\) and \\(\\Y\\).\nNote that if we were to draw a new value for \\(\\Y\\), we would get a new value for \\(\\betahat\\) and \\(\\sigmahat\\). Correspondingly, we would get a new interval, \\(I_\\new\\). If we have drawn our training set randomly, then we might want to take the variability of the interval itself into account.\n\n\nCoefficient variability in the prediction\nRecall that we have already proven, under the normal assumption, that\n\\[\n\\betahat - \\beta \\sim \\gauss{\\zerov, \\sigma^2 (\\X^\\trans \\X)^{-1}}.\n\\]\nFrom this it follows that \\[\n\\xv_\\new^\\trans (\\betahat - \\beta) \\sim\n  \\gauss{\\zerov, \\sigma^2 \\xv_\\new^\\trans (\\X^\\trans \\X)^{-1} \\xv_\\new}.\n\\]\nThe randomness in the preceding expression comes entirely from the randomness in \\(\\resv\\), the training data, which is independent of the new residual, \\(\\res_\\new\\). So the normal random variable \\(\\xv_\\new^\\trans(\\betahat - \\beta)\\) and the normal random variable \\(\\res_\\new\\) are independent, and\n\\[\n\\y_\\new - \\yhat_\\new \\sim \\gauss{0, \\sigma^2 \\left(\\xv_\\new^\\trans (\\X^\\trans \\X)^{-1} \\xv_\\new + 1\\right)}\n\\]\nContrast this expression, which is marginal over the training data, with the condtiional version above.\n\nConditional on the training set, \\(\\y_\\new - \\yhat_\\new\\) is biased by \\(\\betahat - \\beta\\). However, it is marginally unbiased.\nThe conditional variance is strictly smaller than the marginal variance.\nBoth the bias and variance are larger when \\((\\X^\\trans \\X)^{-1} \\xv_\\new\\) is larger.\n\nFor the last point, note that\n\\[\n(\\betahat - \\beta)^\\trans \\xv_\\new = (\\X^\\trans\\Y)^\\trans (\\X^\\trans \\X)^{-1} \\xv_\\new.\n\\]\nUsing the above expression, we can form a (wider) marginal interval using the same trick as before. Rather than conditioning on the training set and hoping the error is small, this interval should cover the prediction marginally over the variability in the training set.\nTo simplify our expressions, let’s define\n\\[\n\\v^2 := \\xv_\\new^\\trans (\\X^\\trans \\X)^{-1} \\xv_\\new + 1\n\\quad\\textrm{so that}\\quad\n\\y_\\new - \\yhat_\\new \\sim \\gauss{0, \\v^2 \\sigma^2}.\n\\]\nPlugging in \\(\\sigmahat \\approx \\sigma\\), we get\n\\[\n\\frac{\\y_\\new - \\yhat_\\new}{\\v \\sigmahat} \\approx\n  \\frac{\\y_\\new - \\yhat_\\new}{\\v \\sigma} \\sim \\gauss{0,1}.\n\\]\nUsing this expression, we can form intervals for \\(\\y_new - \\yhat_\\new\\) using quantiles of the standard normal. But doing so still requires us to plug-in for \\(\\sigmahat\\). How can we account for that variability?\n\n\nAccounting for the variability in the residual variance estimate\n\n\n\n\n\n\nNote to future instructor\n\n\n\nNote: here, we are going to normalize \\(\\sigmahat\\) with \\(N-P\\) rather than \\(N\\). It would be better to be clearer about this next time.\n\n\nBefore, we rearranged this expression to get a standard distribution. Now that we know that\n\\[\n\\sigmahat^2 = \\frac{\\sigma^2}{N-P} \\s\n\\quad\\textrm{where}\\quad\n\\s \\sim \\chisq{N-P},\n\\]\nand furthermore, \\(\\sigmahat^2\\) is independent of \\(\\betahat\\). Let’s try to use this fact to get a probability distribution for \\(\\y_\\new\\) that takes the variability of \\(\\sigmahat\\) into account. Write\n\\[\n\\begin{aligned}\n\\frac{\\y_\\new - \\yhat_\\new}{\\v \\sigmahat} ={}&\n  \\frac{\\y_\\new - \\yhat_\\new}{\\v \\sqrt{\\frac{\\sigma^2}{N-P} \\s}}\n\\\\={}&\n\\left(\\frac{\\y_\\new - \\yhat_\\new}{\\v \\sigma}\\right)\n   /   \\sqrt{\\frac{\\s}{N-P} }.\n\\end{aligned}\n\\]\nAs we showed before, \\(\\frac{\\y_\\new - \\yhat_\\new}{\\v \\sigma} \\sim \\gauss{0,1}\\). Furthermore, it is independent of \\(\\s\\). The denominator is approximately \\(1\\) for large \\(N - P\\), but has some sampling variability. The ratio of a normal to an independent chi squared happens to be a known distribution.\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(\\z \\sim \\gauss{0,1}\\), and \\(\\s \\sim \\chisq{N-P}\\), independently of one another. Then the distribution of \\[\n\\t := \\frac{\\z}{\\sqrt{\\s / (N-P)}}\n\\]\nis called a ``student-t distribution with \\(N-P\\) degrees of freedom.’’ We write \\(\\t \\sim \\studentt{N - P}\\). As long as \\(N - P &gt; 2\\), \\(\\expect{\\t} = 0\\) and \\(\\var{\\t} = (N - P) / (N - P - 2)\\).\n\n\n\n\n\n\n\n\nExercise\n\n\n\nLet \\(\\t \\sim \\studentt{K}\\) with \\(K &gt; 2\\).\n\nProve that \\(\\expect{\\t} = 0\\).\nWithout using the above explicit formulat, prove that \\(\\var{\\t} &gt; 1\\).\n(Hint: use the fact that, for any positive non-constant random variable, \\(\\RV{x}\\), \\(\\expect{1 / \\RV{x}} &gt; 1 / \\expect{\\RV{x}}\\), by Jensen’s inequality.)\n\n\n\nYou can find quantiles of the student-t distributions using the R function qt(), just as you would use rnorm().\nGiven all this, we have shown that\n\\[\n\\prob{\\frac{\\y_\\new - \\yhat_\\new}{\\v \\sigmahat} \\le \\z_\\alpha}  = \\prob{\\t \\le \\z_\\alpha}\n\\quad\\textrm{where}\\quad\n\\t \\sim \\studentt{N - P}.\n\\]\nUsing this formula, we can find exact intervals for our marginal prediction error under normality."
  },
  {
    "objectID": "lectures/Lecture11.html",
    "href": "lectures/Lecture11.html",
    "title": "Consistency of OLS and the residual variance under the Gaussian assumptions",
    "section": "",
    "text": "\\(\\LaTeX\\)"
  },
  {
    "objectID": "lectures/Lecture11.html#gaussian-intervals.",
    "href": "lectures/Lecture11.html#gaussian-intervals.",
    "title": "Consistency of OLS and the residual variance under the Gaussian assumptions",
    "section": "Gaussian intervals.",
    "text": "Gaussian intervals.\nBut first, let’s talk about what we’d do if we did know \\(\\beta\\) and \\(\\sigma\\). One way of expressing uncertainty is an interval, \\((\\y_{low}, \\y_{up})\\), such that \\[\n\\prob{\\y_\\new \\in (\\y_{low}, \\y_{up})} = 1 - \\alpha\n\\]\nfor some small \\(\\alpha\\).\nFor the moment, suppose that we know \\(\\beta\\) and \\(\\sigma\\).\nHow can we find \\(\\y_{low}\\) and \\(\\y_{up}\\)? Let’s do it by building up an expression for \\(\\y_\\new\\) in terms of the standard normal distribution, whose properties we are able to calculate.\nRecall that, if \\(\\z\\) follows standard normal distribution \\(\\RV{\\z} \\sim \\gauss{0,1}\\), then we can compute the tail probability \\(\\prob{\\RV{\\z} \\le \\z_0}\\) for any \\(\\z_0\\). This probability is usually denoted with \\(\\Phi(\\cdot)\\):\n\\[\n\\Phi(\\z_0) := \\prob{\\RV{\\z} \\le \\z_0}.\n\\]\nFurthermore, we can invert \\(\\Phi(\\cdot)\\) to find a \\(\\z_0\\) such that \\(\\Phi(\\z_0)\\) is equal to some value:\n\\[\na := \\prob{\\RV{\\z} \\le \\Phi^{-1}(a)}.\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nProve that \\(\\Phi(\\cdot)\\) has the following properties:\n\n\\(\\Phi(-\\infty) = 0\\)\n\\(\\Phi(\\infty) = 1\\)\n\\(\\Phi(-\\z) = 1 - \\Phi(\\z)\\)\n\\(\\Phi(\\cdot)\\) is increasing\n\\(\\Phi(\\cdot)\\) is continuous\n\\(\\Phi(\\cdot)\\) is invertible\n\n\n\nSuppose we take \\(\\z_\\alpha := \\Phi^{-1}(\\alpha / 2)\\) so that \\(\\prob{\\RV{\\z} \\le \\z_\\alpha} = 1 - \\alpha / 2\\) for \\(\\alpha &lt; 1\\). Note that \\(\\z_\\alpha &gt; 0\\) (why?). Then\n\\[\n\\begin{aligned}\n\\prob{-\\z_\\alpha \\le \\RV{\\z} \\le \\z_\\alpha} ={}&\n1 - \\prob{\\RV{\\z} \\le -\\z_\\alpha \\textrm{ or } \\RV{\\z} \\ge \\z_\\alpha}\n\\\\={}&\n1 - \\left( \\prob{\\RV{\\z} \\le -\\z_\\alpha} + \\prob{\\RV{\\z} \\ge \\z_\\alpha} \\right)\n  &\\textrm{(the regions are disjoint)}\n\\\\={}&\n1 - 2 \\prob{\\RV{\\z} \\ge \\z_\\alpha}\n  &\\textrm{(symmetry of $\\RV{\\z}$)}\n\\\\={}&\n1 - 2 \\left(1 - \\Phi(\\z_\\alpha) \\right)\n  &\\textrm{(definition of $\\Phi$)}\n\\\\={}&\n1 - 2 \\frac{\\alpha}{2}\n  &\\textrm{(definition of $\\z_\\alpha$)}\n\\\\={}&\n1 - \\alpha.\n\\end{aligned}\n\\]\nNow we just need to transform the preceding expression, which is about a standard normal random variable, into an expression about \\(\\y_\\new\\). By standard transformations of normal random variables, \\(\\y_new\\) has the same distribution as \\(\\beta^\\trans \\xv_\\new + \\sigma \\RV{\\z}\\). So we can write\n\\[\n\\begin{aligned}\n1- \\alpha\n={}&\n  \\prob{-\\z_\\alpha \\le \\RV{\\z} \\le \\z_\\alpha}\n\\\\={}&\n  \\prob{\\beta^\\trans \\xv_\\new-\\sigma \\z_\\alpha \\le\n    \\beta^\\trans \\xv_\\new + \\sigma \\RV{\\z} \\le\n    \\beta^\\trans \\xv_\\new + \\sigma \\z_\\alpha}\n\\\\={}&\n  \\prob{\\beta^\\trans \\xv_\\new-\\sigma \\z_\\alpha \\le\n    \\y_\\new \\le\n    \\beta^\\trans \\xv_\\new + \\sigma \\z_\\alpha}.\n\\end{aligned}\n\\]\nWe have thus found an interval for \\(\\y_\\new\\):\n\\[\n\\y_{low} = \\beta^\\trans \\xv_\\new - \\sigma \\z_\\alpha\n\\quad\\textrm{and}\\quad\n\\y_{up} = \\beta^\\trans \\xv_\\new + \\sigma \\z_\\alpha\n\\quad\\Rightarrow\\quad\n\\prob{\\y_{low} \\le \\y_\\new \\le \\y_{up}} = 1- \\alpha.\n\\]\nIn practice, we replace \\(\\sigma\\) with \\(\\sigmahat\\) and \\(\\beta\\) with \\(\\betahat\\), giving\n\\[\n\\hat\\y_{low} = \\betahat^\\trans \\xv_\\new - \\sigmahat \\z_\\alpha\n\\quad\\textrm{and}\\quad\n\\hat\\y_{up} = \\betahat^\\trans \\xv_\\new + \\sigmahat \\z_\\alpha.\n\\]\nIn general, \\(\\prob{\\hat\\y_{low} \\le \\y_\\new \\le \\hat\\y_{up}} \\ne 1 - \\alpha\\). It may be higher or lower. In fact, the probability is random, as it depends on \\(\\betahat\\) and \\(\\sigmahat\\), which depend on the (random) training data. But the probability does approach \\(1- \\alpha\\) as \\(N \\rightarrow \\infty\\) as long as \\(\\betahat \\rightarrow \\beta\\) and \\(\\sigmahat \\rightarrow \\sigma\\). So we now turn to showing that.\n\n\n\n\n\n\nExercise\n\n\n\nIf it is the case that \\(\\betahat \\rightarrow \\beta\\) and \\(\\sigmahat \\rightarrow \\sigma\\), then\n\\[\n\\prob{\\hat\\y_{low} \\le \\y_\\new \\le \\hat\\y_{up}} \\rightarrow 1 - \\alpha\n\\]\nby the continuous mapping theorem applied to \\(\\Phi(\\cdot)\\)."
  },
  {
    "objectID": "lectures/Lecture11.html#method-1-for-coefficient-consistency-use-normality",
    "href": "lectures/Lecture11.html#method-1-for-coefficient-consistency-use-normality",
    "title": "Consistency of OLS and the residual variance under the Gaussian assumptions",
    "section": "Method 1 for coefficient consistency: Use normality",
    "text": "Method 1 for coefficient consistency: Use normality\nPlugging in, we get \\[\n\\betahat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y =\n(\\X^\\trans \\X)^{-1} \\X^\\trans (\\X \\beta + \\resv) =\n\\beta + (\\X^\\trans \\X)^{-1} \\X^\\trans \\resv.\n\\]\nIn this expression, only \\(\\resv\\) is random! In fact, since \\(\\resv\\) is Gaussian, and \\(\\betahat\\) is an affine transformation of \\(\\resv\\), then \\(\\betahat\\) itself is Gaussian — albeit with an unknown mean and variance. Noting that\n\\[\n\\begin{aligned}\n\\expect{\\betahat} ={}& \\beta + (\\X^\\trans \\X)^{-1} \\X^\\trans \\expect{\\resv} = \\beta \\\\\n\\cov{\\betahat} ={}& \\expect{(\\betahat - \\beta) (\\betahat - \\beta)^\\trans}\n\\\\ ={}& \\expect{(\\X^\\trans \\X)^{-1} \\X^\\trans \\resv \\resv^\\trans \\X (\\X^\\trans \\X)^{-1} }\n\\\\ ={}& (\\X^\\trans \\X)^{-1} \\X^\\trans \\expect{\\resv \\resv^\\trans} \\X (\\X^\\trans \\X)^{-1}\n\\\\ ={}& (\\X^\\trans \\X)^{-1} \\X^\\trans \\sigma^2 \\id{} \\X (\\X^\\trans \\X)^{-1}\n\\\\ ={}& \\sigma^2 (\\X^\\trans \\X)^{-1} \\X^\\trans \\X (\\X^\\trans \\X)^{-1}\n\\\\ ={}& \\sigma^2 (\\X^\\trans \\X)^{-1},\n\\end{aligned}\n\\]\nwe get the beautiful expression \\[\n\\betahat \\sim \\gauss{\\beta, \\sigma^2 (\\X^\\trans \\X)^{-1}}.\n\\]\nWe still need to understand how \\((\\X^\\trans \\X)^{-1}\\) behaves. For this, we have assumption A3, which we apply to get \\[\n\\betahat \\sim \\gauss{\\beta, \\frac{1}{N} \\sigma^2 \\left(\\frac{1}{N} \\X^\\trans \\X \\right)^{-1}}\n\\approx \\gauss{\\beta, \\frac{1}{N} \\sigma^2 \\Sigmam_\\X^{-1}}.\n\\]\nIt follows that the variance of \\(\\betahat\\) goes to zero as \\(N \\rightarrow \\infty\\), and so it concentrates at its mean, \\(\\beta\\).\n\n\n\n\n\n\nExercise\n\n\n\nHow would the OLS solution behave if the A3 were violated? Note that there are two parts — positive definiteness of the limit, and existence of the limit when scaling \\(\\X^\\trans \\X\\) by \\(1/N\\).\nConsider two examples:\n\nLet \\(\\zv_n \\sim \\gauss{\\zerov, \\id}\\) denote IID standard Gaussian P-vectors, and take \\(\\xv_n = \\zv_n / n^2\\). Show that A3 is violated. What happens to \\(\\betahat\\) as \\(N \\rightarrow \\infty\\).\nLet \\(\\xv_n = \\onev\\) for all \\(n\\). Show that A3 is violated. What is \\(\\betahat\\)?"
  },
  {
    "objectID": "lectures/Lecture11.html#method-2-for-coefficient-consistency-use-the-lln",
    "href": "lectures/Lecture11.html#method-2-for-coefficient-consistency-use-the-lln",
    "title": "Consistency of OLS and the residual variance under the Gaussian assumptions",
    "section": "Method 2 for coefficient consistency: Use the LLN",
    "text": "Method 2 for coefficient consistency: Use the LLN\nThe previous proof really relied on Normality. However, we can get a similar result using only the LLN. Write:\n\\[\n\\begin{aligned}\n\\betahat - \\beta ={}&  \n  \\left(\\X^\\trans \\X\\right)^{-1} \\X^\\trans \\resv\n\\\\={}&\n  \\left(\\frac{1}{N} \\X^\\trans \\X\\right)^{-1} \\frac{1}{N} \\X^\\trans \\resv.\n\\end{aligned}\n\\]\nWe’ve already assumed that\n\\[\n\\left(\\frac{1}{N} \\X^\\trans \\X \\right)^{-1} \\rightarrow \\Sigmam_\\X^{-1}.\n\\]\nWhat about the second term? This is something to which we can apply the vector LLN. If we write \\(\\zv_n := \\xv_n \\res_n\\), then \\(\\expect{\\zv_n} = \\zerov\\) and\n\\(\\cov{\\zv_n} = \\sigma^2 \\xv_n \\xv_n^\\trans\\). If we assume that \\(\\xv_n \\xv_n^\\trans\\) is bounded for all \\(n\\), then we can apply our LLN to get\n\\[\n\\frac{1}{N} \\X^\\trans \\resv = \\meann \\xv_n \\resv_n \\rightarrow \\expect{\\xv_1 \\resv_1} = \\xv_1  \\expect{\\resv_1} = \\zerov.\n\\]\nFrom this it follows that\n\\[\n\\betahat \\rightarrow \\beta \\quad\\textrm{as }N \\rightarrow \\infty\n\\]\nby the LLN alone.\n\n\n\n\n\n\nExercise\n\n\n\nIn fact, we do not require \\(\\xv_n \\xv_n^\\trans\\) is bounded for all \\(n\\) — all we really need is \\(\\max_{n\\in\\{1, \\ldots, N\\},p} \\frac{1}{N} \\xv_{np}^2  \\rightarrow 0\\) as \\(N \\rightarrow \\infty\\). Refer back to our proof of the LLN and show that (a) this condition suffices for the LLN and (b) that it is implied by our “nice regressor” assumption."
  },
  {
    "objectID": "lectures/Lecture11.html#method-3-for-coefficient-consistency-plug-into-the-general-formula",
    "href": "lectures/Lecture11.html#method-3-for-coefficient-consistency-plug-into-the-general-formula",
    "title": "Consistency of OLS and the residual variance under the Gaussian assumptions",
    "section": "Method 3 for coefficient consistency: Plug into the general formula",
    "text": "Method 3 for coefficient consistency: Plug into the general formula\nAt the beginning, we derived the (initially disappointing) general formula\n\\[\n\\betahat \\rightarrow \\Xcov^{-1} \\expect{\\xv_1 \\y_1}.\n\\]\nIf we revert to letting \\(\\xv_n\\) be random, then we can simply plug in to get\n\\[\n\\begin{aligned}\n\\betahat \\rightarrow{}& \\Xcov^{-1} \\expect{\\xv_1 (\\xv_1^\\trans \\beta + \\res_1)}\n\\\\={}& \\Xcov^{-1} \\expect{\\xv_1 \\xv_1^\\trans} \\beta + \\Xcov^{-1} \\expect{\\xv_1 \\res_1}\n\\\\={}& \\Xcov^{-1} \\Xcov \\beta + \\Xcov^{-1} \\expect{\\xv_1 \\expect{\\res_1 | \\xv_1}}\n\\\\={}& \\beta + \\Xcov^{-1} \\expect{\\xv_1 0}\n\\\\={}& \\beta.\n\\end{aligned}\n\\]\nThough superficially different, all three of these methods relied on the same facts behind the scenes — namely, that \\(\\expect{\\betahat} = \\beta\\), and \\(\\cov{\\betahat} \\rightarrow \\zerov\\) as \\(N \\rightarrow \\infty\\)."
  },
  {
    "objectID": "lectures/Lecture11.html#method-1-apply-lln-to-the-squared-residuals",
    "href": "lectures/Lecture11.html#method-1-apply-lln-to-the-squared-residuals",
    "title": "Consistency of OLS and the residual variance under the Gaussian assumptions",
    "section": "Method 1: Apply LLN to the squared residuals",
    "text": "Method 1: Apply LLN to the squared residuals\nHow can we estimate \\(\\sigma^2\\)? First, note that, if we observed the residuals \\(\\res_n\\) (which we don’t), we could estimate\n\\[\n\\meann \\res_n^2 \\rightarrow \\sigma^2\n\\]\nby the LLN. Of course, we don’t observe \\(\\res_n\\), since we don’t know \\(\\beta\\), and \\(\\res_n = \\y_n - \\xv_n^\\trans\\beta\\). However, we can estimate \\(\\beta\\) with \\(\\betahat\\), giving\n\\[\n\\reshat_n := \\y_n - \\xv_n^\\trans \\betahat,\n\\]\nand estimate\n\\[\n\\sigmahat^2 := \\meann \\reshat_n^2.\n\\]\nDoes this work? Do we commit too large an error? It turns out, no. We can write\n\\[\n\\begin{aligned}\n\\meann \\reshat_n^2 ={}& \\meann (\\y_n - \\xv_n^\\trans \\betahat)^2\n\\\\={}&\n\\meann (\\res_n + \\xv_n^\\trans \\beta - \\xv_n^\\trans \\betahat)^2 & \\textrm{(by A1)}\n\\\\={}&\n\\meann (\\res_n + \\xv_n^\\trans (\\beta - \\betahat))^2\n\\\\={}&\n\\meann \\res_n^2 + 2 (\\beta - \\betahat)^\\trans \\meann \\xv_n \\varepsilon_n +\n  (\\beta - \\betahat)^\\trans \\meann \\xv_n \\xv_n^\\trans (\\beta - \\betahat).\n\\end{aligned}\n\\]\nBy A3, both \\(\\meann \\xv_n \\varepsilon_n\\) and \\(\\meann \\xv_n \\xv_n^\\trans\\) converge to something bounded, and we showed above that \\(\\beta - \\betahat \\rightarrow 0\\), and that \\(\\meann \\res_n^2 \\rightarrow \\sigma^2\\) by the LLN. It follows that\n\\[\n\\sigmahat^2 \\rightarrow \\sigma^2,\n\\]\nas hoped."
  },
  {
    "objectID": "lectures/Lecture1.html",
    "href": "lectures/Lecture1.html",
    "title": "Class organization",
    "section": "",
    "text": "This is a course about linear models. You probably all know what linear models are already — in short, they are models which fit straight lines through data, possibly high-dimensional data. Every setting we consider in this class will have the following attributes:\n\nA bunch of data points. We’ll index with \\(n = 1, \\ldots, N\\).\nEach datapoint consists of:\n\nA scalar-valued “response” \\(y_n\\)\nA vector-valued “regressor” \\(\\xv_n = (\\x_{n1}, \\ldots, \\x_{nP})\\).\n\n\n\n\n\n\n\n\nNotation\n\n\n\nThroughout the course, I will (almost) always use the letter “x” for regressors and the letter “y” for responses. There will always be \\(N\\) datapoints, and the regressors will be \\(P\\) dimensional. Vectors and matrices will be boldface.\nOf course, I may deviate from this (and any) notation convention by saying so explicitly.\n\n\nWe will be interested in what \\(\\xv_n\\) can tell us about \\(\\y_n\\). This setup is called a “regression problem,” and can be attacked with lots of models, including non-linear models. But we will focus on approaches to this problem that operate via fitting straight lines to the dependence of \\(y_n\\) on \\(\\xv_n\\).\nRelative to a lot of other machine learning or statistical procedures, linear models are relatively easy to analyze and understand. Yet they are also complex enough to exhibit a lot of the strengths and pitfalls of all machine learning and statistics. So really, this is only partly a course about linear models per se. I hope to make it a course about concepts in statistics in machine learning more generally, but viewed within the relatively simple framework of linear models. Some examples that I hope to touch on at least briefly include:\n\nAsymptotics under misspecification\nRegularization\nSparsity\nThe bias / variance tradeoff\nThe influence function\nDomain transfer\nDistributed learning\nConformal inference\nPermutation testing\nBayesian methods\nBenign overfitting\n\nOur results and conclusions will be expressed in formal mathematical statements and in software. For the purpose of this class, I view mathematics and coding as analogous to language, grammar, and style: you need to have a command of these things in order to say something. But the content of this course doesn’t stop and math and conding, just as learning language alone does not give you something to say. Linear models will be a mathematical and computational tool for communicating with and about the real world. Datasets can speak to us in the language of linear models, and we can communicate with other humans through the language of linear models. Learning to communicate effectively in this way is the most important content of this course, and is a skill that will remain relevent whether or not you ever interpret or fit another linear model in your life.\nWhether or not a statistical analysis is “good” cannot be evaluated outside a particular context. Why do we care about the conclusions of this analysis? What will they be used for? Who needs to understand our analysis? What are the consequences of certain kinds of errors? Outside of a classroom, you will probably never encounter a linear model without a real question and context attached to it. I will make a real effort in this class to respect this fact, and always present data in context, to the extent possible within a classroom setting. I hope you will in turn get in the habit of always thinking about the context of a problem, even when going through formal homework and lab exercises. For pedagogical reasons we may have to step into abstract territory at times, but I will make an effort to tie what we learn back to reality, and, in grading we’ll make sure to reward your efforts to do so as well. Just as there is not a “correct” essay in an English class, this will often mean that there are not “correct” analyses for a dataset, even though there are certainly better and worse approaches, as well as unambiguous errors."
  },
  {
    "objectID": "lectures/Lecture1.html#exploratory-data-analysis",
    "href": "lectures/Lecture1.html#exploratory-data-analysis",
    "title": "Class organization",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nSpotify example\nEDA is part of every project (start by plotting your data)\nOften a starting point for more detailed analyses\nAnything goes, but correpsondingly the results may not be that meaningful\nHelpful to have a formal understanding of what regression is doing\nLinear algebra perspective"
  },
  {
    "objectID": "lectures/Lecture1.html#prediction-problems",
    "href": "lectures/Lecture1.html#prediction-problems",
    "title": "Class organization",
    "section": "Prediction problems",
    "text": "Prediction problems\n\nBodyfat example\nHave some pairs of responses and explanatory variables\nGiven new explanatory variables, we want a “best guess” for an unseen response\nWe care about how our model fits the data we have, and how it extrapolates\nThe model itself (i.e., the slopes of the fitted line) doesn’t have much meaning\nCare about uncertainty in and calibration of our prediction\nLoss minimization perspective"
  },
  {
    "objectID": "lectures/Lecture1.html#inference-problems",
    "href": "lectures/Lecture1.html#inference-problems",
    "title": "Class organization",
    "section": "Inference problems",
    "text": "Inference problems\n\nAluminum stress-strain curve example\nWe have a question about the world that can’t be expressed as pure prediction\nSometimes we “reify” our model, even if tentatively\nSometimes has a causal intepretation: if we intervene in some aspect of the world, what will happen?\nWe need some notion of the subjective uncertainty of our estimates\nMaximum likelihood perspective\n\nThese perspectives are highly overlapping, and often a problem doesn’t fit neatly into one or the other. For example, good inference should give good predictions, and inference in a very tentatively reified model is close to exploratory data analysis. Still, I’ll lean on this division to help organize the course conceptually.\nWe can look at these examples in Lecture1_examples.ipynb."
  },
  {
    "objectID": "datasets/data.html",
    "href": "datasets/data.html",
    "title": "Datasets",
    "section": "",
    "text": "The following datasets are used in lectures, homework, and labs.",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "datasets/data.html#aluminum-dataset",
    "href": "datasets/data.html#aluminum-dataset",
    "title": "Datasets",
    "section": "Aluminum dataset",
    "text": "Aluminum dataset\nDataset containing stress-strain curves for commercially available aluminum samples at varying tempertures. The data accompanies B.S. Aakash, JohnPatrick Connors, Michael D. Shields, Variability in the thermo-mechanical behavior of structural aluminum, Thin-Walled Structures, Volume 144, 2019, 106122, ISSN 0263-8231. (link)\nPaper abstract: The nominal performance of structural aluminum alloys at elevated temperature has been thoroughly investigated in the past. Although it is well known that the performance of a given material specimen will differ from the nominal behavior, the extent of this variability has not been quantitied to date. This limits the ability to perform reliability and performance-based design and analysis for aluminum structures subjected to high temperatures (e.g. in structural fire engineering). This work presents an experimental investigation of the variability in the stress-strain behavior of AA 6061-T651 (as a model ductile aluminum alloy). We performed steady-state tensile tests on nine different batches of nominally identical material sourced from different suppliers/manufacturers at six different temperatures (20 °C, 100 °C, 150 °C, 200 °C, 250 °C, and 300 °C) under two different geometries to induce uniaxial tension and plane strain stress states in the gauge section. The results are investigated statistically to illustrate variability in the salient features of the stress-strain behavior of the material ranging from nonlinear elastic behavior to strain localization and ductile fracture. Some observations on material performance and its variability are made along the way. Overall, it is illustrated that variations between batches of material can be quite large and – especially as it relates to strain localization, necking, and material failure – variations can be very large even within a fixed batch of material. To encourage data of this nature to be expanded and integrated into research and practice to improve structural design and investigations, the full searchable dataset are publicly available with experimental details published concurrently through Data in Brief.\n\nLink to csv\nSource link (downloaded Dec 2023)",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "datasets/data.html#bodyfat-dataset",
    "href": "datasets/data.html#bodyfat-dataset",
    "title": "Datasets",
    "section": "Bodyfat dataset",
    "text": "Bodyfat dataset\nBodyfat and other physical measurements on a number of individuals.\nMeasurement standards are apparently those listed in Benhke and Wilmore (1974), pp. 45-48 where, for instance, the abdomen 2 circumference is measured “laterally, at the level of the iliac crests, and anteriorly, at the umbilicus”.\nThese data are used to produce the predictive equations for lean body weight given in the abstract “Generalized body composition prediction equation for men using simple measurement techniques”, K.W. Penrose, A.G. Nelson, A.G. Fisher, FACSM, Human Performance Research Center, Brigham Young University, Provo, Utah 84602 as listed in Medicine and Science in Sports and Exercise, vol. 17, no. 2, April 1985, p. 189.\n\nLink to csv\nSource link (downloaded Dec 2023)",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "datasets/data.html#spotify-dataset",
    "href": "datasets/data.html#spotify-dataset",
    "title": "Datasets",
    "section": "Spotify dataset",
    "text": "Spotify dataset\nThis dataset consists of roughly 30,000 Songs from the Spotify API with black-box machine learning quantifications of musical features. No guarantees are made on how the tracks were sampled.\n\nLink to csv\nSource link (downloaded Dec 2023)",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar",
    "section": "",
    "text": "We can just embed the iframe html:"
  },
  {
    "objectID": "assignments/hw5_math.html",
    "href": "assignments/hw5_math.html",
    "title": "STAT151A Homework 5: Due March 22nd",
    "section": "",
    "text": "\\(\\,\\)\n\n1 Reviewing the distribution of \\(\\hat{\\beta}\\) under different assumptions\nThis homework question will reference the following assumptions.\nRegressor assumptions:\n\nR1: The \\(N \\times P\\) matrix \\(\\boldsymbol{X}\\), which has \\(\\boldsymbol{x}_n^\\intercal\\) in the \\(n\\)–th row, is full column rank\nR2: The regressors \\(\\boldsymbol{x}_n\\) are deterministic, and \\(\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\rightarrow \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\), where \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\) is positive definite\nR3: The regressors \\(\\boldsymbol{x}_n\\) are IID, with positive definite covariance \\(\\mathrm{Cov}\\left(\\boldsymbol{x}_n\\right)\\), and \\(\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\rightarrow \\mathbb{E}\\left[\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\right]\\) in probability.\n\nModel assumptions (for all \\(n\\)):\n\nM1: There exists a \\(\\boldsymbol{\\beta}\\) such that \\(y_n = \\boldsymbol{\\beta}^\\intercal\\boldsymbol{x}_n + \\varepsilon_n\\) for all \\(n\\)\nM2: The residuals \\(\\varepsilon_n\\) are IID with \\(\\varepsilon_n \\vert \\boldsymbol{x}_n \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\) for some \\(\\sigma^2\\)\nM3: The residuals \\(\\varepsilon_n\\) are independent with \\(\\mathbb{E}\\left[\\varepsilon_n | \\boldsymbol{x}_n\\right] = 0\\) and \\(\\mathbb{E}\\left[\\varepsilon_n^2 | \\boldsymbol{x}_n\\right] = \\sigma^2\\)\nM4: The residuals \\(\\varepsilon_n\\) are independent with \\(\\mathbb{E}\\left[\\varepsilon_n | \\boldsymbol{x}_n\\right] = 0\\) and \\(\\mathbb{E}\\left[\\varepsilon_n^2 | \\boldsymbol{x}_n\\right] = \\sigma_n^2\\)\nM5: The pairs \\((\\boldsymbol{x}_n, y_n)\\) are IID\nM6: For all finite vectors \\(\\boldsymbol{v}\\), \\(\\frac{1}{N} \\sum_{n=1}^N\\mathbb{E}\\left[y_n - \\boldsymbol{v}^\\intercal\\boldsymbol{x}_n)^2 \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\right] \\rightarrow \\boldsymbol{V}(\\boldsymbol{v}) &lt; \\infty\\), where each entry of the limiting matrix is finite. (The limit depends on \\(\\boldsymbol{v}\\), but importantly \\(\\boldsymbol{V}(\\boldsymbol{v})\\) is finite for all finite \\(\\boldsymbol{v}\\)).\n\nFor M2, M3, and M4 with \\(\\boldsymbol{x}_n\\) is deterministic, take the conditioning to mean “for that value of \\(\\boldsymbol{x}_n\\).”\nFor this homework, you may use the LLN, the CLT, the continuous mapping theorem, and properties of the multivariate normal distribution.\nThe term “limiting distribution” means the distribution that the quantity approaches as \\(N \\rightarrow \\infty\\).\nAssume R1 for all questions.\n\nFind the distribution of \\(\\hat{\\beta}\\) under M1, M2, and R2.\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M2, and R2.\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M2, and R3.\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M3, and R2.\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M3, and R3.\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M4, M6, and R2.\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M4, M6, and R3.\nUnder M5, M6, and R3, identify a \\(\\beta^*\\) such that \\(\\sqrt{N}(\\hat{\\beta}- \\beta^*)\\) converges to a nondegenerate, finite random variable, and find the limiting distribution.\nIn any of the above settings, what is the limiting distribution of \\((\\hat{\\beta}- \\beta)\\)? (The answer is the same no matter which setting you choose.)\n\n\n\n2 Investigating the assumptions\n\nWhat goes wrong if R1 is violated?\nWhat goes wrong if \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\) is not positive definite in R2?\nWhat goes wrong if \\(\\mathbb{E}\\left[\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\right]\\) is not positive definite in R3?\nIn terms of the limiting distributions, what is the practical difference between assumptions R2 and R3?\nAssume R1, R2, M1, and M3, except take \\(\\mathbb{E}\\left[\\varepsilon_n | \\boldsymbol{x}_n\\right] = \\delta \\ne 0\\). What happens to \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) as \\(N \\rightarrow \\infty\\)?\nAssume R1, R3, M5, and M6. In general, is it true that \\(\\mathbb{E}\\left[y_n - \\beta^* \\boldsymbol{x}_n\\right] = 0\\)?\n\n\n\n3 Confidence intervals for components of \\(\\hat{\\beta}\\)\nSuppose that \\(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}\\sim \\mathcal{N}\\left(\\boldsymbol{0}, \\boldsymbol{V}\\right)\\) for some \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{V}\\). Let \\(\\Phi(z) := \\mathbb{P}\\left(\\tilde{z} \\le z\\right)\\) where \\(\\tilde{z} \\sim \\mathcal{N}\\left(0,1\\right)\\).\nFix a vector \\(\\boldsymbol{a}\\) of the same length as \\(\\boldsymbol{\\beta}\\), and \\(0 \\le \\alpha \\le 1\\).\nIn terms of \\(\\Phi\\), \\(\\boldsymbol{V}\\), and \\(\\beta\\), find a scalar \\(b\\) such that\n\\[\n\\mathbb{P}\\left(-b  \\le \\boldsymbol{a}^\\intercal(\\hat{\\beta}- \\beta) \\le b\\right) = 1 - \\alpha.\n\\]\nIn particular, what is the result when \\(\\boldsymbol{a}= (0, \\ldots, 0, 1, 0, \\ldots, 0)\\) is a vector with \\(1\\) in location \\(k\\) and \\(0\\) elsewhere?"
  },
  {
    "objectID": "assignments/hw4_code.html",
    "href": "assignments/hw4_code.html",
    "title": "STAT151A Code homework 4: Due March 8th",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nThis coding assignment will use your work from homework 3 as a starting point. For the assignment, we’ll assume that\n\n\\(\\y_n = \\betav^\\trans \\xv_n + \\res_n\\) for all \\(n\\), including new datapoints\n\\(\\res_n \\sim \\gauss{0,\\sigma^2}\\)\nThe regressors \\(\\xv_n\\) are also random with covariance matrix \\(\\Xcov\\).\n\n\n1 Variability in the training set\nFix \\(N = 500\\), \\(P = 3\\), and set \\(\\betav\\) to some values you choose. Set \\(\\Xcov\\) to have correlation \\(0.9\\) off the diagonal and \\(1.0\\) on the diagonal. Set \\(\\sigma^2 = \\beta^\\trans \\Xcov \\beta\\).\nTake \\(\\xv_\\new\\) to be a single fixed draw from the distribution of regressors, and draw a large number (&gt; 5000) of \\(\\res_{\\new,i}\\), giving a large number of draws from \\(\\y_{\\new,i} | \\xv_\\new\\). The \\(\\y_{\\new,i}\\) should be normally distributed with mean \\(\\xv_\\new^\\trans \\beta\\) and variance \\(\\sigma^2\\).\n\n(a)\nDraw a single training set \\(\\X\\), \\(\\Y\\), and use it to construct an 80% interval for \\(\\y_\\new\\). Find the proportion of \\(\\y_{\\new,i}\\) that lie in the intevarl.\n\n\n(b)\nRepeat (a), but with 10 different training sets. You can keep the \\(\\y_{\\new,i}\\) the same. For each different training set, plot the corresponding intervals. Are they different from one another?\nBy a lot or a little?\n\n\n(c)\nRepeat (b), but with \\(N = 20\\). You can keep the \\(\\y_{\\new,i}\\) the same. How do the results compare to (b)? Why?\n\n\n(d)\nRepeat (b), but with \\(\\sigma\\) very small: specifically, set \\(\\sigma^2 = 0.01 \\beta^\\trans \\Xcov \\beta\\). You will need to draw new \\(\\y_{\\new,i}\\). How do the results compare to (b)?\n\n\n(e)\nRepeat (b), but now take \\(\\xv_\\new\\) to be the smallest eigenvector of \\(\\Xcov\\). (You can find the smallest eigenvector of \\(\\Xcov\\) using the R function eigen.)\nYou will need to draw new \\(\\y_{\\new,i}\\). How do the results compare to (b)?"
  },
  {
    "objectID": "assignments/hw3_code.html",
    "href": "assignments/hw3_code.html",
    "title": "STAT151A Code homework 3: Due February 23rd",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nIn this homework problem, we’ll simulate some data and check our predictions for a regression problem of the form \\(\\y_n = \\xv_n^\\trans \\betav + \\res_n\\), where \\(\\res_n \\sim \\gauss{0, \\sigma^2}\\), independetly of \\(\\xv_n\\).\nFor this problem, do not use lm() or other regression functions (except possibly to sanity check that you have done things correctly). You may (and are in fact encouraged) to use your solutions to past homeworks.\n\n1 Implementation\nImplement the following functions:\n# Generate a random covariance matrix.\n#\n# Args:\n#   dim: The dimension of the covariance matrix\n#\n# Returns:\n#   A valid dim x dim covariance matrix\nDrawCovMat &lt;- function(dim) {\n    # ... your code here ...\n}\n# Generate a random matrix of regressors.\n#\n# Args:\n#   n_obs: The number of regression observations\n#   cov_mat: A dim x dim valid covariance matrix\n#\n# Returns:\n#   A n_obs x dim matrix of normally distributed random regressors\n#   where the rows have covariance cov_mat\nSimulateRegressors &lt;- function(n_obs, cov_mat) {\n    # ... your code here ...\n}\n# Generate the response for a linear model.\n#\n# Args:\n#   x_mat: A n_obs x dim matrix of regressors\n#   beta: A dim-length vector of true regression coefficients\n#   sigma: The standard deviation of the residuals\n#\n# Returns:\n#   A n_obs-vector of responses drawn from the regression\n#   model y_n ~ x_n^T \\beta + \\epsilon_n, where \\epsilon_n\n#   is distributed N(0, sigma^2),\nSimulateResponses &lt;- function(x_mat, beta, sigma) {\n    # ... your code here ...\n}\n# Estimate the regression coefficient.\n#\n# Args:\n#   x: A n_obs x dim matrix of regressors\n#   y: A n_obs-length vector of responses\n#\n# Returns:\n#   A dim-length vector estimating the coefficient \n#   for the least squares regression y ~ x.\nGetBetahat &lt;- function(x, y) {\n    # ... your code here ...\n}\n# Estimate the residual standard deviation.\n#\n# Args:\n#   x: A n_obs x dim matrix of regressors\n#   y: A n_obs-length vector of responses\n#\n# Returns:\n#   An estimate of the residual standard deviation \n#   for the least squares regression y ~ x.\nGetSigmahat &lt;- function(x, y) {\n    # ... your code here ...\n}\n\n\n2 Draw and check\nChoose a dimension. Using a large \\(N\\), check that your functions are working correctly.\n\n\n3 Draw a training set and test set\nSimulate a training set \\(\\X\\) and \\(\\Y\\) with \\(N = 1000\\) and \\(P = 3\\), and values of \\(\\beta\\) and \\(\\sigma\\) that you choose. Use the training set to form estimates \\(\\betavhat\\) and \\(\\sigmahat\\). Then, draw a new set of \\(500\\) test data points, \\(\\X_\\new\\) and \\(\\Y_\\new\\).\nUse \\(\\betavhat\\), \\(\\sigmahat\\), and \\(\\X_\\new\\) to form an approximate 80% predictive interval for each response \\(\\Y_\\new\\). Compute what proportion of the time the \\(\\Y_\\new\\) actually lie in the interval. Is your prediction interval performing as expected? Why or why not?\n\n\n4 Vary the setting\nChoose three of the following questions to answer.\nExplore how the coverage of the test set varies when:\n\n\\(N\\) increases or decreases, all else fixed\nThe value of \\(\\sigma\\) increase and decreases, all else fixed\nThe values in \\(\\beta\\) increase and decreases, all else fixed\n\\(P\\) increase and decreases (and \\(\\beta\\) changes with it)\nThe distribution of the residuals changes (i.e., try something other than a normal)\nYou draw new values for the training set (but keep the test set fixed)\nYou draw new values for the test set (but keep the training set fixed)\nPick something else that you find interesting to vary!"
  },
  {
    "objectID": "assignments/hw2_code.html",
    "href": "assignments/hw2_code.html",
    "title": "STAT151A Code homework 2: Due February 9th",
    "section": "",
    "text": "In this homework, you’ll implement some linear algebra ideas in R.\nFor each problem, provide a solution in the form of a function, and then test it using following provided functions:\n# Check whether matrix `x` == `y` for all entries,\n# up to the tolerance `tol`.  If not, an error is raised.\nAssertMatricesEqual &lt;- function(x, y, tol=1e-9) {\n  if (!(all(dim(x) == dim(y)))) {\n    stop(\"The dimensions do not match.\")\n  }\n  err &lt;- max(abs(x - y))\n  if (err &gt; tol) {\n    stop(sprintf(\"The error %f is greater than the tolerance.\", err))\n  }\n}\n\n\n# Generate an `nrow` x `ncol` matrix with random standard normal entries.\nGenerateMatrix &lt;- function(nrow, ncol) {\n  return(rnorm(nrow * ncol) %&gt;% matrix(nrow, ncol))\n}\nFor example,\n# Should fail --- the matrices have the wrong dimension.\nAssertMatricesEqual(GenerateMatrix(3, 3), GenerateMatrix(4, 3))\n\n# Should fail --- the matrices are not equal.\nA &lt;- GenerateMatrix(3, 3)\nAssertMatricesEqual(GenerateMatrix(3, 3), A + 5)\n\n# Should pass\nAssertMatricesEqual(A, A)\n\n(Example) Starting from GenerateMatrix(), write a function that generates a random symmetric matrix.\n\nSolution:\nGenerateSymmetricMatrix &lt;- function(dim) {\n    a_mat &lt;- GenerateMatrix(dim, dim)\n    return(0.5 * (a_mat + t(a_mat)))\n}\n\na_sym &lt;- GenerateSymmetricMatrix(4)\nAssertMatricesEqual(a_sim, t(a_sim))\n\n(Example) Starting from AssertMatricesEqual(), write a function that checks whether a matrix is symmetric.\n\nSolution:\nCheckSymmetricMatrix &lt;- function(a) {\n    AssertMatricesEqual(a, t(a))\n}\n\nStarting from GenerateMatrix(), write a function to generate a random symmetric positive semi-definite matrix of a given size.\nStarting from GenerateMatrix(), write a function to generate a random symmetric positive definite matrix of a given size whose smallest eigenvalue is greater than one. (Hint: you can add something to your previous solution.)\nWrite a function that takes in a symmetric PSD matrix and returns an inverse computed from the eigendecomposition. You may use eigen().\nWrite a function that takes in a symmetric PSD matrix and returns a square root computed from the eigen-decomposition. You may use eigen().\nWrite a function that takes in a symmetric PSD matrix and returns a (possibly non-symmetric) square root computed from the cholesky decomposition. You may use chol().\nWrite a function that takes in a potentially non-square matrix and returns a projection matrix onto the column span. You may assume the matrix is full-rank.\nWrite a function that takes in a potentially non-square matrix and returns a projection matrix onto the row span. You may assume the matrix is full-rank.\nWrite a function that takes in a potentially non-square matrix and returns a projection matrix onto the orthogonal complement of the column span. You may assume the matrix is full-rank.\nWrite a function that takes in a potentially non-square matrix and returns an orthonormal basis for its column span. You may assume the matrix is full-rank.\nWrite a function that takes in a positive definite covariance matrix and returns a draw from the multivariate normal distribution with mean zero and the given covariance matrix. You may use only rnorm(n, mean=0, sd=1) to generate random variables. Verify that you have succeeded using Monte Carlo draws and AssertMatricesEqual() with an appropriate tolerance."
  },
  {
    "objectID": "assignments/hw1_code.html",
    "href": "assignments/hw1_code.html",
    "title": "STAT151A Code homework 1: Due January 26th",
    "section": "",
    "text": "For all questions below, provide answers in complete sentences, and include correct and readable code to support your answers.\n\n1 Spotify dataset\n\n(a)\nFind another variable (other than danceability) that is associated with popularity according to simple linear regression.\n\n\n(b)\nHow does this association change if you remove low-popularity tracks? You may define low-popularity tracks however you like, but briefly defend your choice.\n\n\n(c)\nIdentify a song that defies the relationship you found. (For example, having found a positive relationship between danceability and popularity, I might find a song that is highly popular but not ``danceable.’’)\nListen to the song on Spotify and comment on whether the result makes sense.\n\n\n\n2 Bodyfat dataset\n\n(a)\nChoose two variables (other than bodyfat). Use lm to regress bodyfat on these two variables and an intercept.\n\n\n(b)\nFor the regression in the previous example, construct your own \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Y}\\) matrices by hand (don’t use the output of lm). Using these, compute your own estimate \\(\\hat{\\beta}\\) and confirm that it matches the output of lm.\n\n\n(c)\nWrite a function in R that computes \\(\\hat{\\beta}\\) from \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Y}\\). Document the function’s inputs and outputs. As an example, you might follow the Function Documentation section of the Amazon R style guide.\n\n\n\n3 Aluminum dataset\n\n(a)\nRun the regression from Lecture 1 using all three specimens, both with and without an intercept term. Plot the results.\nFor convenience, here is a filter function to limit to the right set of data:\nfilter(Strain &lt; 0.0035, Strain &gt; 0.0001,\n       loading_type == \"T\", temp == 20, lot == \"A\")\nComment on whether an intercept should be included and why. When the intercept is estimated, how can it be interpreted?"
  },
  {
    "objectID": "assignments/assignments.html",
    "href": "assignments/assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Due Jan 26th:\n\nHomework 1 (math component)\nHomework 1 (code component)\n\nDue Feb 9th:\n\nHomework 2 (math component)\nHomework 2 (code component)\n\nDue Feb 23rd:\n\nHomework 3 (math component)\nHomework 3 (code component)\n\nDue Mar 8th:\n\nHomework 4 (math component)\nHomework 4 (code component)\n\nDue Mar 22nd:\n\nHomework 5 (math component)\nNo code component this week\nNo Quiz on April 2nd\n\nDue April 19th:\n\nHomework 6 forthcoming\nQuiz on April 23rd covering HW5 and HW6\n\n\n\nFinal project",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/final_project.html",
    "href": "assignments/final_project.html",
    "title": "STAT151A Final Project",
    "section": "",
    "text": "\\(\\,\\)\nBelow are the details of your final project. As usual, these may be subject to change with notice."
  },
  {
    "objectID": "assignments/final_project.html#alternative-projects",
    "href": "assignments/final_project.html#alternative-projects",
    "title": "STAT151A Final Project",
    "section": "Alternative projects",
    "text": "Alternative projects\nThoughtful deviations from this template are welcome. Some possible examples are:\n\nStudying an advanced regression topic (e.g. double descent, regression trees, Bayesian methods) using real or simulated data\nReproducing an existing study, then changing some of the methods to investigate the stability of the results\nStudy the numerical stability of R’s lm with a detailed analysis of its core linear algebra routines\n\nAlternative proposals should still meaningfully engage with the content of the course. For example, a project that simply fits deep neural networks to some data is inappropriate, but a project that meaningfully compares a deep neural network to regression techniques on the same dataset could be appropriate.\nIf you want to deviate from the template, please describe your proposal in detail on the project form."
  },
  {
    "objectID": "assignments/final_project.html#more-detailed-guidelines",
    "href": "assignments/final_project.html#more-detailed-guidelines",
    "title": "STAT151A Final Project",
    "section": "More detailed guidelines",
    "text": "More detailed guidelines\nHere are some more guidelines for the project proposals, as well as what we will look for in a good project.\n\nDatasets\nDatasets should be openly available. Furthermore, they should come with detailed and clear descriptions of how the data were collected. Ideally, the information about data collection is in the form of a published paper or study.\n\nGood example: The bodyfat dataset\nBad example: The marketing dataset in An Introduction to Statistical Learning (no real information about how it was collected)\nBad example: Proprietary data from your aunt’s internet startup (not open access)\n\nI hope that good datasets may become part of future versions of this course.\nHere are some potential places to look for datasets:\n\nKaggle\nUCI ML repository\nOpenintro\nFox regression book\nIEEE Dataport (unfortunately, many cost money)\nROS textbook\n\nPlease feel free to share other suggestions with me and with the rest of the class!\n\n\nQuestions\nQuestions should be about the real world, not framed directly in terms of statistical analyses.\n\nGood example: Can we increase household income by giving cash to poor families?\nGood example: Can we produce a useful predictor of bodyfat from simpler measurements?\nBad example: Is there an association between \\(x_n\\) and \\(y_n\\) in this dataset?\nBad example: Is coefficient \\(\\beta_1\\) in such-and-such a regression statistically significant?\n\nPlease be clear about whether your problem is an inference or a prediction problem (or both, or neither).\n\n\nAttempted answers\nIn order to attempt to answer your question with linear regression, you have to connect your real-world question with a regression analysis. Please be very clear about\n\nWhat assumptions you need to connect your regression to your question\nWhether those assumptions are reasonable\n\nClear thinking will be more important here than definitive answers to your question.\n\n\nCritical analysis\nFinally, please connect the results of your analysis to your question. Here are some of the kinds of questions you might ask:\n\nWhat is the answer to your question?\nWere you not able to answer your question due to some limitation you found in the data?\nHow might you collect different data to successfully answer your question?\nWhat different statitsical analyses might answer the question better?\nWhat evidence is there that your assumptions are satisfied?\nWhat evidence could you imagine collecting to establish that your assumptions were satisfied?\n\nAgain, clear thinking will be more important here than definitive answers to your question."
  },
  {
    "objectID": "assignments/hw1_math.html",
    "href": "assignments/hw1_math.html",
    "title": "STAT151A Homework 1: Due Jan 26th",
    "section": "",
    "text": "1 Simple regression in matrix form\nConsider the simple linear model \\(y_n = \\beta_0 + \\beta_1 z_n + \\varepsilon_n\\).\nLet \\(\\bar{y}:= \\frac{1}{N} \\sum_{n=1}^Ny_n\\) and \\(\\bar{z}:= \\frac{1}{N} \\sum_{n=1}^Nz_n\\). Recall that the ordinary least squares estimates are given by \\[\n\\begin{aligned}\n    \\hat{\\beta}_1 = \\frac{\\frac{1}{N} \\sum_{n=1}^N(z_n - \\bar{z}) (y- \\bar{y})}{\\frac{1}{N} \\sum_{n=1}^N(z_n - \\bar{z})^2}\n    \\quad\\textrm{and}\\quad\n    \\hat{\\beta}_0 = \\bar{y}- \\hat{\\beta}_1 \\bar{z}.\n\\end{aligned}\n\\]\n\n(a)\nWrite the set of equations\n\\[\ny_n = \\beta_0 + \\beta_1 z_n + \\varepsilon_n\n\\]\nfor \\(n \\in \\{1, \\ldots, N\\}\\) in matrix form. That is, let \\(\\boldsymbol{X}\\) denote an \\(N \\times\n2\\) matrix, \\(\\boldsymbol{Y}\\) and \\(\\boldsymbol{\\varepsilon}\\) denote \\(N \\times 1\\) matrices, \\(\\boldsymbol{b}= (\\beta_0, \\beta_1)^\\intercal\\), and express the matrices \\(\\boldsymbol{Y}\\), \\(\\boldsymbol{X}\\), and \\(\\boldsymbol{\\varepsilon}\\) in terms of the scalars \\(y_n\\), \\(z_m\\), and \\(\\varepsilon_n\\) so that \\(\\boldsymbol{Y}= \\boldsymbol{X}\\boldsymbol{b}+ \\boldsymbol{\\varepsilon}\\) is equivalent to the set of regression equations.\n\n\n(b)\nDefine \\[\n\\begin{aligned}\n    \\overline{zz} := \\frac{1}{N} \\sum_{n=1}^Nz_n^2\n    \\quad\\textrm{and}\\quad\n    \\overline{zy} := \\frac{1}{N} \\sum_{n=1}^Nz_n y_n\n\\end{aligned}\n\\]\nWrite an explict expressions for \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\), \\(\\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\), and \\(\\left(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1}\\), all in terms of \\(\\bar{y}\\), \\(\\bar{z}\\), \\(\\overline{zz}\\), \\(\\overline{zy}\\), and \\(N\\). Verify that the inverse is correct by direct multiplication.\n\n\n(c)\nCompute \\((\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\). Show that the first row is equal to \\(\\hat{\\beta}_0\\) and the second row is equal to \\(\\hat{\\beta}_1\\) as given by the ordinary least squares formula in the problem statement above.\n\n\n\n2 Mean zero residuals.\nConsider the model \\(y_n = \\beta z_n + \\varepsilon_n\\). Let \\(\\hat{\\beta}\\) denote the least squares estimator and \\(\\hat{\\varepsilon}_n = y_n - \\hat{\\beta}z_n\\).\n\n(a)\nSuppose \\(z_n\\) is not a constant. Is it necessarily the case that \\(\\frac{1}{N} \\sum_{n=1}^N\\hat{\\varepsilon}_n = 0\\)? Prove your answer.\n\n\n(b)\nSuppose \\(z_n\\) is a constant, but \\(z_n \\equiv 5\\) for every \\(n \\in \\{1, \\ldots, N\\}\\). Is it necessarily the case that \\(\\frac{1}{N} \\sum_{n=1}^N\\hat{\\varepsilon}_n = 0\\)? Prove your answer.\n\n\n(c)\nNow the model \\(y_n = \\beta_1 z_{n1} + \\beta_2 z_{n2} + \\varepsilon_n\\). Suppose that \\(z_{n1} = 1\\) is \\(n\\) is even, and is \\(0\\) otherwise. Similarly, suppose that \\(z_{n2} = 1\\) is \\(n\\) is odd, and is \\(0\\) otherwise. Let \\(N\\) be even. Is it necessarily the case that \\(\\frac{1}{N} \\sum_{n=1}^N\\hat{\\varepsilon}_n = 0\\)? Prove your answer.\n\n\n\n3 Inner products and covariances\nLet \\(\\boldsymbol{z}= (z_1, \\ldots, z_N)\\) and \\(\\boldsymbol{y}= (y_1, \\ldots, y_N)\\). Let \\(\\boldsymbol{X}\\) denote an \\(N \\times P\\) matrix whose \\(n\\)–th row is the transpose of the \\(P\\)-vector \\(\\boldsymbol{x}_n^\\intercal\\).\n(Note: this question involves limits of random variables, and there are many distinct ways that random variables can converge to limits. If you’re familiar with these different modes of probabilisitic convergence, feel free to state what mode of convergence applies, but if you are not, don’t worry — modes of convergence will not matter much for this class, and you can state your result heuristically.)\nFor a set of quantities (numbers, vectors, pairs of vectors, etc), the “empirical distribution” over that set refers to drawing an element with replacement from the set with equal probability given to each entry. For example, if \\(\\mathcal{Z}'\\) is a drawn from the empirical distribution over the set \\(\\{z_1, \\ldots, z_N \\}\\), then \\(\\mathbb{P}\\left(\\mathcal{Z}' = z_n\\right) = 1/N\\) for each \\(n\\). Similarly, if \\((\\mathcal{Z}', \\mathcal{Y}')\\) is drawn from the empirical distribution over the pairs \\(\\{(z_1, y_1), \\ldots, (z_N, y_N)\\}\\), then \\(\\mathbb{P}\\left((\\mathcal{Z}', \\mathcal{Y}') = (z_n, y_n)\\right) = 1/N\\) for all \\(n\\).\n(Hint: it may help to recall that the bootstrap uses draws from the empirical distribution, and that, in the empirical distribution, the elements of the set are fixed and not random.)\n\n(a)\nLet \\((\\mathcal{Z}', \\mathcal{Y}')\\) denote a draw from the empirical distribution over the set \\(\\{(y_1, z_1), \\ldots, (y_N, z_N)\\}\\).\nProve that \\(\\frac{1}{N} \\boldsymbol{z}^\\intercal\\boldsymbol{y}= \\mathbb{E}\\left[\\mathcal{Z}' \\mathcal{Y}'\\right]\\). Then prove that \\(\\frac{1}{N} \\boldsymbol{1}^\\intercal\\boldsymbol{z}= \\mathbb{E}\\left[\\mathcal{Z}'\\right]\\) as a special case.\n\n\n(b)\nNow suppose that the entries of \\(\\boldsymbol{z}\\) are independent and identically distributed (IID) realizations of the random variable \\(\\mathcal{Z}\\), and that the entries of \\(\\boldsymbol{y}\\) are similarly IID realizations of a random variable \\(\\mathcal{Y}\\). Assuming that \\(\\mathbb{E}\\left[|\\mathcal{Z}|\\right] &lt; \\infty\\) and \\(\\mathbb{E}\\left[|\\mathcal{Y}|\\right] &lt; \\infty\\), prove that\n\\[\n\\frac{1}{N} \\boldsymbol{z}^\\intercal\\boldsymbol{y}\\rightarrow\n    \\mathbb{E}\\left[\\mathcal{Z} \\mathcal{Y}\\right]\n    \\textrm{ as }N \\rightarrow \\infty\n\\]\n(Hint: don’t prove this from scratch, appeal to a probability theorem.)\n\n\n(c)\nUsing only inner products involving \\(\\boldsymbol{y}\\), \\(\\boldsymbol{z}\\), and \\(\\boldsymbol{1}\\), write an expression for \\(\\mathrm{Cov}\\left(\\mathcal{Y}', \\mathcal{Z}'\\right)\\). Prove that the expression converges with probability one to \\(\\mathrm{Cov}\\left(\\mathcal{Y}, \\mathcal{Z}\\right)\\). (Hint: again, use your previous results and a theorem from probability.)\n\n\n(d)\nNow, let \\((\\mathcal{X}', \\mathcal{Y}')\\) denote a draw from the empirical distribution over \\(\\{(x_1, y_1), \\ldots, (x_N, y_N) \\}\\). (Recall that the vector \\(x_n\\) is a length–\\(P\\) column vector, and \\(x_n^\\intercal\\) is the \\(n\\)–th row of the matrix \\(\\boldsymbol{X}\\).)\n\\[\n\\begin{aligned}\n\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}= \\mathbb{E}\\left[\\mathcal{X}' \\mathcal{X}'^\\intercal\\right]\n\\quad\\textrm{and}\\quad\n\\frac{1}{N} \\boldsymbol{X}^\\intercal y= \\mathbb{E}\\left[\\mathcal{X}' \\mathcal{Y}'\\right].\n\\end{aligned}\n\\]\n\n\n(e)\nNow, suppose that rows of \\(\\boldsymbol{X}\\) are IID realizations of the random \\(P\\)–vector \\(\\mathcal{X}\\), and that \\(\\mathbb{E}\\left[|\\mathcal{X}_p|\\right] &lt; \\infty\\) for each \\(p \\in \\{ 1, \\ldots, P \\}\\). Assume, as above, that \\(\\mathbb{E}\\left[|\\mathcal{Y}|\\right] &lt; \\infty\\).\nProve that, as \\(N \\rightarrow \\infty\\),\n\\[\n\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\rightarrow\n    \\mathbb{E}\\left[\\mathcal{X} \\mathcal{X}^\\intercal\\right]\n\\quad\\textrm{and}\\quad\n\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\rightarrow\n    \\mathbb{E}\\left[\\mathcal{X} \\mathcal{Y}\\right],\n\\]\nwhere both limits are with probability one.\n\n\n(f)\nNow assume that, for each \\(p \\in \\{1, \\ldots, P\\}\\) and \\(q \\in \\{1, \\ldots, P\\}\\), \\(\\mathbb{E}\\left[\\left|\\mathcal{X}'_p\\right| \\left|\\mathcal{X}'_q\\right| \\mathcal{Y}^2\\right] &lt; \\infty\\). Prove that, as \\(N \\rightarrow \\infty\\),\n\\[\n\\frac{1}{\\sqrt{N}}\n\\left( \\boldsymbol{X}^\\intercal\\boldsymbol{Y}- \\mathbb{E}\\left[\\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\right] \\right) \\rightarrow \\mathcal{Z},\n\\]\nwhere \\(\\mathcal{Z}\\) is a multivariate normal random variable. What is the covariance of \\(\\mathcal{Z}\\)? (Hint: again, appeal to a probability theorem.)"
  },
  {
    "objectID": "assignments/hw2_math.html",
    "href": "assignments/hw2_math.html",
    "title": "STAT151A Homework 2: Due February 9th",
    "section": "",
    "text": "1 Transformation of variables\nConsider two different regressions, \\(\\boldsymbol{Y}\\sim \\boldsymbol{X}\\boldsymbol{b}\\) and \\(\\boldsymbol{Y}\\sim \\boldsymbol{Z}\\boldsymbol{\\alpha}\\), with the same \\(\\boldsymbol{Y}\\), where \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\) are both \\(N \\times P\\) and are both full-rank. Let the \\(n\\)–th row of \\(\\boldsymbol{X}\\) be written \\(\\boldsymbol{x}_n^\\intercal\\), and the \\(n\\)–th row of \\(\\boldsymbol{Z}\\) be \\(\\boldsymbol{z}_n^\\intercal\\).\n\n(a)\nSuppose \\(\\boldsymbol{x}_n = \\boldsymbol{A}\\boldsymbol{z}_n\\) for an invertible \\(\\boldsymbol{A}\\) and for all \\(n = 1,\\ldots,N\\). Find an expression for \\(\\hat{\\alpha}\\) in terms of \\(\\hat{\\beta}\\) that does not explicitly use \\(\\boldsymbol{Y}\\), \\(\\boldsymbol{X}\\), or \\(\\boldsymbol{Z}\\).\n\n\n(b)\nSuppose that, for all \\(n=1,\\ldots,N\\), \\(\\boldsymbol{x}_n = f(\\boldsymbol{z}_n)\\) for some invertible but non-linear function \\(f(\\cdot)\\). In general, can you find an expression for \\(\\hat{\\alpha}\\) in terms of \\(\\hat{\\beta}\\) that does not explicitly use \\(\\boldsymbol{Y}\\), \\(\\boldsymbol{X}\\), or \\(\\boldsymbol{Z}\\)? Prove why or why not. (To prove that you cannot, finding a single counterexample is enough.)\n\n\n(c)\nNow consider only the regression \\(\\boldsymbol{Y}\\sim \\boldsymbol{X}\\boldsymbol{b}\\), but suppose we are not interested in \\(\\boldsymbol{b}\\), but rather some other \\(\\boldsymbol{\\gamma}= \\phi(\\boldsymbol{b})\\), where \\(\\phi\\) is an invertible function. Prove that the least squares estimator of \\(\\boldsymbol{\\gamma}\\) is given by \\(\\hat{\\boldsymbol{\\gamma}}= \\phi(\\hat{\\boldsymbol{b}})\\).\n\n\n(d)\nProve that result (a) is special case of the result (c). (Hint: find the corresponding \\(\\phi\\).)\n\n\n\n2 Spaces of possible estimators.\nConsider the simple linear model \\(y_n = \\beta_0 + \\beta_1 z_n + \\varepsilon_n\\). Assume that \\(\\frac{1}{N} \\sum_{n=1}^Nz_n \\ne 0\\).\n\n(a)\nFix \\(\\beta_0 = \\frac{1}{N} \\sum_{n=1}^Ny_n\\) and find a value of \\(\\beta_1\\) such that \\(\\frac{1}{N} \\sum_{n=1}^N\\varepsilon_n = 0\\). How does your answer depend on whether or not \\(\\frac{1}{N} \\sum_{n=1}^Nz_n = 0\\)?\n\n\n(b)\nFix \\(\\beta_1 = 10,000,000\\) and find a value of \\(\\beta_0\\) such that \\(\\frac{1}{N} \\sum_{n=1}^N\\varepsilon_n = 0\\).\n\n\n(c)\nIn general, how many different choices of \\(\\beta_0\\) and \\(\\beta_1\\) can you find that satisfy \\(\\frac{1}{N} \\sum_{n=1}^N\\varepsilon_n = 0\\)? Are all of them reasonable? Are any of them reasonable?\n\n\n(d)\nFind an \\(N\\)–dimensional vector \\(\\boldsymbol{v}\\) such that \\[\n\\frac{1}{N} \\sum_{n=1}^N\\varepsilon_n = 0 \\quad\\Leftrightarrow\\quad \\boldsymbol{v}^\\intercal\\boldsymbol{\\varepsilon}= \\boldsymbol{0}.\n\\]\n\n\n(e)\nSuppose I give you a general \\(N\\)–dimensional vector \\(\\boldsymbol{v}\\) and a scalar \\(a\\). How many different choices of \\(\\beta_0\\) and \\(\\beta_1\\) can you find such that \\(\\boldsymbol{v}^\\intercal\\boldsymbol{\\varepsilon}= a\\)?\n\n\n(f) (Optional — this will not be graded)\nSuppose I give you two different vectors, \\(\\boldsymbol{v}_1\\) and \\(\\boldsymbol{v}_2\\). Under what circumstances can you find \\(\\beta_0\\) and \\(\\beta_1\\) such that\n\\[\n\\begin{aligned}\n\\boldsymbol{v}_1^\\intercal\\boldsymbol{\\varepsilon}= \\boldsymbol{0}\n\\quad\\textrm{and}\\quad\n\\boldsymbol{v}_2^\\intercal\\boldsymbol{\\varepsilon}= \\boldsymbol{0}?\n\\end{aligned}\n\\]\nWhen are there infinitely many solutions? When is there only one solution? (Hint: what if \\(\\boldsymbol{v}_1^\\intercal\\boldsymbol{1}= \\boldsymbol{v}_2^\\intercal\\boldsymbol{1}= 0\\)?)\n\n\n(g)\nNow, consider the general linear model \\(\\boldsymbol{Y}= \\boldsymbol{X}\\boldsymbol{b}+ \\varepsilon\\). Prove that there always exists \\(\\boldsymbol{b}\\) and \\(\\varepsilon\\) so that the \\(\\boldsymbol{Y}= \\boldsymbol{X}\\boldsymbol{b}+ \\varepsilon\\).\n\n\n(h) (Optional — this will not be graded)\nSuppose, for the general linear model, that the matrix \\(\\boldsymbol{X}\\) is full-rank (that is, of rank \\(P\\), where \\(P\\) is the number of columns of \\(\\boldsymbol{X}\\)). Suppose I give you a \\(N \\times D\\) matrix \\(\\boldsymbol{V}\\), and ask you to find \\(\\boldsymbol{b}\\) such that \\(\\boldsymbol{V}^\\intercal\\boldsymbol{\\varepsilon}= \\boldsymbol{0}\\). Under what circumstances are there no solutions? A single solution? An infinite set of solutions? (Hint: you already answered this question for \\(P = 2\\), now you just need to state the result in matrix form.)\n\n\n\n3 Collinear regressors\nSuppose that \\(\\boldsymbol{X}\\) does not have full column rank — that is, \\(\\boldsymbol{X}\\) is \\(N \\times P\\) but has column rank \\(Q &lt; P\\).\n\n(a)\nHow many solutions \\(\\hat{\\beta}\\) are there to the least-squares problem \\[\n\\hat{\\beta}:= \\underset{\\beta}{\\mathrm{argmin}}\\, \\left\\Vert\\boldsymbol{Y}- \\boldsymbol{X}\\beta\\right\\Vert_2^2?\n\\]\n\n\n(b)\nRelate the solutions \\(\\hat{\\beta}\\) from part (a) to spaces spanned by eigenvectors of \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\). Among the solutions, identify the one with the smallest norm, \\(\\left\\Vert\\hat{\\beta}\\right\\Vert_2^2\\).\n\n\n(c)\nSuppose that \\(\\boldsymbol{X}'\\) is a full column-rank \\(N \\times Q\\) matrix with the same column span as \\(\\boldsymbol{X}\\), and let \\(\\hat{\\gamma}\\) be the OLS estimator for the regression \\(\\boldsymbol{Y}\\sim \\boldsymbol{X}'\\gamma\\). Compare the fits \\(\\hat{\\boldsymbol{Y}}= \\boldsymbol{X}\\hat{\\beta}\\) and \\(\\hat{\\boldsymbol{Y}}' = \\boldsymbol{X}' \\hat{\\gamma}\\), and compare the sum of squared residuals for the two regressions."
  },
  {
    "objectID": "assignments/hw3_math.html",
    "href": "assignments/hw3_math.html",
    "title": "STAT151A Homework 3: Due February 23rd",
    "section": "",
    "text": "\\(\\,\\)\n\n1 Normal intervals\nFor these problems, assume I give you a computer program that can compute the function \\(\\Phi(z) = \\mathbb{P}\\left(\\tilde{z} \\le z\\right)\\) where \\(\\tilde{z}\\) is a standard scalar-valued random variable.\nLet \\(\\tilde{x}\\) denote a scalar-valued \\(N(\\mu, \\sigma^2)\\) random variable. Using only \\(\\Phi(z)\\) and elementary arithmetic, construct functions that evaluate the following:\n\n(a)\n\\(a \\mapsto \\mathbb{P}\\left(\\tilde{x} \\le a\\right)\\)\n\n\n(b)\n\\(b \\mapsto \\mathbb{P}\\left(\\tilde{x} \\ge b\\right)\\)\n\n\n(c)\n\\(a, b \\mapsto \\mathbb{P}\\left(b \\le \\tilde{x} \\le a\\right)\\)\n\n\n(d)\n\\(a \\mapsto \\mathbb{P}\\left(\\left|\\tilde{x}\\right| \\le a\\right)\\)\n\n\n(e)\n\\(a \\mapsto \\mathbb{P}\\left(\\left|\\tilde{x}\\right| \\ge a\\right)\\)\n\n\n(f)\n\\(a \\mapsto \\mathbb{P}\\left(\\left|\\tilde{x}\\right| &gt; a\\right)\\)\n\n\n(g)\n\\(a \\mapsto \\mathbb{P}\\left(\\left|\\tilde{x}\\right| = a\\right)\\)\n\n\n\n2 Multivariate CLT\nLet \\(\\tilde{\\boldsymbol{x}}_n\\) denote an IID sequence of random variables in \\(\\mathbb{R}^{P}\\) (not necessarily normal), each with zero mean and finite covariance matrix \\(\\boldsymbol{\\Sigma}\\). Let \\(\\boldsymbol{a}\\in \\mathbb{R}^{P}\\) denote a fixed vector.\n\n(a)\nUsing the univariate CLT, find the limiting distribution of\n\\[\n\\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\boldsymbol{a}^\\intercal\\tilde{\\boldsymbol{x}_n}.\n\\]\n\n\n(b)\nUsing the multivariate CLT and the continuous mapping theorem, find the limiting distribution of\n\\[\n\\boldsymbol{a}^\\intercal\\left( \\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\tilde{\\boldsymbol{x}_n}\\right).\n\\]\n\n\n(c)\nNow, suppose that \\(P = 2\\) and\n\\[\\boldsymbol{\\Sigma}=\n\\begin{pmatrix}\n1 & -1 \\\\\n-1 & 1\n\\end{pmatrix}.\n\\]\nNote that we can write \\[\n\\tilde{\\boldsymbol{x}}_n =\n\\begin{pmatrix}\n\\tilde{\\boldsymbol{x}}_{n1} \\\\\n\\tilde{\\boldsymbol{x}}_{n2}\n\\end{pmatrix},\n\\] where \\(\\tilde{\\boldsymbol{x}}_{n1}\\) and \\(\\tilde{\\boldsymbol{x}}_{n2}\\) are scalars. Find the limiting distributions of each of the following expressions:\n\\[\n\\begin{aligned}\n\\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\tilde{\\boldsymbol{x}}_{n1} \\rightarrow& \\textrm{?}\\\\\n\\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\tilde{\\boldsymbol{x}}_{n2} \\rightarrow& \\textrm{?} \\\\\n\\frac{1}{\\sqrt{N}} \\sum_{n=1}^N(\\tilde{\\boldsymbol{x}}_{n1} + \\tilde{\\boldsymbol{x}}_{n2}) \\rightarrow& \\textrm{?}\n\\end{aligned}\n\\]\n(This result demonstrates why it’s not enough to only look at the marginal distribution of the vector components when using a multivariate CLT.)\n\n\n\n3 Valid covariance matrices\nSuppose I were to tell you that the vector-valued random variable \\(\\boldsymbol{x}\\) has a covariance matrix \\(\\mathrm{Cov}\\left(\\boldsymbol{x}\\right) = \\boldsymbol{\\Sigma}\\) where \\(\\boldsymbol{\\Sigma}\\) is not positive semi-definite (i.e., \\(\\boldsymbol{\\Sigma}\\) has at least one negative eigenvalue). Show that, if this were true, you could construct a scalar-valued random variable with negative variance, which is impossible.\n(It follows from this argument every covariance matrix must be postive semi-definite.)"
  },
  {
    "objectID": "assignments/hw4_math.html",
    "href": "assignments/hw4_math.html",
    "title": "STAT151A Homework 4: Due March 8th",
    "section": "",
    "text": "\\(\\,\\)\n\n1 Chi squared random variables\nLet \\(s\\sim \\mathcal{\\chi}^2_{K}\\). Prove that\n\n\\(\\mathbb{E}\\left[s\\right] = K\\)\n\\(\\mathrm{Var}\\left(s\\right) = 2 K\\) (hint: if \\(z\\sim \\mathcal{N}\\left(0,\\sigma^2\\right)\\), then \\(\\mathbb{E}\\left[z^4\\right] = 3\\sigma^4\\))\nIf \\(a_n \\sim \\mathcal{N}\\left(0,\\sigma^2\\right)\\) IID for \\(1,\\ldots,N\\), then \\(\\frac{1}{\\sigma^2} \\sum_{n=1}^Na_n^2 \\sim \\mathcal{\\chi}^2_{N}\\)\n\\(\\frac{1}{K} s\\rightarrow 1\\) as \\(K \\rightarrow \\infty\\)\n\\(\\frac{1}{\\sqrt{K}} (s- K) \\rightarrow \\mathcal{N}\\left(0, 2\\right)\\) as \\(K \\rightarrow \\infty\\)\nLet \\(\\boldsymbol{a}\\sim \\mathcal{N}\\left(\\boldsymbol{0}, \\boldsymbol{I}\\right)\\) where \\(a\\in \\mathbb{R}^{K}\\). Then \\(\\left\\Vert\\boldsymbol{a}\\right\\Vert_2^2 \\sim \\mathcal{\\chi}^2_{K}\\)\nLet \\(\\boldsymbol{a}\\sim \\mathcal{N}\\left(\\boldsymbol{0}, \\boldsymbol{\\Sigma}\\right)\\) where \\(a\\in \\mathbb{R}^{K}\\). Then \\(\\boldsymbol{a}^\\intercal\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{a}\\sim \\mathcal{\\chi}^2_{K}\\)\n\n\n\n2 Predictive variance for different regressors\nThis question will take the residuals of the training data to be random, and will consider variablity under sampling of the training data. The regressors for both the training data and test data will be taken as fixed.\nLet \\(\\boldsymbol{x}_n  = (x_{n1}, x_{n2})^\\intercal\\) be IID normal regressors, with\n\n\\(\\mathbb{E}\\left[x_{n1}\\right] = \\mathbb{E}\\left[x_{n2}\\right] = 0\\),\n\\(\\mathrm{Var}\\left(x_{n1}\\right) = \\mathrm{Var}\\left(x_{n2}\\right) = 1\\), and\n\\(\\mathrm{Cov}\\left(x_{n1}, x_{n2}\\right) = 0.99\\).\n\n(Note there is no intercept.)\nAssume that \\(y_n = \\beta^\\intercal\\boldsymbol{x}_n + \\varepsilon_n\\) for some \\(\\beta\\), and that the residuals \\(\\varepsilon_n\\) are IID with mean \\(0\\), variance \\(\\sigma^2 = 2\\), and are independent of \\(\\boldsymbol{x}_n\\).\n\n(a)\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\).\n\n\n(b)\nDefine the expected prediction error \\[\n\\hat{y}_\\mathrm{new}- \\mathbb{E}\\left[y_\\mathrm{new}\\right] :=  (\\hat{\\beta}- \\beta)^\\intercal x_\\mathrm{new},\n\\]\nand approximate the limiting variance \\(\\mathrm{Var}\\left(\\hat{y}_\\mathrm{new}- \\mathbb{E}\\left[y_\\mathrm{new}\\right]\\right)\\) for the following new regression vectors:\n\n\\(x_\\mathrm{new}= (1, 1)^\\intercal\\)\n\\(x_\\mathrm{new}= (1, -1)^\\intercal\\)\n\\(x_\\mathrm{new}= (100, 100)^\\intercal\\)\n\\(x_\\mathrm{new}= (0, 0)^\\intercal\\)\n\nYou may assume that \\(N\\) is large, so that you can apply the CLT to \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\). Even with the CLT approximation your answer will depend on \\(N\\); just make this dependence explicit.\n\n\n(c)\nWhy are some variances in (b) large and some small? Explain each in plain language and intuitive terms.\n\n\n\n3 The sandwich covariance matrix under homoeskedasticity\nFor this problem, make the following assumptions.\n\nThe regressors are non-random, with \\(\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\rightarrow \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\) for positive definite \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\)\nThe responses are \\(y_n = \\boldsymbol{\\beta}^\\intercal\\boldsymbol{x}_n + \\varepsilon_n\\) for some unknown \\(\\boldsymbol{\\beta}\\)\nThe residuals are IID with \\(\\mathbb{E}\\left[\\varepsilon_n\\right] = 0\\) and \\(\\mathrm{Var}\\left(\\varepsilon_n\\right) = \\sigma^2\\) (but not necessarily normal)\n\nUnder these assumptions, show that the sandwich covariance matrix and the standard covariance matrix converge to the same quantity. That is, show that\n\\[\n\\hat\\Sigma_{sand}  =\nN \\left(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1}\n  \\left(\\sum_{n=1}^Nx_n x_n^\\intercal\\hat{\\varepsilon}_n^2\\right)\n  \\left(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1} \\rightarrow \\boldsymbol{S}\n  \\quad\\textrm{and}\\quad\n\\hat\\Sigma_{h}  =\nN \\left(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1} \\hat{\\sigma}^2 \\rightarrow \\boldsymbol{S}\n\\]\nfor the same \\(\\boldsymbol{S}\\), where \\(\\hat{\\sigma}^2 := \\frac{1}{N} \\sum_{n=1}^N\\hat{\\varepsilon}_n^2\\)."
  },
  {
    "objectID": "assignments/midterm_grades.html",
    "href": "assignments/midterm_grades.html",
    "title": "STAT151A Mid-semester grades",
    "section": "",
    "text": "SIS.User.ID\noverall_grade\noverall_grade_with_drops\nattendance\nhomework\nquizzes\noverall_score\noverall_score_with_drops\n\n\n\n\n23043880\nF\nF\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n\n\n3032753492\nF\nF\n0.3333333\n0.0000000\n0.0000000\n0.0606061\n0.0606061\n\n\n3032769469\nB\nB\n0.9166667\n0.9638889\n0.7333333\n0.8505051\n0.8872727\n\n\n3034798937\nC\nA\n0.8333333\n0.6638889\n0.8250000\n0.7679293\n0.9631313\n\n\n3035247554\nA-\nA\n0.8333333\n0.9861111\n0.8916667\n0.9154040\n0.9492424\n\n\n3035621796\nF\nC\n0.2500000\n0.4888889\n0.5500000\n0.4732323\n0.7343434\n\n\n3035661147\nB\nA+\n0.7500000\n0.9930556\n0.7500000\n0.8383838\n1.0000000\n\n\n3035700134\nA-\nA\n1.0000000\n0.9972222\n0.8000000\n0.9080808\n0.9595960\n\n\n3035738042\nA+\nA+\n1.0000000\n0.9833333\n1.0000000\n0.9939394\n1.0000000\n\n\n3035762651\nF\nA-\n0.7500000\n0.4986111\n0.5916667\n0.5866162\n0.9030303\n\n\n3035767435\nF\nF\n0.3333333\n0.3277778\n0.3500000\n0.3388889\n0.3919192\n\n\n3035771153\nB\nA\n0.8333333\n0.9902778\n0.8083333\n0.8790404\n0.9689394\n\n\n3035798440\nD\nB\n0.7500000\n0.7597222\n0.4916667\n0.6361111\n0.8335859\n\n\n3035847996\nC\nB+\n0.7500000\n0.9625000\n0.5750000\n0.7477273\n0.8909091\n\n\n3035857512\nB\nA\n0.8333333\n0.9777778\n0.8083333\n0.8744949\n0.9515152\n\n\n3035859475\nB-\nB+\n0.9166667\n0.9888889\n0.6333333\n0.8141414\n0.8982323\n\n\n3035861477\nA\nA\n0.8333333\n0.9888889\n0.9666667\n0.9505051\n0.9824242\n\n\n3035888673\nD\nC-\n0.9166667\n0.3222222\n0.7500000\n0.6247475\n0.7030303\n\n\n3035917572\nD+\nC-\n1.0000000\n0.9125000\n0.4000000\n0.6954545\n0.7151515\n\n\n3036041982\nC\nA\n0.7500000\n0.6611111\n0.8916667\n0.7820707\n0.9477273\n\n\n3036514469\nC\nC\n1.0000000\n0.9486111\n0.4583333\n0.7351010\n0.7781818\n\n\n3036546761\nA\nA\n1.0000000\n0.9736111\n0.9333333\n0.9601010\n0.9672727\n\n\n3036578780\nA\nA+\n0.9166667\n0.9902778\n1.0000000\n0.9813131\n1.0000000\n\n\n3036588816\nC\nC\n1.0000000\n0.9638889\n0.4166667\n0.7217172\n0.7611111\n\n\n3036766978\nA\nA+\n0.9166667\n1.0000000\n1.0000000\n0.9848485\n1.0000000\n\n\n3037009717\nB\nA\n1.0000000\n0.6666667\n0.9333333\n0.8484848\n0.9848485\n\n\n3037109513\nA\nA\n1.0000000\n0.9666667\n0.9000000\n0.9424242\n0.9484848\n\n\n3037126700\nA\nA\n1.0000000\n0.9944444\n0.8583333\n0.9335859\n0.9747475\n\n\n3037135189\nA\nA\n0.7500000\n0.9902778\n0.9666667\n0.9358586\n0.9840909\n\n\n3037187699\nA-\nA\n0.8333333\n0.9763889\n0.8666667\n0.9005051\n0.9545455\n\n\n3037215637\nB\nA\n1.0000000\n0.9847222\n0.7250000\n0.8694444\n0.9487374\n\n\n3037322050\nA\nA+\n0.8333333\n0.9694444\n1.0000000\n0.9585859\n0.9984848\n\n\n3037876883\nC\nC+\n0.9166667\n0.9652778\n0.5333333\n0.7601010\n0.7969697\n\n\n3037878742\nD\nB\n1.0000000\n0.6555556\n0.4666667\n0.6323232\n0.8222222\n\n\n3037976957\nA\nA+\n0.8333333\n0.9958333\n1.0000000\n0.9681818\n1.0000000\n\n\n3038177014\nB\nA\n1.0000000\n0.9694444\n0.7416667\n0.8714646\n0.9472222\n\n\n3038275879\nD+\nC+\n1.0000000\n0.7791667\n0.5000000\n0.6924242\n0.7962121\n\n\n3039214206\nC\nA\n0.9166667\n0.6486111\n0.7666667\n0.7510101\n0.9396465\n\n\n3039716682\nA\nA\n0.8333333\n0.9750000\n0.9583333\n0.9416667\n0.9810606\n\n\n3039763235\nC\nB\n0.7500000\n0.9944444\n0.5416667\n0.7441919\n0.8484848\n\n\n3039811050\nD\nA\n0.7500000\n0.6638889\n0.6333333\n0.6656566\n0.9277778\n\n\n3039812142\nB\nA\n1.0000000\n0.9958333\n0.7583333\n0.8886364\n0.9444444\n\n\n3039815119\nA+\nA+\n1.0000000\n0.9875000\n1.0000000\n0.9954545\n1.0000000\n\n\n3039815392\nA\nA\n1.0000000\n0.9847222\n0.9166667\n0.9565657\n0.9674242\n\n\n3039819500\nF\nF\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n\n\n3039821632\nA\nA+\n0.9166667\n0.9916667\n0.9833333\n0.9742424\n1.0000000\n\n\n3039823751\nA\nA+\n0.9166667\n0.9902778\n0.9833333\n0.9737374\n0.9916667\n\n\n3039823816\nA+\nA+\n1.0000000\n0.9958333\n1.0000000\n0.9984848\n1.0000000\n\n\n3039824921\nA+\nA+\n1.0000000\n0.9902778\n1.0000000\n0.9964646\n0.9992424\n\n\n3039826533\nF\nF\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n\n\n3039828548\nF\nF\n0.1666667\n0.0000000\n0.0000000\n0.0303030\n0.0303030\n\n\n3039830160\nF\nF\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n\n\n3039831525\nB+\nA\n0.5833333\n0.9750000\n0.9500000\n0.8924242\n0.9742424\n\n\n3039831863\nA-\nA\n1.0000000\n0.9722222\n0.8333333\n0.9141414\n0.9285354\n\n\n3039832955\nB-\nA+\n0.5833333\n0.9958333\n0.7500000\n0.8090909\n1.0000000\n\n\n3039849972\nF\nF\n0.3333333\n0.9833333\n0.1333333\n0.4787879\n0.4848485"
  },
  {
    "objectID": "course_policies.html",
    "href": "course_policies.html",
    "title": "Syllabus and Course Structure",
    "section": "",
    "text": "Objectives\nBy the end of the course, you should be able to\n\nExpress standard regression analyses both mathematically and in R code\nCritically relate the intended use of a regression analysis to its methods and assumptions\nIdentify common practical and conceptual pitfalls of regression analysis, and to improve the analysis when possible\nCommunicate the process and results of a regression analysis simply and clearly for a broad audience, using well-organized prose, reproducible code, and effective data visualizations.\n\n\n\nAssignments, Exams, and Grading\n\nAttendance\nAttendance in lectures will be required and will contribute to the participation portion of the students’ grade. Laptops will not be permitted in lectures, and violation of this policy can constitute an absence for the purpose of the participation grade.\nIpads and phones will be permitted during lecture for note-taking as long as their use doesn’t inhibit participation.\nEach student will be given four lecture absences without losing any participation points, with the expectation that these absences will be used for illness and emergencies.\nAttendance at labs will be optional.\n\n\nGrading.\nThe weighting for the grades will be:\n\nHomework completion (each weighted equally): 16%\nHomework correctness (each weighted equally): 4%\nQuizzes (each weighted equally): 25%\nFinal exam: 25%\nGroup project: 20%\nParticipation (primarily lecture attendance): 10%\n\nGrades will not be curved except where otherwise noted. Letter grades will be assigned according the weighted points earned. A score within [90-92%) will earn an A-, [92-98%) will earn an A, and [99-100%) will earn an A+. Scores in the 80’s will receive B’s, in the 70’s will receive C’s, in the 60’s will receive D’s, with the same thresholds for plusses and minuses. Scores below 60% will be considered failing. Grades will be non-negotiable.\n\n\nFinal exam.\nAn in-person pencil-and-paper final exam will be scheduled during the usual final exam week. Students will be allowed a one-page double-sided “cheatsheet” on the final exam.\n\n\nQuizzes.\nEvery two weeks we will have an twenty-minute in-class quiz, typically on the Tuesday following a homework due date. These quizzes will take the place of a sitdown midterm exam (i.e., there will be no midterm). No external materials, including cheatsheets, will be allowed during quizzes.\n\n\nHomework.\nHomework assignments will be due every two weeks on Fridays two weeks later at 9pm. I will try to release new homework on the website as soon as possible after the old one is due, typically the following Monday evening. Homework will typically consist of a combination of mathematical problems and data analysis in R. All homework will be due as a pdf via Gradescope unless otherwise noted. Students can use whatever tool they like to produce the pdf (latex, Rmd, Jupyter, scanned handwritten notes for mathematical problems, etc.).\nThe purpose of homework is for students to attempt to work through problems on their own, both to advance their own understanding, and to allow the instructors to monitor student learning. Neither of these objectives are served if students are copying answers. For that reason, thoughtful and complete homework answers will receive nearly full credit (80% of the available homework points) even if incorrect. We strongly encourage students to submit their own best efforts, even if imperfect, rather than copy a correct answer.\n\n\nFinal group project.\nStudents will form groups of up to three people to submit a final project consisting of an analysis of a real dataset applying principles and techniques from the course.\n\n\nTurning-in assignments\nYou will be turning in your assignments on a platform called Gradescope. This is also the platform where your assignments will be graded, so you can return there to get feedback on your work. You are welcome to file a regrade request if you notice that we made an error in applying the rubric to your work, but be sure to do so within a week of the grades being posted. We will not accept regrade requests past that point.\nIn order to provide flexibility around emergencies that might arise for you throughout the semester (for example, missing a quiz due to COVID), we will apply for everyone:\n\none emergency drop for quizzes\n\ntwo emergency drops for homework\n\nThis means that we will drop your lowest quiz score (which would be a 0 if you were absent) before computing your quiz average. For homework, we will drop your two lowest. Unless students are excused by official university policies, additional drops will not be given.\nWe strongly recommend that students reserve their emergency drops for real emergencies.\n\n\nLate Work\nLate work will not be accepted. If work is not submitted on time, it will receive a zero.\nIt is entirely the students’ responsibility to turn work in on time.\n\n\n\nPrerequisites\nThis course will assume familiarity with the material in STAT 135 or STAT 102. STAT 135 implies other prerequisite courses (STAT 134 and its prerequisites). In particular, you must have had linear algebra, so you should be familiar with basic matrix operations, vector subspaces and projections, rank and invertibility of matrices, and quadratic forms.\nThis semester of Stat151A will include labs and projects in the R language. Proficiency with R at the level of the is a prerequisite. Students with a strong background in another programming language (e.g. Python) will be permitted to enroll with the understanding that they will learn R on their own prior to the start of the class.\n\n\nMaterials\nUnelss otherwise noted, the primary materials for the course are the lecture notes, which will be posted to the course website in advance of class. The following textbooks are useful supplementary texts and are all freely available online:\n\nLinear Models and Extensions Ding\nRegression and other Stories Gelman, Hill, Vehtari\nAn Introduction to Statistical Learning James, Witten, Hastie, Tibshirani\nR for Data Science, Wickham, Grolemund\nEconometric Theory and Methods Davidson, MacKinnon\n\n\nRStudio\nThe software that we’ll be using for our data analysis is the free and open-source language called R that we’ll be interacting with via software called RStudio. If you have difficulty installing RStudio, please reach out to an instructor.\n\n\nCourse website\nAll of the assignments will be posted to the course website at https://stat151a.berkeley.edu/spring-2024/. This also holds the course notes, the syllabus, and links to Gradescope and RStudio.\n\n\n\nPolicies\n\nCourse Culture\nStudents taking STAT151A come from a wide range of backgrounds. We hope to foster an inclusive and supportive learning environment based on curiosity rather than competition. All members of the course community—the instructor, students, tutors, and readers—are expected to treat each other with courtesy and respect.\nYou will be interacting with course staff and fellow students in several different environments: in class, over the discussion forum, and in office hours. Some of these will be in person, some of them will be online, but the same expectations hold: be kind, be respectful, be professional.\nIf you are concerned about classroom environment issues created by other students or course staff, please come talk to the instructors about it.\n\n\nCollaboration policy\nYou are encouraged to collaborate with your fellow students on problem sets and labs, but the work you turn in should reflect your own understanding and all of your collaborators must be cited. The individual component of quizzes, reading questions, and exams must reflect only your work.\nResearchers don’t use one another’s research without permission; scholars and students always use proper citations in papers; professors may not circulate or publish student papers without the writer’s permission; and students may not circulate or post non-public materials (quizzes, exams, rubrics-any private class materials) from their class without the written permission of the instructor.\nThe general rule: you must not submit assignments that reflect the work of others unless they are a cited collaborator.\nThe following examples of collaboration are allowed and in fact encouraged!\n\nDiscussing how to solve a problem with a classmate.\nShowing your code to a classmate along with an error message or confusing output.\nPosting snippets of your code to the discussion forum when seeking help.\nHelping other students solve questions on the discussion with conceptual pointers or snippets of code that doesn’t whole hog give away the answer.\nGoogling the text of an error message.\nCopying small snippets of code from answers on Stack Overflow.\n\nThe following examples are not allowed:\n\nLeaving a representation of your assignment (the text, a screenshot) where students (current and future) can access it. Examples of this include websites like course hero, on a group text chain, over discord/slack, or in a file passed on to future students.\nAccessing and submitting solutions to assignments from other students distributed as above. This includes copying written answers from other students and slightly modifying the language to differentiate it.\nSearching or using generative AI to produce complete problem solutions.\nWorking on the final exam or individual quizzes in collaboration with other people or resources. These assignments must reflect individual work.\nSubmitting work on an exam that reflects consultation with outside resources or other people. Exams must reflect individual work.\n\nIf you have questions about the boundaries of the policy, please ask. We’re always happy to clarify.\n\n\nViolations of the collaboration policy\nThe integrity of our course depends on our ability to ensure that students do not violate the collaboration policy. We take this responsibility seriously and forward cases of academic misconduct to the Center for Student Conduct.\nStudents determined to have violated the academic misconduct policy by the Center for Student Conduct will receive a grade penalty in the course and a sanction from the university which is generally: (i) First violation: Non-Reportable Warning and educational intervention, (ii) Second violation: Suspension/Disciplinary Probation and educational interventions, (iii) Third violation: Dismissal.\nAnd again, if you have questions about the boundaries of the collaboration policy, please ask!\n\n\nLaptop policy\nLaptops will not be permitted in lecture, but will be required for labs.\nIf you do not have access to a laptop, you can borrow one from the University library. See the UC Berkeley hardware lending program for more details. The Student Technology Equity Program is another good resource. Feel free to contact the instructor if you have concerns about your access to needed technology.\n\n\nCOVID policy\nMaintaining your health and that of the Berkeley community is of primary importance to course staff, so if you are feeling ill or have been exposed to illness, please do not come to class. All of the materials used in class will be posted to the course website. You’re encouraged to reach out to fellow students to discuss the class materials or stop by group tutoring or office hours to chat with a tutor or the instructor.\n\n\nAccomodations for students with disabilities\nStat 151A is a course that is designed to allow all students to succeed. If you have letters of accommodations from the Disabled Students’ Program, please share them with your instructor as soon as possible, and we will work out the necessary arrangements.\n\n\n\n\n\n\nNote\n\n\n\nThese course polices are based on a template and text generously shared by Andrew Bray. Thanks, Andrew!",
    "crumbs": [
      "Course Policies"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 151A: Linear Models",
    "section": "",
    "text": "Gradescope\n\n  RStudio\n\n  BCourses\n\n  ED\n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Statistics 151A: Linear Models",
    "section": "Instructors",
    "text": "Instructors\n\n\nInstructor: Ryan Giordano  Office: 389 Evans Hall Office hours: 9:30am-11am Mondays (subject to change) rgiordano@berkeley.edu pronouns: He / him\n\n\nGSI: Dohyeong Ki  Office: TBD Evans Hall Office hours: 3:30pm-5:30pm Thursdays (subject to change) dohyeong_ki@berkeley.edu pronouns: He / him\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDay-to-day announcements can be found in BCourses. Discussions can be found in ED. (See links above.)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Statistics 151A: Linear Models",
    "section": "Schedule",
    "text": "Schedule\nLectures will be held Jan 16 2024 – May 03 2024 on Tuesday and Thursday, 9:30 am – 10:59 am, in Etcheverry 3108.\nLabs will be held on Wednesdays from 9:00 am – 11:00 am and 1:00pm – 3:00 pm in Evans 344.\n(Link to official course calendar.)\nThe following subject schedule is tentative and subject to change.\n\n\n   Week 1\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Jan 16:\n           \n           Lecture 1 Class goals and organization.\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Jan 18:\n           \n           Lecture 2 Simple regression as EDA, prediction, and inference\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 2\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Jan 23:\n           \n           Lecture 3 Multilinear regression as loss minimization\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Jan 25:\n           \n           Lecture 4 Multilinear regression as projection\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 3\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Jan 30:\n           \n           Lecture 5 Different ways to draw lines through points\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Feb 1:\n           \n           Lecture 6 Transformations of regressors: Some payoffs from the linear algebra perspective\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 4\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Feb 6:\n           \n           Lecture 7 Univariate statistics and limit theorems\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Feb 8:\n           \n           Lecture 8 Vector and matrix-valued statistics and limit theorems\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 5\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Feb 13:\n           \n           Lecture 9 The Gaussian assumption\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Feb 15:\n           \n           Lecture 10 Simulations and the law of large numbers\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 6\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Feb 20:\n           \n           Lecture 11 Consistency of OLS and the residual variance under the Gaussian assumptions\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Feb 22:\n           \n           Lecture 12 Linear algebra review and Quiz 2 retake\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 7\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Feb 27:\n           \n           Lecture 13 Unbiased estimates of the residual variance\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Feb 29:\n           \n           Lecture 14 Sampling variability of the coefficients\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 8\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Mar 5:\n           \n           Lecture 15 Implications of Gaussianity (and deviations from it)\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Mar 7:\n           \n           Lecture 16 Heteroskedasticity and the sandwich covariance matrix\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 9\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Mar 12:\n           \n           Lecture 17 Interpreting the coefficients\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Mar 14:\n           \n           Lecture 18 Inference on the coefficients: Gaussian versus asymptotic results\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 10\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Mar 19:\n           \n           Lecture 19 The role of the regressor covariance in uncertainty estimates\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Mar 21:\n           \n           Lecture 20 Hypothesis testing and confidence intervals\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 11\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Mar 25:\n           \n           Holiday  Spring break\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 12\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Apr 2:\n           \n           Lecture 21 Variable selection and the F-test\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Apr 4:\n           \n           Lecture 22 Cross validation and information criteria\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 13\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Apr 9:\n           \n           Lecture 23 Ridge regression\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Apr 11:\n           \n           Lecture 24 LASSO and variable selection\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 14\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Apr 16:\n           \n           Lecture 25 Conformal prediction\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Apr 18:\n           \n           Lecture 26 Influence and outliers\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 15\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Apr 23:\n           \n           Lecture 27 Project consultation\n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Apr 25:\n           \n           Lecture 28 Project consultation\n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "lectures/Lecture10.html",
    "href": "lectures/Lecture10.html",
    "title": "Estimating prediction uncertainty: Estimating the residual variance",
    "section": "",
    "text": "\\(\\LaTeX\\)"
  },
  {
    "objectID": "lectures/Lecture10.html#method-1-apply-lln-to-the-squared-residuals",
    "href": "lectures/Lecture10.html#method-1-apply-lln-to-the-squared-residuals",
    "title": "Estimating prediction uncertainty: Estimating the residual variance",
    "section": "Method 1: Apply LLN to the squared residuals",
    "text": "Method 1: Apply LLN to the squared residuals"
  },
  {
    "objectID": "lectures/Lecture10.html#method-2-projection-to-get-the-unbiased-estimate",
    "href": "lectures/Lecture10.html#method-2-projection-to-get-the-unbiased-estimate",
    "title": "Estimating prediction uncertainty: Estimating the residual variance",
    "section": "Method 2: Projection to get the unbiased estimate",
    "text": "Method 2: Projection to get the unbiased estimate\n(Note connection to RSS.)"
  },
  {
    "objectID": "lectures/Lecture12.html",
    "href": "lectures/Lecture12.html",
    "title": "Residual distribution under normality",
    "section": "",
    "text": "Derive the exact sampling distribution of the residuals under Gaussianity\n\nFind an unbiased estimate of \\(\\sigma^2\\)\nProve consistency using the exact distribution\nProve independence of \\(\\sigmahat^2\\) and \\(\\betahat\\)"
  },
  {
    "objectID": "lectures/Lecture12.html#consequences-of-the-chi-squared-distribution",
    "href": "lectures/Lecture12.html#consequences-of-the-chi-squared-distribution",
    "title": "Residual distribution under normality",
    "section": "Consequences of the chi-squared distribution",
    "text": "Consequences of the chi-squared distribution\nFrom standard properties of the chi-squared distribution, some immediate results follow.\n\nVariance\nAs expected, the variance of our estimator around the truth goes to zero as \\(N \\rightarrow \\infty\\), since\n\\[\n\\var{\\sigmahat^2 - \\sigma^2} = \\sigma^4 \\var{\\frac{\\s}{N} - 1}\n= \\frac{\\sigma^4}{N^2} \\var{\\s} = \\frac{2(N - P)}{N^2}\\sigma^4 \\rightarrow 0.\n\\]\nThis is essentially the chi-squared version of the proof we already gave using the LLN.\n\n\nBias\nFirst, we can see that\n\\[\n\\expect{\\sigmahat^2} = \\sigma^2 \\frac{N-P}{N}.\n\\]\nThat is, our estimator \\(\\sigmahat^2\\) is downwardly biased, since\n\\[\n\\expect{\\sigmahat^2 - \\sigma^2} = \\sigma^2\\left( \\frac{N-P}{N} - 1 \\right) = \\sigma^2 \\frac{-P}{N}.\n\\]\nThis bias vanishes as \\(N \\rightarrow \\infty\\), but for any particular \\(N\\), we will under-estimate \\(\\sigma^2\\) on average. Indeed, this might be expected! Recall that we chose\n\\[\n\\resvhat^\\trans \\resvhat = \\argmin{\\betav'} (\\Y - \\X\\betav')^\\trans(\\Y - \\X\\betav') \\le\n  (\\Y - \\X\\betav)^\\trans(\\Y - \\X\\betav) = \\resv^\\trans \\resv.\n\\]\nThat is, because we are fitting the sum of squares, we always get a lower estimate of the sum of squared residuals than we would have if we had used the true value of \\(\\beta\\). Since using the true value gives us an unbiased estimator, we bias \\(\\sigmahat^2\\) by searching over \\(\\beta\\). And we bias it more, the more “degrees of freedom” we search over.\nFor this reason, many authors use the estimator\n\\[\n\\sigmahat^2_{0} = \\frac{1}{N - P} \\sumn \\reshat_n^2\n\\quad\\textrm{instead of}\\quad\n\\sigmahat^2 = \\frac{1}{N} \\sumn \\reshat_n^2,\n\\]\nsince \\[\n\\expect{\\sigmahat^2_{0}} = \\sigma^2\n\\quad\\textrm{but}\\quad\n\\expect{\\sigmahat^2} = \\frac{N-P}{N} \\sigma^2 &lt; \\sigma^2.\n\\]\nIn other words, \\(\\sigmahat^2_0\\) is unbiased. Normalizing by \\(N - P\\) instead of \\(N\\) doesn’t affect the fact that \\(\\sigmahat^2_0 \\rightarrow \\sigma^2\\), so in a sense one may as well use \\(\\sigmahat^2_0\\), though in practice the difference is small as long as \\(P \\ll N\\).\nSince both \\(\\sigmahat^2_{0}\\) and \\(\\sigmahat^2\\) go to the truth as \\(N \\rightarrow \\infty\\), it doesn’t matter much asymptotically which one you use. Therefore it makes sense to use \\(\\sigmahat^2_{0}\\), since it will sometimes be better, and is no worse for large \\(N\\).\n\n\nIndependence\nRecall that we’ve shown that\n\\[\n\\betavhat = \\betav + (\\X^\\trans \\X)^{-1} \\X^\\trans \\resv\n\\quad\\textrm{and}\\quad\n\\resvhat = \\proj{\\X^\\perp} \\resv.\n\\]\nAs a consequence, we have shown that both are multivariate normal. Furthermore,\n\\[\n\\cov{\\betavhat, \\resvhat}\n  = \\cov{(\\X^\\trans \\X)^{-1} \\X^\\trans \\resv, \\proj{\\X^\\perp} \\resv}\n  = \\expect{(\\X^\\trans \\X)^{-1} \\X^\\trans \\resv \\resv^\\trans \\proj{\\X^\\perp} }\n  = \\sigma^2 (\\X^\\trans \\X)^{-1} \\X^\\trans \\proj{\\X^\\perp}\n  = \\zerov,\n\\]\nsince \\(\\proj{\\X^\\perp} \\X = \\zerov\\). So it follows that not only are \\(\\betavhat\\) and \\(\\resvhat\\) normal, they are independent, since for normal random variables, uncorrelatedness implies independence.\nFurther, since \\(\\sigmahat^2\\) is simply a deterministic function of \\(\\resvhat\\), it follows that \\(\\betavhat\\) and \\(\\sigmahat^2\\) are independent! This will be useful shortly when we derive confidence intervals for \\(\\betahat\\) under the normal assumptions."
  },
  {
    "objectID": "lectures/Lecture12.html#bonus-content-alternative-simpler-derivation-of-the-bias-of-the-variance-estimator",
    "href": "lectures/Lecture12.html#bonus-content-alternative-simpler-derivation-of-the-bias-of-the-variance-estimator",
    "title": "Residual distribution under normality",
    "section": "Bonus content: Alternative (simpler) derivation of the bias of the variance estimator",
    "text": "Bonus content: Alternative (simpler) derivation of the bias of the variance estimator\nIf all we want is \\(\\expect{\\sigmahat^2}\\), we can do a trick to avoid the chi squared distribution and eigendecomposition of the projection matrix.\nIn particualr, \\(\\reshat^\\trans \\reshat\\) is a scalar, and the matrix trace of a scalar is also a scalar, so\n\\[\n\\resv^\\trans \\proj{\\X^\\perp}  \\resv = \\trace{\\resv^\\trans \\proj{\\X^\\perp}  \\resv} =\n\\trace{\\proj{\\X^\\perp}  \\resv \\resv^\\trans },\n\\]\nwhere we have used the general property of the trace that \\(\\trace{AB} = \\trace{BA}\\). Combining the above results, we see that\n\\[\n\\begin{aligned}\n\\expect{\\resvhat^\\trans \\resvhat} ={}&\n\\expect{\\trace{\\proj{\\X^\\perp}  \\resv \\resv^\\trans }}\n\\\\={}&\n\\trace{\\proj{\\X^\\perp}  \\expect{\\resv \\resv^\\trans }}\n\\\\={}&\n\\trace{\\proj{\\X^\\perp}  \\sigma^2 \\id } & \\textrm{(by A2)}\n\\\\={}&\n\\sigma^2  \\trace{\\proj{\\X^\\perp} } & \\textrm{(by A2)}.\n\\end{aligned}\n\\]\nNow, since \\(\\proj{\\X^\\perp}\\) is a projection matrix onto an \\(N - P\\) dimensional space, \\(\\trace{\\proj{\\X^\\perp}} = N - P\\), so we get\n\\[\n\\expect{\\resvhat^\\trans \\resvhat} = (N - P) \\sigma^2.\n\\]"
  },
  {
    "objectID": "lectures/Lecture14.html",
    "href": "lectures/Lecture14.html",
    "title": "Implications of Gaussianity (and deviations from it)",
    "section": "",
    "text": "\\(\\,\\)"
  },
  {
    "objectID": "lectures/Lecture14.html#distribution-of-the-ols-coefficients",
    "href": "lectures/Lecture14.html#distribution-of-the-ols-coefficients",
    "title": "Implications of Gaussianity (and deviations from it)",
    "section": "Distribution of the OLS coefficients",
    "text": "Distribution of the OLS coefficients\nWe still have that\n\\[\n\\betahat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y =\n  (\\X^\\trans \\X)^{-1} \\X^\\trans (\\X \\beta + \\resv) =\n  \\beta + (\\X^\\trans \\X)^{-1} \\X^\\trans \\resv.\n\\]\nThat means that \\(\\expect{\\betahat} = \\beta\\), so our estimator is still unbiased. But the term \\((\\X^\\trans \\X)^{-1} \\X^\\trans \\resv\\) is no longer normal. It remains the case that\n\\[\n\\cov{(\\X^\\trans \\X)^{-1} \\X^\\trans \\resv} =\n  (\\X^\\trans \\X)^{-1} \\X^\\trans \\expect{\\resv\\resv^\\trans} \\X (\\X^\\trans \\X)^{-1} =\n  \\sigma^2 (\\X^\\trans \\X)^{-1} \\rightarrow \\zerov.\n\\]\nThis means that \\(\\betahat \\rightarrow \\beta\\). This is expected, since we can recall our LLN proof of the consistency of \\(\\betahat\\):\n\\[\n\\begin{aligned}\n\\betahat - \\beta ={}& (\\X^\\trans \\X)^{-1} \\X^\\trans \\resv \\\\\n={}& (\\frac{1}{N} \\X^\\trans \\X)^{-1} \\frac{1}{N} \\X^\\trans \\resv \\\\\n={}& (\\meann \\xv_n \\xv_n^\\trans)^{-1} \\meann \\xv_n \\res_n.\n\\end{aligned}\n\\]\nNow, \\((\\meann \\xv_n \\xv_n^\\trans)^{-1} \\rightarrow \\Xcov^{-1}\\) by the LLN and the continuous mapping theorem, and\n\\[\n\\meann \\xv_n \\res_n \\rightarrow \\expect{\\xv_n \\res_n} = \\xv_n \\expect{\\res_n} = \\zerov,\n\\]\nsimply using the fact that \\(\\xv_n\\) and \\(\\res_n\\) are independent (\\(\\xv_n\\) is still fixed) and \\(\\expect{\\res_n} = 0\\).\nAlthough we don’t know the finite-sample distribution of \\(\\betahat - \\beta\\), the LLN points to a way to approximation the asymptotic distribution of \\(\\betahat - \\beta\\) via the CLT. Specifically, note that \\(\\xv_n \\res_n\\) are not IID, but \\(\\expect{\\xv_n \\res_n} = 0\\), and \\(\\cov{\\xv_n \\res_n} = \\xv_n \\xv_n^\\trans \\sigma^2\\). Noting that\n\\[\n\\meann \\cov{\\xv_n \\res_n} = \\sigma^2 \\meann \\xv_n \\xv_n^\\trans \\rightarrow \\sigma^2 \\Xcov,\n\\]\nby the multivariate CLT, \\[\n\\frac{1}{\\sqrt{N}} \\sumn \\xv_n \\res_n \\rightarrow \\gauss{0, \\sigma^2 \\Xcov}.\n\\]\nThus, by the continuous mapping theorem,\n\\[\n\\sqrt{N}(\\betahat - \\beta) = (\\meann \\xv_n \\xv_n^\\trans)^{-1} \\frac{1}{\\sqrt{N}} \\sumn \\xv_n \\res_n\n\\rightarrow \\Xcov^{-1} \\RV{z}\n\\quad\\textrm{where}\\quad \\RV{\\z} \\sim \\gauss{0, \\sigma^2 \\Xcov}.\n\\]\nNow, by properties of the multivariate normal,\n\\[\n\\Xcov^{-1} \\RV{z} \\sim \\gauss{0, \\sigma^2 \\Xcov^{-1} \\Xcov \\Xcov^{-1}} = \\gauss{0, \\sigma^2 \\Xcov^{-1}},\n\\]\nso\n\\[\n\\sqrt{N}(\\betahat - \\beta) \\rightarrow \\gauss{0, \\sigma^2 \\Xcov^{-1}}.\n\\]"
  },
  {
    "objectID": "lectures/Lecture14.html#plug-in-estimators-for-the-variance",
    "href": "lectures/Lecture14.html#plug-in-estimators-for-the-variance",
    "title": "Implications of Gaussianity (and deviations from it)",
    "section": "Plug-in estimators for the variance",
    "text": "Plug-in estimators for the variance\nOf course, in practice, we do not observe the terms in the variance \\(\\sigma^2 \\Xcov^{-1}\\). A natural solution is to plug in their consistent estimators,\n\\[\n\\begin{aligned}\n\\sigmahat^2 \\rightarrow \\sigma^2 \\quad\\textrm{and}\\quad\n\\frac{1}{N} \\X^\\trans \\X \\rightarrow \\Xcov.\n\\end{aligned}\n\\]\nWe thus say that\n\\[\n\\betahat - \\beta \\sim \\gauss{0, \\frac{1}{N} \\sigmahat^2 \\left( \\frac{1}{N} \\X^\\trans \\X \\right)^{-1}}\n\\quad\\textrm{approximately for large }N.\n\\]\nRecall that, under normality, we had\n\\[\n\\betahat - \\beta \\sim \\gauss{0, \\frac{1}{N} \\sigma^2 \\left(\\frac{1}{N} \\X^\\trans \\X\\right)^{-1}}\n\\quad\\textrm{exactly, under the normal assumption, for all }N.\n\\]\nWe see that the CLT gives the matching distribution for large \\(N\\) — the difference is that the Normal distribution is justified for large \\(N\\) by the CLT rather than by exact normality. In this sense, the normal assumptions are not essential for approximating the sampling distribution of \\(\\betahat - \\beta\\)."
  },
  {
    "objectID": "lectures/Lecture14.html#using-the-limiting-distribution-in-the-predictive-distribution",
    "href": "lectures/Lecture14.html#using-the-limiting-distribution-in-the-predictive-distribution",
    "title": "Implications of Gaussianity (and deviations from it)",
    "section": "Using the limiting distribution in the predictive distribution",
    "text": "Using the limiting distribution in the predictive distribution\nUnfortunately, the normal assumption plays a much more important role in the predictive distribution. To see this, we can write as usual\n\\[\n\\y_new - \\yhat_\\new = (\\beta - \\betahat)^\\trans \\xv_\\new + \\res_n.\n\\]\nWe can say that \\((\\beta - \\betahat)^\\trans \\xv_\\new\\) is approximately normal for large \\(N\\) using the CLT. However, since the distribution of \\(\\res_n\\) is unknown, the distribution of \\(\\y_new - \\yhat_\\new\\) is unknown, even for large \\(N\\).\nAs a simple example, we could take\n\\[\n\\res_n = \\begin{cases}\n1 & \\textrm{with probability }1/2\\\\\n-1 & \\textrm{with probability }1/2\\\\\n\\end{cases}.\n\\]\nThese residuals satisfy the assumptions, but are very non-normal and, normal intervals will in general have poor coverage.\nThere are good solutions to produce well-calibrate predictive intervals even in the case of severe non-normality, using only the assumption that \\((\\xv_n, \\y_n)\\) are IID. For interested students, I recommend starting with A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification by Anastasios N. Angelopoulos, Stephen Bates. If we have time, we will cover conformal inference towards the end of the course."
  },
  {
    "objectID": "lectures/Lecture16.html",
    "href": "lectures/Lecture16.html",
    "title": "Sampling variability of the coefficients",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\n\\(\\,\\)\n\nGoals\n\nDerive the exact sampling distribution of the coefficients under normality\n\nReview properties of the variance estimator\nIntroduce the student t distribution and some of its basic properties\n\n\n\n\nSetup\nUp to now, we’ve been studying prediction intervals. We’ll shortly be turning to inference on the regression coefficients themselves. But first, we will wrap up one remaining topic: the exact sampling distribution of the regression coefficients under normality.\nFor this lecture, we’ll return to the normal assumptions:\n\n\\(\\y_n = \\betav^\\trans \\xv_n + \\res_n\\) for some \\(\\betav\\)\n\\(\\res_n \\sim \\gauss{0, \\sigma^2}\\), IID\nThe regressors are non-stochastic, \\(\\X\\) is full rank, and \\(\\frac{1}{N} \\X^\\trans \\X \\rightarrow \\Xcov\\) for positive definite \\(\\Xcov\\).\n\nRecall that we have already proven, under the normal assumption, that\n\\[\n\\betavhat \\sim \\gauss{\\betav, \\sigma^2 (\\X^\\trans \\X)^{-1}}.\n\\]\nUp until now we have formed predictive intervals for an unknown \\(\\y_\\new\\). Let’s instead try to form a random interval \\(I\\) such that some particular entry of \\(\\beta\\) is in the interval with some given probability, i.e., for some \\(\\alpha \\in (0,1)\\),\n\\[\n\\prob{\\beta_k \\in I} = 1 - \\alpha.\n\\]\nFirst we need to relate \\(\\beta_k\\) to the distribution of \\(\\betav\\). Note that, if we take \\(\\vv\\) to be a vector with \\(0\\) everywhere except in the \\(k\\)–th location, then \\(\\beta_k = \\vv^\\trans \\betav\\). For any vector \\(\\av\\), we have that\n\\[\n\\av^\\trans \\betavhat \\sim \\gauss{\\av^\\trans\\betav, \\sigma^2 \\av^\\trans(\\X^\\trans \\X)^{-1}\\av}.\n\\]\nIn particular,\n\\[\n\\betahat_k \\sim \\gauss{\\beta_k, \\sigma^2 (\\X^\\trans \\X)^{-1}_{kk}}.\n\\]\nHere, \\((\\X^\\trans \\X)^{-1}_{kk}\\) is the \\(k\\)–th diagonal entry of the matrix \\((\\X^\\trans \\X)^{-1}\\). For compactness, let’s write \\(\\v_k := (\\X^\\trans \\X)^{-1}_{kk}\\), noting that \\(\\v_k\\) is a constant under the present assumptions.\n\n\n\n\n\n\nExercise\n\n\n\nProve that, when \\(\\X^\\trans \\X\\) is diagonal, then\n\\[\n(\\X^\\trans \\X)^{-1}_{kk} = \\frac{1}{(\\X^\\trans\\X)_{kk}}.\n\\]\nThen show that this equality does not hold in general when \\(\\X^\\trans \\X\\) is not diagonal.\n\n\nIf we knew \\(\\sigma\\), we could write as usual\n\\[\n\\begin{aligned}\n\\frac{\\betahat_k - \\beta_k}{\\sigma \\v_k} \\sim{}& \\gauss{0, 1} \\Rightarrow\\\\\n\\prob{-a \\le \\frac{\\betahat_k - \\beta_k}{\\sigma \\v_k} \\le a} ={}& \\Phi(a) - \\Phi(-a) \\\\\n={}& 1 - \\Phi(-a) - \\Phi(-a)  \\\\\n={}& 1 - 2 \\Phi(-a)  = 1 - \\alpha \\Rightarrow \\\\\n\\Phi(-a) ={}& \\frac{\\alpha}{2} \\Rightarrow \\\\\na ={}& - \\Phi^{-1}\\left(\\frac{\\alpha}{2}\\right) \\Rightarrow \\\\\nI ={}& (\\betahat_k - \\sigma \\v_k a, \\betahat_k + \\sigma \\v_k a).\n\\end{aligned}\n\\]\nHowever, we don’t know \\(\\sigma^2\\). We can use the fact that \\(\\sigmahat \\rightarrow \\sigma\\), and use\n\\[\n\\hat{I} ={} (\\betahat_k - \\sigmahat \\v_k a, \\betahat_k + \\sigmahat \\v_k a).\n\\]\nIntuitively, we may want to account for the variability in \\(\\sigmahat\\) as well. In the normal case, we can do that in a precise way.\n\n\nAccounting for variance variance\nRecall that we have shown that\n\\[\n\\sigmahat^2 := \\frac{1}{N - P} \\sumn \\reshat_n^2\n= \\frac{\\sigma^2}{N-P} \\s\n\\quad\\textrm{where}\\quad\n\\s \\sim \\chisq{N-P},\n\\]\nand that, furthermore, \\(\\sigmahat\\) and \\(\\betahat\\) are independent of one another.\n\n\n\n\n\n\nBad notation warning\n\n\n\nNote: here, we are going to normalize \\(\\sigmahat\\) with \\(N-P\\) rather than \\(N\\).\n\n\nLet’s try to use this fact to get a probability distribution for \\(\\betahat_k\\) that takes the variability of \\(\\sigmahat\\) into account. Write\n\\[\n\\begin{aligned}\n\\frac{\\betahat_k - \\beta_k}{\\v \\sigmahat} ={}&\n\\frac{\\betahat_k - \\beta_k}{\\v \\sqrt{\\frac{\\sigma^2}{N-P} \\s}}\n\\\\={}&\n\\left(\\frac{\\betahat_k - \\beta_k}{\\v \\sigma}\\right)\n   \\Big/   \\sqrt{\\frac{\\s}{N-P} }.\n\\end{aligned}\n\\]\nAs we showed before, \\(\\frac{\\betahat_k - \\beta_k}{\\v \\sigma} \\sim \\gauss{0,1}\\). Furthermore, it is independent of \\(\\s\\). The denominator is approximately \\(1\\) for large \\(N - P\\), but has some sampling variability. The ratio of a normal to an independent chi squared happens to be a known distribution.\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(\\z \\sim \\gauss{0,1}\\), and \\(\\s \\sim \\chisq{N-P}\\), independently of one another. Then the distribution of \\[\n\\t := \\frac{\\z}{\\sqrt{\\s / (N-P)}}\n\\]\nis called a ``student-t distribution with \\(N-P\\) degrees of freedom.’’ We write \\(\\t \\sim \\studentt{N - P}\\). As long as \\(N - P &gt; 2\\), \\(\\expect{\\t} = 0\\) and \\(\\var{\\t} = (N - P) / (N - P - 2)\\).\n\n\n\n\n\n\n\n\nExercise\n\n\n\nLet \\(\\t \\sim \\studentt{K}\\) with \\(K &gt; 2\\).\n\nProve that \\(\\expect{\\t} = 0\\).\nWithout using the above explicit formulat, prove that \\(\\var{\\t} &gt; 1\\).\n(Hint: use the fact that, for any positive non-constant random variable, \\(\\RV{x}\\), \\(\\expect{1 / \\RV{x}} &gt; 1 / \\expect{\\RV{x}}\\), by Jensen’s inequality.)\n\n\n\nYou can find quantiles of the student-t distributions using the R function qt(), just as you would use rnorm().\nGiven all this, we have shown that\n\\[\n\\prob{\\frac{\\betahat_k - \\beta_k}{\\v \\sigmahat} \\le \\z_\\alpha}  = \\prob{\\t \\le \\z_\\alpha}\n\\quad\\textrm{where}\\quad\n\\t \\sim \\studentt{N - P}.\n\\]\nUsing this formula, we can find exact intervals for our marginal prediction error under normality.\n\n\n\n\n\n\nExercise\n\n\n\nDerive student t intervals for \\(\\av^\\trans \\beta\\), for a generic vector \\(\\av\\)."
  },
  {
    "objectID": "lectures/Lecture18.html",
    "href": "lectures/Lecture18.html",
    "title": "Interpreting the coefficients: The FWL theorem",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\n\\(\\,\\)\n\nGoals\n\nInterpret the OLS errors and estimates for multiple linear regression\n\nThe The Frisch–Waugh–Lovell Theorem (FWL) theorem (section 7 of Prof. Ding’s lecture notes)\nThe role of regressor covariance in the OLS standard errors\n\n\n\n\nCorrelated regressors\nTake \\(\\X = (\\xv_1, \\ldots, \\xv_P)\\), where \\(\\xv_1 = \\onev\\), so that we are regressing on \\(P-1\\) regressors and a constant. If the regressors are all orthogonal to one another (\\(\\xv_k^\\trans \\xv_j = 0\\) for \\(k \\ne j\\)), then we know that\n\\[\n\\betahat = (\\X^\\trans\\X)^{-1} \\X^\\trans \\Y =\n\\begin{pmatrix}\n\\xv_1^\\trans \\xv_1 & 0 & \\ldots 0 \\\\\n0 & \\ddots &  0 \\\\\n0 &  \\ldots &  \\xv_P^\\trans \\xv_P  \\\\\n\\end{pmatrix}^{-1}\n\\begin{pmatrix}\n\\xv_1^\\trans \\Y \\\\\n\\vdots \\\\\n\\xv_P^\\trans \\Y \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{\\xv_1^\\trans \\Y}{\\xv_1^\\trans \\xv_1} \\\\\n\\vdots \\\\\n\\frac{\\xv_P^\\trans \\Y}{\\xv_P^\\trans \\xv_P} \\\\\n\\end{pmatrix}.\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nWhat is the limiting behavior of \\(\\frac{1}{N}\\X^\\trans \\X\\) when the \\(\\xv_n\\) are independent of one another? What if they are independent and \\(\\expect{\\xv_n} = 0\\), except for a constant \\(\\xv_{n1} = 1\\)?\n\n\nHowever, typically the regressors are not orthogonal to one another. When they are not, we can ask\n\nHow can we interpret the coefficients?\nHow does the relation between the regressors affect the \\(\\betavhat\\) covariance matrix?\n\nFor simplicity, the remainder of this lecture will assume homeskedastic errors and random regressors.\n\n\nThe FWL theorem\nThe FWL theorem gives an expression for sub-vectors of \\(\\betavhat\\). Specifically, let’s partition our regressors into two sets:\n\\(\\y_n \\sim \\xv_n^\\trans \\beta = \\av_{n}^\\trans \\betav_a + \\bv_{n}^\\trans \\betav_b\\),\nwhere \\(\\betav^\\trans = (\\betav_a^\\trans, \\betav_b^\\trans)\\) and \\(\\xv_n^\\trans = (\\av_n^\\trans, \\bv_n^\\trans)\\). We can similarly partition our regressors matrix into two parts \\[\n\\X = (\\X_a \\, \\X_b).\n\\]\nA particular example to keep in mind is where\n\\[\n\\begin{aligned}\n\\xv_n^\\trans =& (\\x_{n2}, \\ldots, \\x_{n(P-1)}, 1)^\\trans \\\\\n\\bv_n =& (1) \\\\\n\\av_n^\\trans =& (\\x_{n2}, \\ldots, \\x_{n(P-1)})^\\trans \\\\\n\\end{aligned}\n\\]\nLet us ask what is the effect on \\(\\betav_a\\) of including \\(\\bv_n\\) as a regressor?\nThe answer is given by the FWL theorem. Recall that\n\\[\n\\resvhat = \\Y - \\X\\betavhat = \\Y - \\X_a \\betavhat_a - \\X_b \\betavhat_b,\n\\]\nand that \\(\\X^\\trans \\resvhat = \\zerov\\), so \\(\\X_a^\\trans \\resvhat = \\zerov\\) and \\(\\X_b^\\trans \\resvhat = \\zerov\\). Recall also the definition of the projection matrix perpendicular to the span of \\(\\X_b\\):\n\\[\n\\proj{\\X_b^\\perp} := \\id{} - \\X_b (\\X_b^\\trans \\X_b)^{-1} \\X_b^\\trans.\n\\]\nApplying \\(\\proj{\\X_b^\\perp}\\) to both sides of \\(\\resvhat = \\Y - \\X\\betavhat\\) gives\n\\[\n\\proj{\\X_b^\\perp} \\resvhat = \\resvhat = \\proj{\\X_b^\\perp} \\Y - \\proj{\\X_b^\\perp} \\X_a \\betavhat_a - \\proj{\\X_b^\\perp} \\X_b \\betavhat_b\n= \\proj{\\X_b^\\perp} \\Y - \\proj{\\X_b^\\perp} \\X_a \\betavhat_a.\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nVerify that \\(\\proj{\\X_b^\\perp} \\resvhat = \\resvhat\\) and \\(\\proj{\\X_b^\\perp} \\X_b \\betavhat_b = \\zerov\\).\n\n\nNow appying \\((\\proj{\\X_b^\\perp} \\X_a)^\\trans\\) to both sides of the preceding expression gives\n\\[\n\\begin{aligned}\n\\X_a^\\trans \\resvhat ={}& \\zerov = \\X_a^\\trans \\proj{\\X_b^\\perp} \\Y - \\X_a^\\trans \\proj{\\X_b^\\perp} \\X_a \\betavhat_a\n  \\quad \\Rightarrow \\\\\n(\\proj{\\X_b^\\perp} \\X_a)^\\trans \\proj{\\X_b^\\perp} \\X_a \\betavhat_a  ={}&  (\\proj{\\X_b^\\perp} \\X_a)^\\trans \\proj{\\X_b^\\perp} \\Y\n\\end{aligned}\n\\]\nIf we assume that \\(\\X\\) is full-rank, then \\(\\proj{\\X_b^\\perp} \\X_a\\) must be full-rank as well, since otherwise one of the columns of \\(\\X_a\\) would be a linear combination of columns of \\(\\X_b\\). Therefore we can invert to get\n\\[\n\\betavhat_a = \\left((\\proj{\\X_b^\\perp} \\X_a)^\\trans \\proj{\\X_b^\\perp} \\X_a \\right)^{-1} (\\proj{\\X_b^\\perp} \\X_a)^\\trans \\proj{\\X_b^\\perp} \\Y.\n\\]\nThis is exactly the same as the linear regression\n\\[\n\\tilde{\\Y} \\sim \\tilde{\\X_a} \\betav_a \\quad\\textrm{where }\n\\tilde{\\X_a} := \\proj{\\X_b^\\perp} \\X_a \\textrm{ and } \\tilde{\\Y} := \\proj{\\X_b^\\perp} \\Y.\n\\]\nThat is, the OLS coefficient on \\(\\X_a\\) is the same as projecting all the responses and regressors to a space orthogonal to \\(\\X_b\\), and running ordinary regression.\nThis means the value of \\(\\betavhat_a\\)\n\nThe special case of a constant regressor\nSuppose we want to regress \\(\\Y \\sim \\beta_0 + \\betav^\\trans \\xv_n\\). We’d like to know what \\(\\betavhat\\) is, and in particular, what is the effect of including a constant.\nWe can answer this with the FWL theorem by taking \\(\\bv_n = (1)\\) and \\(\\av_n = \\xv_n\\). Then \\(\\betavhat\\) will be the same as in the regression\n\\[\n\\tilde{Y} \\sim \\tilde{X} \\betav\n\\]\nwhere \\(\\tilde{Y} = \\proj{\\X_b^\\perp} \\Y\\) and \\(\\tilde{X} = \\proj{\\X_b^\\perp} \\X\\).\nA particular special case is useful for intuition. Take \\(\\xv_b\\) to simply be the constant regressor, \\(1\\). Then \\(\\X_b = \\onev\\), and\n\\[\n\\proj{\\X_b^\\perp} = \\id{} - \\onev (\\onev^\\trans \\onev)^{-1} \\onev^\\trans = \\id{} - \\frac{1}{N} \\onev \\onev^\\trans.\n\\]\n\\(\\onev^\\trans \\onev = \\sumn 1 \\cdot 1 = N\\)\nIf we take\n\\[\n\\begin{aligned}\n\\onev^\\trans \\Y =& \\sumn 1 \\cdot \\y_n = \\sumn \\y_n\\\\\n\\frac{1}{N} \\onev^\\trans \\Y =& \\meann 1 \\cdot \\y_n = \\meann \\y_n = \\ybar\\\\\n\\onev \\frac{1}{N} \\onev^\\trans \\Y =& \\onev \\ybar =\n\\begin{pmatrix}\n\\ybar \\\\\n\\vdots \\\\\n\\ybar\n\\end{pmatrix} \\\\\n\\proj{\\X_b^\\perp}  \\Y =\n\\left(\\id - \\onev \\frac{1}{N} \\onev^\\trans \\right) \\Y =&\n\\Y - \\begin{pmatrix}\n\\ybar \\\\\n\\vdots \\\\\n\\ybar\n\\end{pmatrix}\n= \\begin{pmatrix}\ny_1 - \\ybar \\\\\ny_2 - \\ybar \\\\\n\\vdots \\\\\ny_N - \\ybar\n\\end{pmatrix}\n\\end{aligned}\n\\]\nThe projection matrix \\(\\proj{\\X_b^\\perp}\\) thus simply centers a vector at its sample mean.\nSimilarly,\n\\[\n\\tilde{\\X_a} := \\proj{\\X_b^\\perp} \\X_a = \\X_a - \\frac{1}{N} \\onev \\onev^\\trans \\X_a\n= \\X_a - \\onev \\xbar^\\trans \\\\\n\\textrm{ where } \\xbar^\\trans := \\begin{pmatrix} \\meann \\x_{n1} & \\ldots & \\meann \\x_{n(P-1)} \\end{pmatrix},\n\\]\nso that the \\(n\\)–th row of \\(\\proj{\\X_b^\\perp} \\X_a\\) is simply \\(\\xv_n^\\trans - \\xbar^\\trans\\), and each regressor is centered. So\n\\[\n\\betavhat_a = (\\tilde{\\X_a}^\\trans \\tilde{\\X_a})^\\trans \\tilde{\\X_a}^\\trans \\tilde{\\Y},\n\\]\nthe OLS estimator where both the regressors and responses have been centered at their sample means. In this case, by the LLN,\n\\[\n\\frac{1}{N} \\tilde{\\X_a}^\\trans \\tilde{\\X_a} \\rightarrow \\cov{\\xv_n},\n\\]\nin contrast to the general case, where\n\\[\n\\frac{1}{N} \\X^\\trans \\X \\rightarrow \\expect{\\xv_n \\xv_n^\\trans} = \\cov{\\xv_n} + \\expect{\\xv_n}\\expect{\\xv_n^\\trans}.\n\\]\nFor this reason, when thinking about the sampling behavior of OLS coefficients where a constant is included in the regression, it’s enough to think about the covariance of the regressors, rather than the outer product.\n\n\n\n\n\n\nExercise\n\n\n\nDerive our simple least squares estimator of \\(\\betavhat\\) using the FWL theorem.\n\n\n\n\nCovariances with the FWL theorem\nThough it may not be obvious, estimating standard errors using the residuals from \\(\\tilde{\\Y} \\sim \\tilde{\\X_a}\\betav_a\\) versus \\(\\Y \\sim \\X_a \\betav_a + \\X_b \\betav_b\\) is equivalent, whether the heteroskedastic or sandwich covariance matrices are used. See section 7.3 of Prof. Ding’s book for a proof, which uses the Schur representation of sub-matrices of \\((\\X^\\trans \\X)^{-1}\\).\n\n\n\nThe role of regressor covariance\nFor simplicity, let’s consider a simple regression where \\(\\y_n \\sim \\beta_0 + \\beta_1 \\x_n\\), with \\(\\var{\\x_n} = \\sigma_x^2\\) and \\(\\var{\\res_n} = \\sigma_\\res^2\\). By the FWL theorem, we can estimate\n\\[\n\\betahat_1 = \\frac{\\meann (\\y_n - \\ybar) (\\x_n - \\xbar)}{\\meann (\\x_n - \\xbar)^2}\n\\quad\\textrm{with}\\quad\nN \\cov{\\betahat_1} \\approx\n  \\frac{\\meann (\\y_n  - \\ybar - \\betahat_1 (\\x_n - \\xbar))^2}{\\meann (\\x_n - \\xbar)^2} \\rightarrow\n  \\frac{\\sigma_\\res^2}{\\sigma_x^2}.\n\\]\nThe variance of the (rescaled) regression coefficient thus depends on the ratio of the residual noise to the regressor variance, both after projection."
  },
  {
    "objectID": "lectures/Lecture20.html",
    "href": "lectures/Lecture20.html",
    "title": "Variable selection and the F-test",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\n\\(\\,\\)\n\nGoals\n\nDerive tests for multiple coefficients at once under normality\n\nThe F-test and F-distribution\nAs a special case test whether a group of variables should be included in the model\nIntroduce \\(R^2\\), discuss its limitations, and how the F-test approximately corrects them\n\n\n\n\nTesting multiple coefficients at once\nSo far we have devised tests for one coefficient at a time — or, more precisely, for one linear combination of test statistics at a time. We derived tests under normality, and asserted that they made sense under homoskedasticity for large \\(N\\).\nSuppose we want to test multiple coefficients at a time. A particular use case we will work up to is asking whether our regression did anything at all. In particular, we might ask whether the fit \\(\\yhat = \\X \\betahat\\) is any better than the fit \\(\\yhat = 0\\) that did not run a regression at all.\n(Note that, by the FWL theorem, we can equivalently compare \\(\\yhat = \\X \\betahat\\) with \\(\\yhat = \\ybar\\), simply by centering everything first.)\nFormally, we’ll be testing the null hypothesis that\n\\[\n\\textrm{Null hypothesis }H_0:  \\A \\beta = \\av\n\\]\nwhere \\(A\\) is a \\(Q \\times P\\) matrix, and \\(c\\) is a \\(Q\\)–vector. As a special case, we will test our “do nothing” hypothesis by taking \\(\\A = \\id\\) and \\(\\av=\\zerov\\).\nOur trick will be the same as always: transform into known distributions. Specifically, under \\(H_0\\) and the normal assumptions, we know that\n\\[\n\\A \\betahat \\sim \\gauss{\\A \\beta, \\A (\\X^\\trans \\X)^{-1} \\A^\\trans \\sigma^2} = \\gauss{\\a, \\A (\\X^\\trans \\X)^{-1} \\A^\\trans \\sigma^2}.\n\\]\nIt follows that\n\\[\n\\zv := \\sigma^{-1} \\left(\\A (\\X^\\trans \\X)^{-1} \\A^\\trans  \\right)^{-1/2} \\left(\\A \\betahat - \\av \\right) \\sim \\gauss{\\zerov, \\id},\n\\]\nwhich is a \\(Q\\)–dimensional standard normal. Now, to test \\(H_0\\), we need a single test statistic, and this is a \\(Q\\)–vector. We can get a single number by taking the norm to get a \\(\\chisq{P}\\) distribution:\n\\[\n\\zeta := \\zv^\\trans \\zv =\n  \\sigma^{-2}  \\left(\\A \\betahat - \\av \\right)^\\trans \\left(\\A (\\X^\\trans \\X)^{-1} \\A^\\trans  \\right)^{-1} \\left(\\A \\betahat - \\av \\right)\n  \\sim \\chisq{Q}.\n\\]\nIf \\(\\zeta\\) is large, it’s evidence against \\(H_0\\). However, we still don’t know \\(\\sigma^2\\). So we can do our usual trick of normalizing by \\(\\sigmahat^2\\), which is independent of \\(\\zeta\\) under normality, giving our final computable statistic:\n\\[\n\\phi := \\frac{1}{Q} \\frac{\\left(\\A \\betahat - \\av \\right)^\\trans \\left(\\A (\\X^\\trans \\X)^{-1} \\A^\\trans  \\right)^{-1} \\left(\\A \\betahat - \\av \\right)}{\\sigmahat^2} = \\frac{\\s_1 / P}{\\s_2 / (N-P)} \\sim \\fdist{Q}{N-P}.\n\\]\nwhere \\(\\s_1\\) and \\(\\s_2\\) are independent chi–squared statistics. This is again a standard distribution, with R quantile function qf (etc). Note that values around \\(1\\) are typical, especially for large \\(N-P\\), and that it only makes sense to reject for large values.\n\n\nTotal regression as a special case\nWhat do we get when we apply the F-test to \\(\\beta = \\zerov\\)? The statistic is\n\\[\n\\begin{aligned}\n\\zeta :={}& \\sigma^{-2}  \\betahat^\\trans \\X^\\trans \\X \\betahat \\\\\n\\phi :={}& \\frac{\\frac{1}{P}\\betahat^\\trans \\X^\\trans \\X \\betahat}{\\sigmahat^2} \\sim \\fdist{P}{N-P}.\n\\end{aligned}\n\\]\nThis can be re-written in terms of some interpretable quantities. Let’s define\n\\[\n\\begin{aligned}\nTSS :={}& \\Y^\\trans \\Y =  \\sumn \\y_n^2 \\\\\nRSS :={}& \\resvhat^\\trans \\resvhat =  \\sumn \\reshat_n^2\n= (\\Y - \\X\\betahat)^\\trans (\\Y - \\X\\betahat) =  \\Y^\\trans \\Y - \\Yhat^\\trans \\Yhat.\n\\end{aligned}\n\\]\nA commonly used measure of “fit” is\n\\[\nR^2 = 1 - \\frac{RSS}{TSS}.\n\\]\nOf course, \\(R^2\\) can only decrease as we add regressors.\nOur test statistic behaves better, though.\n\\[\n\\phi = \\frac{N-P}{P} \\frac{\\Yhat^\\trans \\Yhat}{\\resvhat^\\trans \\resvhat}  \n= \\frac{N-P}{P} \\frac{TSS - RSS}{RSS}\n= \\frac{N-P}{P} \\left(\\frac{TSS}{RSS} - 1\\right)\n\\]\nAlthough it does not count as a formal hypothesis test, it makes sense that “good” regressors have large \\(\\phi\\). To make \\(\\phi\\) large, we want to make \\(\\frac{P}{N-P} RSS\\) small, since \\(TSS\\) doesn’t change. As you add regressors, \\(P\\) grows and \\(RSS\\) shrinks. Intuitively, to be included in a regression, you want the \\(RSS\\) to improve more than \\(P\\) increases.\nNext time, we’ll study better ways to make the decision of what regressors to include."
  },
  {
    "objectID": "lectures/Lecture3.html",
    "href": "lectures/Lecture3.html",
    "title": "Multilinear regression as loss minimzation.",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\n\\(\\textcolor{white}{\\LaTeX}\\)\n\nGoals\n\nDerive the general form of the ordinary least squares (OLS) estimator in matrix notation\n\nReview simple least squares derivation\nReview matrix notation\nReview vector calculus\nDerive the general OLS formula and show that the simple least squares is a special case\n\n\n\n\nSiomple least squares\nRecall the simple least squares model:\n\\[\n\\begin{align*}\n\\y_n :={}& \\textrm{Response (e.g. body fat)} \\\\\n\\x_n :={}& \\textrm{Regressor (e.g. waist measurement)}\\\\\ny_n ={}& \\beta_2 \\x_n + \\beta_1 + \\res_n \\textrm{ Model (straight line through data)}.\n\\end{align*}\n\\tag{1}\\]\n\n\n\n\n\n\nNotation\n\n\n\nHere are some key quantities and their names:\n\n\\(\\y_n\\): The ‘response’\n\\(\\x_n\\): The ‘regressors’ or ‘explanatory’ variables\n\nFor a linear model, we also have:\n\n\\(\\res_n\\): The ‘error’ or ‘residual’\n\\(\\beta_2, \\beta_1\\): The ‘coefficients’, ‘parameters’, ‘slope and intercept’\n\nWe might also have estimates of these quantities:\n\n\\(\\betahat_p\\): Estimate of \\(\\beta_p\\)\n\\(\\reshat\\): Estimate of \\(\\res_n\\)\n\\(\\yhat_n\\): A ‘prediction’ or ‘fitted value’ \\(\\yhat_n = \\betahat_1 + \\betahat_2 \\x_n\\)\n\nWhen we form the estimator by minimizing the estimated residuals, we might call the estimate\n\n‘Ordinary least squares’ (or ‘OLS’)\n‘Least-squares’\n‘Linear regression’\n\nAn estimate will implicitly be least-squares estimates, but precisely what we mean by an estimate may have to come from context.\n\n\nNote that for any value of \\(\\beta\\), we get a value of the “error” or “residual” \\(\\res_n\\):\n\\[\n\\res_n = \\y_n - (\\beta_2 \\x_n + \\beta_1).\n\\]\nThe “least squares fit” is called this because we choose \\(\\beta_1\\) and \\(\\beta_2\\) to make \\(\\sumn \\res_n^2\\) as small as possible:\n\\[\n\\begin{align*}\n\\textrm{Choose }\\betahat_2,\\betahat_1\\textrm{ so that }\n\\sumn \\res_n^2 = \\sumn \\left(  \\y_n - (\\betahat_2 \\x_n + \\betahat_1) \\right)^2\n\\textrm{ is as small as possible.}\n\\end{align*}\n\\]\nHow do we do this for the simple least squares model? And what if we have more regressors?\n\n\nSiomple least squares estimator derivation\nThe quantity we’re trying to minimize is smooth and convex, so if there is a minimum it would satisfy\n\\[\n\\begin{align*}\n\\fracat{\\partial \\sumn \\res_n^2}{\\partial \\beta_1}{\\betahat_1, \\betahat_2} ={}& 0 \\quad\\textrm{and} \\\\\n\\fracat{\\partial \\sumn \\res_n^2}{\\partial \\beta_2}{\\betahat_1, \\betahat_2} ={}& 0.\n\\end{align*}\n\\]\n\n\n\n\n\n\nQuestion\n\n\n\nWhen is it sufficient to set the gradient equal to zero to find a minumum?\n\n\nThese translate to (after dividing by \\(-2 N\\))\n\\[\n\\begin{align*}\n\\meann \\y_n - \\betahat_2 \\meann \\x_n - \\betahat_1 ={}& 0 \\quad\\textrm{and}\\\\\n\\meann \\y_n \\x_n - \\betahat_2 \\meann \\x_n^2 - \\betahat_1 \\meann \\x_n  ={}& 0.\n\\end{align*}\n\\]\nLet’s introduce the notation\n\\[\n\\begin{align*}\n\\overline{y} ={}& \\meann \\y_n \\\\\n\\overline{xy} ={}& \\meann \\x_n \\y_n \\\\\n\\overline{xx} ={}& \\meann \\x_n ^2,\n\\end{align*}\n\\]\nOur estimator them must satisfy\n\\[\n\\begin{align*}\n\\overline{x} \\betahat_2  + \\betahat_1 ={}& \\overline{y} \\quad\\textrm{and}\\\\\n\\overline{xx} \\betahat_2  + \\overline{x} \\betahat_1  ={}& \\overline{yx}.\n\\end{align*}\n\\]\nWe have a linear system with two unknowns and two equations. An elegant way to solve them is to subtract \\(\\overline{x}\\) times the first equation from the second, giving:\n\\[\n\\begin{align*}\n\\overline{x} \\betahat_1 - \\overline{x} \\betahat_1 +\n    \\overline{xx} \\betahat_2 - \\overline{x}^2 \\betahat_2 ={}&\n    \\overline{xy} - \\overline{x} \\overline{y} \\Leftrightarrow\\\\\n\\betahat_2 ={}&\n    \\frac{\\overline{xy} - \\overline{x} \\overline{y}}\n         {\\overline{xx}  - \\overline{x}^2},\n\\end{align*}\n\\]\nas long as \\(\\overline{xx} - \\overline{x}^2 \\ne 0\\).\n\n\n\n\n\n\nQuestion\n\n\n\nIn ordinary language, what does it mean for \\(\\overline{xx} - \\overline{x}^2 = 0\\)?\n\n\nWe can then plug this into the first equation giving\n\\[\n\\betahat_1 = \\overline{y} - \\betahat_2 \\overline{x}.\n\\]\n\n\nMatrix multiplication version\nAlternatively, our criterion can be written in matrix form as\n\\[\n\\begin{pmatrix}\n1  & \\overline{x} \\\\\n\\overline{x} & \\overline{xx}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\betahat_1 \\\\\n\\betahat_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\overline{y} \\\\\n\\overline{xy}\n\\end{pmatrix}\n\\tag{2}\\]\nRecall that there is a special matrix that allows us to get an expression for \\(\\betahat_1\\) and \\(\\betahat_2\\):\n\\[\n\\begin{align*}\n\\begin{pmatrix}\n1  & \\overline{x} \\\\\n\\overline{x} & \\overline{xx}\n\\end{pmatrix}^{-1} =\n\\frac{1}{\\overline{xx} - \\overline{x}^2}\n\\begin{pmatrix}\n\\overline{xx}  & - \\overline{x} \\\\\n-\\overline{x} & 1\n\\end{pmatrix}\n\\end{align*}\n\\]\nThis matrix is called the “inverse” because\n\\[\n\\begin{align*}\n\\begin{pmatrix}\n1  & \\overline{x} \\\\\n\\overline{x} & \\overline{xx}\n\\end{pmatrix}^{-1}\n\\begin{pmatrix}\n1  & \\overline{x} \\\\\n\\overline{x} & \\overline{xx}\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}.\n\\end{align*}\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nVerify the preceding property.\n\n\nMultiplying both sides of Equation 2 by the matrix inverse gives\n\\[\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\betahat_1 \\\\\n\\betahat_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\betahat_1 \\\\\n\\betahat_2\n\\end{pmatrix} =\n\\frac{1}{\\overline{xx} - \\overline{x}^2}\n\\begin{pmatrix}\n\\overline{xx}  & - \\overline{x} \\\\\n-\\overline{x} & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\overline{y} \\\\\n\\overline{xy}\n\\end{pmatrix}.\n\\]\nFrom this we can read off the familiar answer\n\\[\n\\begin{align*}\n\\betahat_2 ={}& \\frac{\\overline{xy} - \\overline{x}\\,\\overline{y}}{\\overline{xx} - \\overline{x}^2}\\\\\n\\betahat_1 ={}& \\frac{\\overline{xx}\\,\\overline{y} - \\overline{xy}\\,\\overline{x}}{\\overline{xx} - \\overline{x}^2}\\\\\n  ={}& \\frac{\\overline{xx}\\,\\overline{y} -\n      \\overline{x}^2 \\overline{y} + \\overline{x}^2 \\overline{y} - \\overline{xy}\\,\\overline{x}}\n    {\\overline{xx} - \\overline{x}^2}\\\\\n  ={}& \\overline{y} - \\frac{\\overline{x}^2 \\overline{y} - \\overline{xy}\\,\\overline{x}}\n    {\\overline{xx} - \\overline{x}^2} \\\\\n  ={}& \\overline{y} - \\betahat_1 \\overline{x}.\n\\end{align*}\n\\]\n\n\nMatrix notation\nThe preceding formula came from combining the equations that set the univariate gradients equal to zero, and then recognizing a matrix equation. We can in fact do both at the same time! But first we need some notation\nHere is a formal definition of the type of model that we will study for the vast majority of the semester:\n\\[\n\\begin{align*}\n\\y_n ={}& \\beta_1 \\x_{n1} + \\beta_2 \\x_{n2} + \\ldots + \\x_{nP} + \\res_{n}, \\quad\\textrm{For }n=1,\\ldots,N.\n\\end{align*}\n\\tag{3}\\]\n\n\n\n\n\n\nNotation\n\n\n\nI will always use \\(N\\) for the number of observed data points, and \\(P\\) for the dimension of the regression vector.\n\n\nEquation 3 is a general form of simpler cases. For example, if we take \\(\\x_{n1} \\equiv 1\\), \\(\\x_{n2}= \\x_n\\) to be some scalar, and \\(P = 2\\), then Equation 3 becomes Equation 1:\n\\[\n\\begin{align*}\n\\y_n ={}& \\beta_1  + \\beta_2 \\x_{n} + \\res_{n}, \\quad\\textrm{For }n=1,\\ldots,N.\n\\end{align*}\n\\]\nThe residuals \\(\\res_n\\) measure the “misfit” of the line. If you know \\(\\beta_1, \\ldots, \\beta_P\\), then you can compute\n\\[\n\\begin{align*}\n\\res_n ={}& \\y_n -  (\\beta_1 \\x_{n1} + \\beta_2 \\x_{n2} + \\ldots + \\x_{nP}).\n\\end{align*}\n\\]\nBut in general we only observe \\(\\y_n\\) and \\(\\x_{n1}, \\ldots, \\x_{nP}\\), and we choose \\(\\beta_1, \\ldots, \\beta_P\\) to make the residuals small. (How we do this precisely will be something we talk about at great length.)\nThe general form of Equation 3 can be written more compactly using matrix and vector notation. Specifically, if we let\n\\[\n\\begin{align*}\n\\xv_n :=\n\\begin{pmatrix}\n  \\x_{n1} \\\\ \\x_{n2} \\\\ \\vdots \\\\ \\x_{nP}\n\\end{pmatrix}\n\\quad\n\\textrm{and}\n\\quad\n\\bv :=\n\\begin{pmatrix}\n  \\beta_{1} \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_{P}\n\\end{pmatrix}\n\\end{align*}\n\\]\n\n\n\n\n\n\nNotation\n\n\n\nBold lowercase variables are column vectors (unless otherwise specified).\n\n\nRecall that the “transpose” operator \\((\\cdot)^\\trans\\) flips the row and columns of a matrix. For example,\n\\[\n\\begin{align*}\n\\xv_n ^\\trans =\n\\begin{pmatrix}\n  \\x_{n1} & \\x_{n2} & \\ldots & \\x_{nP}\n\\end{pmatrix}.\n\\end{align*}\n\\]\nBy matrix multiplication rules,\n\\[\n\\begin{align*}\n\\xv_n^\\trans \\bv =\n\\begin{pmatrix}\n  \\x_{n1} & \\x_{n2} & \\ldots & \\x_{nP}\n\\end{pmatrix}\n\\quad\\quad\\quad\n\\begin{pmatrix}\n  \\beta_{1} \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_{P}\n\\end{pmatrix}\n= \\beta_1 \\x_{n1} + \\beta_2 \\x_{n2} + \\ldots + \\x_{nP}.\n\\end{align*}\n\\]\n\n\n\n\n\n\nNotation\n\n\n\nI have written \\(\\xv_n^\\trans \\bv\\) for the “dot product” or “inner product” between \\(\\xv_n\\) and \\(\\bv\\). Writing it in this way clarifies the relationship with matrix notation below.\nThere are many other ways to denote inner products in the literature, including \\(\\xv_n \\cdot \\bv\\) and \\(&lt;\\xv_n, \\bv&gt;\\).\n\n\nThen we can compactly write\n\\[\n\\begin{align*}\n\\y_n ={}& \\xv_n ^\\trans \\bv + \\res_{n}, \\quad\\textrm{For }n=1,\\ldots,N.\n\\end{align*}\n\\]\nWe can compactify it even further if we stack the \\(n\\) observations: % \\[\n\\begin{align*}\n\\y_1 ={}& \\xv_1 ^\\trans \\bv + \\res_{1} \\\\\n\\y_2 ={}& \\xv_2 ^\\trans \\bv + \\res_{2} \\\\\n\\vdots\\\\\n\\y_N ={}& \\xv_N ^\\trans \\bv + \\res_{N} \\\\\n\\end{align*}\n\\]\nAs before we can stack the responses and residuals:\n\\[\n\\begin{align*}\n\\Y :=\n\\begin{pmatrix}\n  \\y_{1} \\\\ \\y_2 \\\\ \\vdots \\\\ \\y_{P}\n\\end{pmatrix}\n\\quad\n\\textrm{and}\n\\quad\n\\resv :=\n\\begin{pmatrix}\n  \\res_{1} \\\\ \\res_2 \\\\ \\vdots \\\\ \\res_{P}\n\\end{pmatrix}\n\\end{align*}\n\\]\nWe can also stack the regressors:\n\\[\n\\begin{align*}\n\\X :=\n\\begin{pmatrix}\n  \\x_{11} & \\x_{12} & \\ldots & \\x_{1P}\\\\\n  \\x_{21} & \\x_{22} & \\ldots & \\x_{2P}\\\\\n  \\vdots\\\\\n  \\x_{n1} & \\x_{n2} & \\ldots & \\x_{nP}\\\\\n  \\vdots\\\\\n    \\x_{N1} & \\x_{N2} & \\ldots & \\x_{NP}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\xv_{1}^\\trans \\\\ \\xv_{2}^\\trans \\\\ \\vdots \\\\ \\xv_n^\\trans \\\\ \\vdots \\\\ \\xv_{N}^\\trans\n\\end{pmatrix}\n\\end{align*}\n\\]\n\n\n\n\n\n\nNotation\n\n\n\nI will use upper case bold letters for multi-dimensional matrices like \\(\\X\\). But I may also use upper case bold letters even when the quantity could also be a column vector, when I think it’s more useful to think of the quantity as a matrix with a single column. Examples are \\(\\Y\\) above, or \\(\\X\\) when \\(P = 1\\).\n\n\nNote that by matrix multiplication rules,\n\\[\n\\begin{align*}\n\\X  \\bv =\n\\begin{pmatrix}\n  \\xv_{1}^\\trans \\\\ \\xv_{2}^\\trans \\\\ \\vdots \\\\ \\xv_n^\\trans \\\\ \\vdots \\\\ \\xv_{N}^\\trans\n\\end{pmatrix}\n\\quad\\quad\\quad\n\\bv\n=\n\\begin{pmatrix}\n  \\xv_{1}^\\trans\\bv \\\\ \\xv_{2}^\\trans\\bv \\\\ \\vdots \\\\ \\xv_n^\\trans\\bv \\\\ \\vdots \\\\ \\xv_{N}^\\trans\\bv\n\\end{pmatrix}\n\\end{align*}\n\\]\nso we end up with the extremely tidy expression\n\\[\n\\begin{align*}\n\\y_n ={}& \\beta_1 \\x_{n1} + \\beta_2 \\x_{n2} + \\ldots + \\x_{nP} + \\res_{n}, \\quad\\textrm{For }n=1,\\ldots,N\n\\\\\\\\&\\textrm{is the same as}\\quad\\\\\\\\\n\\Y ={}& \\X \\bv + \\resv.\n\\end{align*}\n\\tag{4}\\]\nIn the case of simple least squares, we can write\n\\[\n\\begin{align*}\n\\X :=\n\\begin{pmatrix}\n  1 & \\x_{1}\\\\\n  1 & \\x_{2}\\\\\n  \\vdots & \\vdots\\\\\n  1 & \\x_{N}\\\\\n\\end{pmatrix},\n\\end{align*}\n\\tag{5}\\]\nand verify that the \\(n\\)–th row of Equation 4 is the same as Equation 1.\n\n\nLeast squares in matrix notation\nUsing our tidy expression Equation 4, we can easily write out the sum of the squared errors as\n\\[\n\\begin{align*}\n\\sumn \\res_n^2 =\n    \\resv^\\trans \\resv = (\\Y - \\X \\bv)^\\trans (\\Y - \\X \\bv).\n\\end{align*}\n\\]\nThis is a function of the vector \\(\\bv\\). We wish to find the minimum of this quantity as a function of \\(\\bv\\). My might hope that the minimum occurs at a point where the gradient of this expression is zero. Rather than compute the univariate derivative with respect to each component, we can compute the multivariate gradient with respect to the vector.\nLet’s recall some facts from vector calculus.\n\n\n\n\n\n\nNotation\n\n\n\nTake \\(\\z \\in \\mathbb{R}^P\\) to be a \\(P\\)–vector. and let \\(\\mybold{f}(\\z) \\in \\mathbb{R}^Q\\) denote a \\(Q\\)–vector. We write\n\\[\n\\frac{\\partial \\mybold{f}}{\\partial \\zv^\\trans}  =\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial \\z_1} f_1(\\z) & \\ldots &\n    \\frac{\\partial}{\\partial \\z_P} f_1(\\zv) \\\\\n    & \\vdots & \\\\\n\\frac{\\partial}{\\partial \\z_1} f_Q(\\zv) & \\ldots &\n    \\frac{\\partial}{\\partial \\z_P} f_Q(\\zv)\n\\end{pmatrix}.\n\\]\nThat is, the partial \\(\\partial \\mybold{f} / \\partial \\zv^\\trans\\) is a \\(Q \\times P\\) matrix with components of \\(\\mybold{f}\\) in the rows and components of the derivative in the columns. This matrix is called the “Jacobian matrix” of the function \\(\\mybold{f}(\\zv)\\).\nNote that many authors omit the transpose in the denominator of the partial derivative, but I will try to do so to emphasize the dimension of the output.\nI will also sometimes write\n\\[\n\\frac{\\partial \\mybold{f}^\\trans}{\\partial \\zv}\n=\n\\left(\n    \\frac{\\partial \\mybold{f}}{\\partial \\zv^\\trans}\n\\right)^\\trans.\n\\]\nWhen \\(Q = 1\\) and \\(f\\) is a scalar, I will often write\n\\[\n\\frac{\\partial f}{\\partial \\zv} =\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial \\zv_1} f(\\zv) \\\\\n    \\vdots  \\\\\n    \\frac{\\partial}{\\partial \\zv_P} f(\\zv)\n\\end{pmatrix}.\n\\]\n\n\nRecall a couple rules from vector calculus:\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\zv} \\zv^\\trans \\zv = 2 \\zv\n\\quad\\textrm{and}\\quad\n\\frac{\\partial}{\\partial \\zv^\\trans} \\mybold{A} \\zv = \\mybold{A}\n\\quad\\textrm{and}\\quad\n\\frac{\\partial}{\\partial \\zv} f(\\mybold{g}(\\zv)) =\n    \\frac{\\partial \\mybold{g}^\\trans}{\\partial \\zv} \\frac{\\partial f}{\\partial \\mybold{g}}\n     \\quad\n    \\textrm{($f$ is scalar-valued and $\\mybold{g}$ is vector-valued)}\n\\end{align*}\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nProve these results above using univariate derivatives and our stacking convention.\n\n\nBy the chain rule, we then get\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\bv} \\resv^\\trans \\resv ={}&\n2 \\frac{\\partial \\resv^\\trans}{\\partial \\bv}  \\resv  \\\\\n={}& 2 \\frac{\\partial (\\Y - \\X \\bv)^\\trans}{\\partial \\bv}  (\\Y - \\X \\bv)  \\\\\n={}& -2 \\X ^\\trans (\\Y - \\X \\bv) \\\\\n={}& -2 \\X^\\trans \\Y + 2  \\X^\\trans \\X \\bv.\n\\end{align*}\n\\]\nAssuming our estimator \\(\\betahat\\) sets these partial derivatives are equal to zero, we then get\n\\[\n\\begin{align*}\n\\X^\\trans \\X \\bvhat ={}& \\X^\\trans \\Y.\n\\end{align*}\n\\tag{6}\\]\nThis is a set of \\(P\\) equations in \\(P\\) unknowns. If it is not degenerate, one can solve for \\(\\bvhat\\). That is, if the matrix \\(\\X^\\trans \\X\\) is invertible, then we can multiply both sides of Equation 6 by \\((\\X^\\trans \\X)^{-1}\\) to get\n\\[\n\\bvhat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\n\\]\n\n\n\n\n\n\nNotation\n\n\n\nI will use \\(\\onev\\) to denote a vector full of ones. Usually it will be a \\(P\\)–vector, but sometimes its dimension will just be implicit. Similarly, \\(\\zerov\\) is a vector of zeros.\n\n\nIndeed, by plugging Equation 5 into @ols-est-eq, we get\n\\[\n\\X^\\trans \\X =\n\\begin{pmatrix}\n1 & \\ldots & 1 \\\\\nx_1 & \\ldots & x_N \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n  1 & \\x_{1}\\\\\n  \\vdots & \\vdots\\\\\n  1 & \\x_{N}\\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\onev^\\trans \\onev  & \\onev^\\trans \\xv \\\\\n\\xv^\\trans \\onev  & \\xv^\\trans \\xv \\\\\n\\end{pmatrix}\n=\nN\n\\begin{pmatrix}\n1  & \\overline{x} \\\\\n\\overline{x}  & \\overline{xx} \\\\\n\\end{pmatrix} \\\\\n%\n\\]\nand\n\\[\n\\X^\\trans \\Y =\n\\begin{pmatrix}\n1 & \\ldots & 1 \\\\\nx_1 & \\ldots & x_N \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n  \\y_{1}\\\\\n  \\vdots\\\\\n  \\y_{N}\\\\\n\\end{pmatrix} =\nN\n\\begin{pmatrix}\n\\overline{y} \\\\\n\\overline{xy}\n\\end{pmatrix}.\n\\]\nCanceling \\(N\\) shows that Equation 6 is the same as Equation 2.\n\n\nWhat if the matrix is not invertible?\nI’ll end with a short note on what happens with simmple linear regression if \\(\\overline{xx} - \\overline{x}^2 = 0\\). Recall that if \\(\\overline{xx} - \\overline{x}^2 = 0\\) then the matrix in Equation 6 is not invertible.\n\n\n\n\n\n\nExercise\n\n\n\nProve that \\(\\overline{xx} - \\overline{x}^2 = 0\\) means \\(\\x_n\\) is a constant. Hint: look at the sample variance of \\(\\x_n\\).\n\n\nFor simplicity, let’s take \\(\\x_n = 1\\). In that case we can rewrite our estimating equation as\n\\[\n\\y_n = \\beta_1 + \\beta_2 \\x_n + \\res_n\n     = (\\beta_1 + \\beta_2) + \\res_n.\n\\]\nNote that there are an infinite number of combinations \\(\\beta_1\\) and \\(\\beta_2\\) that give exactly the same regression line. For example, if I take\n\\[\n\\beta_1' = \\beta_1 - 5\n\\quad\\textrm{and}\\quad\n\\beta_2' = \\beta_2 + 5\n\\]\nthen\n\\[\n\\y_n = (\\beta_1 + \\beta_2) + \\res_n\n     = (\\beta_1' + 5 + \\beta_2' - 5) + \\res_n\n     = (\\beta_1' + \\beta_2') + \\res_n,\n\\]\neven though \\(\\beta_1 \\ne \\beta_1'\\) and \\(\\beta_2 \\ne \\beta_2'\\). We cannot possibly hope to find a unique least squares solution — even though the expressivity of the line we are able to fit is unchanged.\nIn that case,\n\\[\n\\begin{align*}\n\\overline{x} ={}& \\meann 1 = 1 \\\\\n\\overline{xx} ={}& \\meann 1^2 = 1 \\\\\n\\overline{xy} ={}& \\meann 1 \\y_n = \\overline{y},\n\\end{align*}\n\\]\nand we see that our system of equations is\n\\[\n\\begin{pmatrix}\n1  & \\overline{x} \\\\\n\\overline{x} & \\overline{xx}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_1 \\\\ \\beta_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  & 1 \\\\\n1 &  1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_1 \\\\ \\beta_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\overline{y} \\\\ \\overline{y}\n\\end{pmatrix}\n\\]\nThe degeneracy we noticed before manifests in linear algebra form by noticing that\n\\[\n\\begin{pmatrix}\n1  & 1 \\\\\n1 &  1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\ -1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  -1 \\\\ 1 - 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\ 0\n\\end{pmatrix}\n\\]\nso that\n\\[\n\\begin{pmatrix}\n1  & 1 \\\\\n1  & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_1 + 5\\\\ \\beta_2 - 5\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  & 1 \\\\\n1  & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_1\\\\ \\beta_2\n\\end{pmatrix}\n+ 5\n\\begin{pmatrix}\n1  & 1 \\\\\n1  & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n1\\\\ -1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  & 1 \\\\\n1  & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_1\\\\ \\beta_2\n\\end{pmatrix}.\n\\]\nIn other words, the fact that the matrix \\(\\X^\\trans \\X\\) maps a non-zero vector to zero results in the non-existence of the OLS solution. This will turn out to be very general, and only one of the ways in which the structure of \\(\\X^\\trans \\X\\) will reveal a lot about the behavior of the OLS estimate and fit."
  },
  {
    "objectID": "lectures/Lecture6.html",
    "href": "lectures/Lecture6.html",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "",
    "text": "\\(\\LaTeX\\)"
  },
  {
    "objectID": "lectures/Lecture6.html#linear-combinations-of-variables-in-simple-least-squares",
    "href": "lectures/Lecture6.html#linear-combinations-of-variables-in-simple-least-squares",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "Linear combinations of variables in simple least squares",
    "text": "Linear combinations of variables in simple least squares\nTo answer question 1, let’s look at a slightly simpler example, where we perform simple least squares on heigh.\n\nreg_height &lt;- lm(bodyfat ~ Height, bodyfat_df)\nprint(reg_height$coefficients)\n\n(Intercept)      Height \n 33.4944938  -0.2044753 \n\nprint(sprintf(\"Error: %f\", mean(reg_height$residuals^2)))\n\n[1] \"Error: 69.199176\"\n\nreg_height_norm &lt;- lm(bodyfat ~ height_norm, bodyfat_df)\nprint(reg_height_norm$coefficients)\n\n(Intercept) height_norm \n 19.1507937  -0.7489636 \n\nprint(sprintf(\"Error: %f\", mean(reg_height_norm$residuals^2)))\n\n[1] \"Error: 69.199176\"\n\n\nWhy do these two have different coefficients, but the same fit? The answer comes from our projection result. Let’s let \\(\\x_n = \\textrm{Height}_n\\), and \\(\\z_n = \\textrm{height\\_norm}_n\\). Recall that R includes a constant in the regression by default, so our two regressions are:\n\\[\n\\begin{aligned}\n\\textrm{height: } && \\textrm{height\\_norm} \\\\\n\\y_n = \\beta_0 + \\beta_1 \\x_n + \\res_n &&\n\\y_n = \\gamma_0 + \\gamma_1 \\z_n + \\nu_n.\n\\end{aligned}\n\\]\nWhat’s the relationship between these two regressions? Well, taking \\(\\overline{x} := \\meann \\x_n\\) and \\(\\sigmahat_x := \\sqrt{\\meann (\\x_n - \\overline{x})^2}\\). In our case, \\(\\sigmahat_x &gt; 0\\), so we’ve set\n\\[\n\\z_n = \\frac{\\x_n - \\overline{x}}{\\sigmahat_x} =\n  \\frac{1}{\\sigmahat_x} \\x_n - \\frac{\\overline{x}}{\\sigmahat_x}.\n\\tag{1}\\]\nThat means we can write\n\\[\n\\begin{aligned}\n\\gamma_0 + \\gamma_1 \\z_n + \\nu_n ={}&\n\\gamma_0 + \\gamma_1 \\left( \\frac{1}{\\sigmahat_x} \\x_n -\n  \\frac{\\overline{x}}{\\sigmahat_x} \\right) + \\nu_n  \\\\\n={}&\n\\left(\\gamma_0 - \\frac{\\overline{x}}{\\sigmahat_x} \\gamma_1 \\right)  +\n   \\left( \\frac{\\gamma_1}{\\sigmahat_x}\\right) \\x_n  + \\nu_n.\n\\end{aligned}\n\\]\nBy identifying\n\\[\n\\gamma_0 - \\frac{\\overline{x}}{\\sigmahat_x} \\gamma_1 = \\beta_0 \\quad\\textrm{and}\\quad\n\\frac{\\gamma_1}{\\sigmahat_x} = \\beta_1,\n\\]\nwe see that the two regressions are exactly the same! From this two conclusions follow:\n\nIt is impossible for you to get a better fit with one than the other.\nIn general the coefficients will be different.\n\nIn fact, we can write down the rule to convert between the two. Let’s check that it works:\n\nxbar &lt;- mean(bodyfat_df$Height)\nsigmahat &lt;- sd(bodyfat_df$Height)\ngammahat_0 &lt;- reg_height_norm$coefficients[\"(Intercept)\"]\ngammahat_1 &lt;- reg_height_norm$coefficients[\"height_norm\"]\nbetahat_0 &lt;- reg_height$coefficients[\"(Intercept)\"]\nbetahat_1 &lt;- reg_height$coefficients[\"Height\"]\n\nprint(\"Intercept: \")\n\n[1] \"Intercept: \"\n\ncat(betahat_0, \" = \", gammahat_0 - gammahat_1 * xbar / sigmahat, \"\\n\")\n\n33.49449  =  33.49449 \n\ncat(betahat_1, \" = \", gammahat_1  / sigmahat, \"\\n\")\n\n-0.2044753  =  -0.2044753"
  },
  {
    "objectID": "lectures/Lecture6.html#linear-combinations-of-variables-in-matrix-form",
    "href": "lectures/Lecture6.html#linear-combinations-of-variables-in-matrix-form",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "Linear combinations of variables in matrix form",
    "text": "Linear combinations of variables in matrix form\nTo extend what we just did, let’s put our result in matrix notation. First, write\n\\[\n\\xv_n = \\begin{pmatrix}\n1 \\\\ \\x_n\n\\end{pmatrix}\n\\quad\\textrm{and}\\quad\n\\zv_n = \\begin{pmatrix}\n1 \\\\ \\z_n\n\\end{pmatrix}.\n\\]\nIn this notation, we can write Equation 1 as\n\\[\n\\zv_n =\n\\begin{pmatrix}\n1 & 0 \\\\\n-\\frac{\\overline{x}}{\\sigmahat_x} & \\frac{1}{\\sigmahat_x}\n\\end{pmatrix}\n\\xv_n\n=: \\A \\xv_n.\n\\]\nWe can see that \\(\\A\\) is invertible whenever \\(\\sigmahat_x &gt; 0\\). Using the matrix \\(\\A\\), we can write \\(\\Z = \\X \\A^\\trans\\), so\n\\[\n\\Y = \\Z \\gamma + \\etav =\n\\X \\A^\\trans \\gamma + \\etav =\n\\X  \\beta + \\resv,\n\\]\nwhich gives\n\\[\n\\etav = \\resv\n\\quad\\Leftrightarrow \\quad\n\\A^\\trans \\gamma =  \\beta\n\\quad\\Leftrightarrow \\quad\n\\gamma = (\\A^\\trans)^{-1} \\beta\n.\n\\]\nWe can confirm that this condition does indeed hold at the optimum of each regression:\n\na_mat = matrix(NA, 2, 2)\na_mat[1,1] &lt;- 1\na_mat[1,2] &lt;- 0\na_mat[2,1] &lt;- -xbar / sigmahat\na_mat[2,2] &lt;- 1 / sigmahat\nprint(t(a_mat) %*% reg_height_norm$coefficients %&gt;% as.numeric())\n\n[1] 33.4944938 -0.2044753\n\nprint(reg_height$coefficients)\n\n(Intercept)      Height \n 33.4944938  -0.2044753 \n\n\nNote that we were able to do this only because we included an intercept in the regression! In fact, we can show that we get different fits if we don’t include the intercept:\n\nreg_height_noint &lt;- lm(bodyfat ~ Height - 1, bodyfat_df)\nprint(sprintf(\"Error: %f\", mean(reg_height_noint$residuals^2)))\n\n[1] \"Error: 72.237550\"\n\nreg_height_norm_noint &lt;- lm(bodyfat ~ height_norm - 1, bodyfat_df)\nprint(sprintf(\"Error: %f\", mean(reg_height_norm_noint$residuals^2)))\n\n[1] \"Error: 435.952073\""
  },
  {
    "objectID": "lectures/Lecture6.html#linear-combinations-of-variables-in-general",
    "href": "lectures/Lecture6.html#linear-combinations-of-variables-in-general",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "Linear combinations of variables in general",
    "text": "Linear combinations of variables in general\nNote that the argument which we made in the special case above actually holds in general. Let’s go to \\(P\\)–dimensional regression, but otherwise retaining the notation \\(\\y_n \\sim \\z_n^\\trans\\gamma\\) and \\(\\y_n \\sim \\x_n^\\trans \\beta\\). If we can find an invertible \\(\\A\\) such that \\(\\z_n = \\A \\x_n\\), then we still have\n\\[\n\\Y = \\Z \\gamma + \\etav =\n\\X \\A^\\trans \\gamma + \\etav =\n\\X  \\beta + \\resv,\n\\]\n\\[\n\\etav = \\resv\n\\quad\\Leftrightarrow \\quad\n\\A^\\trans \\gamma =  \\beta\n\\quad\\Leftrightarrow \\quad\n\\gamma = (\\A^\\trans)^{-1} \\beta\n.\n\\]\nHow can we recognize a linear transformation from \\(\\x_n\\) to \\(\\z_n\\)?\nIt’s enough for each entry of \\(\\z_n\\) to be a linear transform \\(\\z_{np}= \\a_p^\\trans \\x_n\\), since we can then just stack the \\(\\a_p^\\trans\\) vectors to form \\(\\A\\).\n\nLinear transformations of \\(\\x_n\\) always look like \\(\\z_{np} = \\a_{p1} \\x_{n1} + \\ldots \\a_{pP} \\x_{nP}\\).\nEntries that are unchanged are always the (trivial) identity linear transformation \\(\\a_{p} = (0, 0, \\ldots, 0, 1,  0, \\ldots, 0)\\)\n\nNote that adding a constant is an affine, not a linear transformation. However, it can be a linear transformation if your model includes an intercept — or if some sum of the entries of \\(\\x_n\\) is always a constant.\nNot that it is not always easy to see whether a linear transformation is invertible! We will talk about that problem shortly.\nIn this way, one can see that the normalization of Question 1 is a linear transformation."
  },
  {
    "objectID": "lectures/Lecture6.html#linear-transformations-and-column-spans",
    "href": "lectures/Lecture6.html#linear-transformations-and-column-spans",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "Linear transformations and column spans",
    "text": "Linear transformations and column spans\nFinally, recall our projection result that states\n\\[\n\\Yhat = \\proj{\\S_\\X} \\Y = \\X (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y,\n\\]\nwhere \\(\\S_\\X\\) is the space of linear combinations of the columns of \\(\\X\\). If \\(\\A\\) is invertible, then the column span of \\(\\Z = \\X \\A^\\trans\\) is equal to the column span of \\(\\X\\). Consequently, a regression on an invertible linear combination of regressors cannot affect the fit.\nAlthough this intuition is clear enough if you’re comfortable with linear algebra, you can also check directly that\n\\[\n\\proj{\\S_\\Z} \\Y =\n\\Z (\\Z^\\trans \\Z)^{-1} \\Z^\\trans \\Y =\n\\X \\A^\\trans (\\A \\X^\\trans \\X \\A^\\trans)^{-1} \\A \\X^\\trans \\Y =\n\\X \\A^\\trans (\\A^\\trans)^{-1} (\\X^\\trans \\X)^{-1} \\A^{-1} \\A \\X^\\trans \\Y =\n\\X (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y =\n\\proj{\\S_\\X} \\Y.\n\\]"
  },
  {
    "objectID": "lectures/Lecture6.html#redundant-variables-in-simple-regression",
    "href": "lectures/Lecture6.html#redundant-variables-in-simple-regression",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "Redundant variables in simple regression",
    "text": "Redundant variables in simple regression\nOnce again, it will be useful to start with a simpler example. Suppose you have a regression \\(\\y_n \\sim \\xv_n^\\trans \\beta\\), with \\(\\xv_n = (1, \\x_n)^\\trans\\), where \\(\\sigmahat_x &gt; 0\\) (as defined above). Now suppose you want to run a new regression \\(\\y_n \\sim \\z_n^\\trans \\gamma\\) with\n\\[\n\\zv_n =\n\\begin{pmatrix}\n1 \\\\\n\\x_n \\\\\n\\x_n - 1\n\\end{pmatrix}.\n\\]\nNote that the new regression is the same as\n\\[\n\\begin{aligned}\n\\y_n ={}& \\gamma_1 + \\gamma_2 \\x_n + \\gamma_3 (\\x_n - 1) + \\eta_n \\\\\n={}& (\\gamma_1 - \\gamma_3) + (\\gamma_2 + \\gamma_3) \\x_n +  \\eta_n \\\\\n={}& \\beta_1 + \\beta_2 \\x_n +  \\res_n.\n\\end{aligned}\n\\]\nWe achieve the same fit — \\(\\res_n = \\eta_n\\) for all \\(n\\) — whenever\n\\[\n\\gamma_1 - \\gamma_3 = \\beta_1\n\\quad\\textrm{and}\\quad\n\\gamma_2 + \\gamma_3 = \\beta_2.\n\\]\nBy the arguments above, the two regressions must have the same optimal fit. But in this case, there are actually a whole infinite dimensional set of \\(\\gammav\\) that correspond to each \\(\\bv\\). In particular, for any \\(a\\), we can take\n\\[\n\\begin{aligned}\n\\gamma_1 &\\rightarrow \\gamma_1 + a \\\\\n\\gamma_2 &\\rightarrow \\gamma_2 - a \\\\\n\\gamma_3 &\\rightarrow \\gamma_3 + a,\n\\end{aligned}\n\\]\nand achieve exactly the same fit. In other words, the optimal fit \\(\\Yhat\\) is the same for the two regressions, but the optimal parameter\n\\[\n\\gammahat := \\argmin{\\gamma} \\meann \\eta_n^2\n\\]\nis not uniquely defined — there is a whole family of coefficients that acheive the same optimal fit."
  },
  {
    "objectID": "lectures/Lecture6.html#redundant-variables-in-matrix-form",
    "href": "lectures/Lecture6.html#redundant-variables-in-matrix-form",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "Redundant variables in matrix form",
    "text": "Redundant variables in matrix form\nHow can we understand this in matrix form? In this case, we can write the third element of the \\(\\zv_n\\) regressor as a linear combination of the other two:\n\\[\n\\zv_{n3} = \\x_n - 1 = \\zv_{n2} - \\zv_{n1}\n\\quad\\Leftrightarrow\\quad\n\\zv_{n3} - \\zv_{n2} + \\zv_{n1}  = 0\n\\quad\\Leftrightarrow\\quad\n\\zv_n^\\trans\n\\begin{pmatrix}\n1 \\\\\n-1 \\\\\n1\n\\end{pmatrix} =: \\zv_n^\\trans \\vv = 0.\n\\]\nSince this is true for every row, we have that \\(\\Z \\vv = \\zerov\\) as well, and so\n\\[\n\\vv^\\trans (\\Z^\\trans \\Z) \\vv = 0.\n\\]\nIn other words, \\(\\vv\\) is a non-zero vector in the nullspace of \\(\\Z^\\trans \\Z\\). It follows that \\(\\Z^\\trans \\Z\\) is not invertible, and the OLS coefficient \\((\\Z^\\trans\\Z)^{-1} \\Z^\\trans \\Y\\) is not defined!\nBut that does not prevent us from finding the optimal projection. That is, we can still find\n\\[\n\\textrm{Well-defined: }\\quad\n\\min_{\\gamma} \\norm{\\Y - \\Z \\gamma}_2^2\n\\quad\\quad\\quad\n\\quad\\quad\\quad\n\\textrm{Ill-defined: }\\quad\n\\argmin{\\gamma} \\norm{\\Y - \\Z \\gamma}_2^2.\n\\]\nSpecifically,\n\\[\n\\min_{\\gamma} \\norm{\\Y - \\Z \\gamma}_2^2 = \\norm{(\\id - \\proj{\\S_\\Z}) \\Y}_2^2\n\\]\nis the norm of the projection perpendicular to the space spanned by the columns of \\(\\Z\\), it’s just that this space is two-dimensional, not three-dimensional."
  },
  {
    "objectID": "lectures/Lecture6.html#in-r",
    "href": "lectures/Lecture6.html#in-r",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "In R",
    "text": "In R\nIt turns out that R does not actually estimate the OLS fit by forming \\((\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\\). It uses an iterative procedure that is roughly similar to Gaussian elimination to estimate one component at a time. When it gets to a component that cannot be estimated beacuse \\(\\X^\\trans \\X\\) has a non-trivial nullspace, then the fit terminates, and it reports the values for the coefficients estimated up to that point, with NA for the rest.\nIn our example, the difference hw_diff is a linear combination of height_norm and weight_norm. Thus the regressors have a non-trivial nullspace, and the best fit \\(\\Yhat\\) is defined even though the regressors are not."
  },
  {
    "objectID": "lectures/Lecture8.html",
    "href": "lectures/Lecture8.html",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "",
    "text": "\\(\\LaTeX\\)"
  },
  {
    "objectID": "lectures/Lecture8.html#the-bivariate-normal-distribution",
    "href": "lectures/Lecture8.html#the-bivariate-normal-distribution",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "The bivariate normal distribution",
    "text": "The bivariate normal distribution\nSuppose that \\(\\RV{\\u}_1\\) and \\(\\RV{\\u}_2\\) are independent standard normal random variables. Suppose we define the new random variables\n\\[\n\\begin{aligned}\n\\RV{\\x_1} :={}& a_{11} \\RV{\\u}_1 + a_{12} \\RV{\\u}_2 \\\\\n\\RV{\\x_2} :={}& a_{21} \\RV{\\u}_1 + a_{22} \\RV{\\u}_2.\n\\end{aligned}\n\\]\nWe can see that \\(\\RV{\\x_1}\\) and \\(\\RV{x_2}\\) are each normal random variables, and we can compute their means and variances:\n\\[\n\\begin{aligned}\n\\expect{\\RV{\\x_1}} :={}& a_{11} \\expect{\\RV{\\u}_1} + a_{12} \\expect{\\RV{\\u}_2} = 0 \\\\\n\\expect{\\RV{\\x_2}} :={}& a_{21} \\expect{\\RV{\\u}_1} + a_{22} \\expect{\\RV{\\u}_2} = 0 \\\\\n\\var{\\RV{\\x_1}} :={}& a_{11}^2 \\var{\\RV{\\u}_1} + a_{12}^2 \\var{\\RV{\\u}_2} = a_{11}^2 + a_{12}^2 \\\\\n\\var{\\RV{\\x_2}} :={}& a_{21}^2 \\var{\\RV{\\u}_1} + a_{22}^2 \\var{\\RV{\\u}_2} = a_{21}^2 + a_{22}^2.\n\\end{aligned}\n\\]\nBut in general \\(\\RV{\\x_1}\\) and \\(\\RV{x_2}\\) are not independent. If they were, we would have \\(\\expect{\\RV{\\x_1} \\RV{\\x_2}} = \\expect{\\RV{\\x_1}} \\expect{\\RV{\\x_2}} = 0\\), but in fact we have\n\\[\n\\expect{\\RV{\\x_1} \\RV{\\x_2}} =\n\\expect{(a_{11} \\RV{\\u}_1 + a_{12} \\RV{\\u}_2)(a_{21} \\RV{\\u}_1 + a_{22} \\RV{\\u}_2)} =\na_{11} a_{21} \\expect{\\RV{\\u}_1^2} + a_{12} a_{22} \\expect{\\RV{\\u}_2^2} =\na_{11} a_{21} + a_{12} a_{22}.\n\\]\nThe variables \\(\\RV{\\x_1}\\) and \\(\\RV{x_2}\\) are instances of the bivariate normal distribution, which is the two-dimensional analogue of the normal distribution. One can define the bivariate normal distribution in many ways. Here, we have defined it by taking linear combinations of independent univariate normal distributions. It turns out this is always possible, and for the purposes of this class, we will see that it is enough to use such a definition.\nThe properties we derived above can in fact be represented more succinctly in vector notation. We can write\n\\[\n\\RV{\\xv} =\n\\begin{pmatrix}\n\\RV{\\x_1} \\\\\n\\RV{\\x_2} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\RV{\\u}_1 \\\\\n\\RV{\\u}_2 \\\\\n\\end{pmatrix} =:\n\\A \\RV{\\uv}.\n\\]\nNote that the matrix \\(\\A\\) is not random. Defining the expectation of a vector to be the vector of expectations of its entries, we get\n\\[\n\\expect{\\RV{\\xv}} = \\A \\expect{\\RV{\\uv}} = \\A \\zerov = \\zerov,\n\\]\njust as above.\nDefining the “variance” of \\(\\RV{\\xv}\\) is more subtle. Recall that a normal distribution is fully characterized by its variance. We would like this to be the case for the bivariate normal as well. But for this it is not enough to know the marginal varianecs \\(\\var{\\RV{\\x_1}}\\) and \\(\\var{\\RV{\\x_2}}\\), since the “covariance” \\(\\cov{\\RV{\\x_1}, \\RV{\\x_2}}\\) is also very important for the behavior of the random variable.\n\n\n\n\n\n\nNotation\n\n\n\nWhen dealing with two scalar– or vector–valued random variables, I will write the covariance as a function of two arguments, e.g., \\(\\cov{\\RV{\\x_1}, \\RV{\\x_2}}\\). However, when speaking of only a single random variable I might write \\(\\cov{\\RV{\\xv}}\\) as a shorthand for the covariance of a vector with itself, e.g., \\(\\cov{\\RV{\\xv}} = \\cov{\\RV{\\xv}, \\RV{\\xv}}\\). I will try to reserve the variance \\(\\var{\\RV{\\x}}\\) for only the variance of a single scalar random variable.\n\n\nA convenient way to write all the covariances in a single expression is to define\n\\[\n\\begin{aligned}\n\\cov{\\RV{\\xv}} ={}&\n  \\expect{\\left(\\RV{\\xv} - \\expect{\\RV{\\xv}} \\right)\n          \\left(\\RV{\\xv} - \\expect{\\RV{\\xv}} \\right)^\\trans}\n          \\\\={}&\n  \\expect{\n    \\begin{pmatrix}\n      (\\RV{\\x_1} - \\expect{\\RV{\\x_1}})^2 &\n        (\\RV{\\x_1} - \\expect{\\RV{\\x_1}})(\\RV{\\x_2} - \\expect{\\RV{\\x_2}}) \\\\\n      (\\RV{\\x_1} - \\expect{\\RV{\\x_1}})(\\RV{\\x_2} - \\expect{\\RV{\\x_2}}) &\n        (\\RV{\\x_2} - \\expect{\\RV{\\x_2}})^2\n    \\end{pmatrix}\n  }\n          \\\\={}&\n  \\begin{pmatrix}\n  \\var{\\RV{\\x_1}} & \\cov{\\RV{\\x_1}, \\RV{\\x_2}} \\\\\n  \\cov{\\RV{\\x_1}, \\RV{\\x_2}} & \\var{\\RV{\\x_2}}\n  \\end{pmatrix}.\n\\end{aligned}\n\\]\nNote that the dimension of the matrix inside the expectation is \\(2 \\times 2\\), since we take the transpose on the left. By expanding, we see that each entry of this “covariance matrix” has the covariance between two entries of the vector, and the diagonal contains the “marginal” variances.\nThis expression in fact allows quite convenient calculation of all the covariances above, since\n\\[\n\\begin{aligned}\n\\cov{\\RV{\\xv}} ={}&\n\\expect{\\A \\RV{\\uv} \\RV{\\uv}^\\trans \\A^\\trans}\n\\\\={}&\n\\A  \\expect{\\RV{\\uv} \\RV{\\uv}^\\trans } \\A^\\trans\n\\\\={}&\n\\A  \\id \\A^\\trans\n\\\\={}&\n\\A \\A^\\trans.\n\\end{aligned}\n\\]\nYou can readily verify that the expression matches the ones derived manually above."
  },
  {
    "objectID": "lectures/Lecture8.html#the-multivariate-normal-distribution",
    "href": "lectures/Lecture8.html#the-multivariate-normal-distribution",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "The multivariate normal distribution",
    "text": "The multivariate normal distribution\nWe can readily generalize the previous section to \\(P\\)–dimensional vectors. Let \\(\\RV{\\uv}\\) denote a vector of \\(P\\) standard normal random variables. Then define\n\\[\n\\RV{\\xv} := \\A \\RV{\\uv} + \\muv.\n\\]\nThen we say that \\(\\RV{\\xv}\\) is a multivariate normal random variable with mean\n\\[\n\\expect{\\RV{\\xv}} = \\A \\expect{\\RV{\\uv}} + \\muv = \\muv\n\\]\nand\n\\[\n\\cov{\\RV{\\xv}} =\n\\expect{(\\RV{\\xv} - \\muv)(\\RV{\\xv} - \\muv)^\\trans} =\n\\expect{(\\A \\RV{\\uv})(\\A \\RV{\\uv})^\\trans} =\n\\A \\A^\\trans,\n\\]\nwriting\n\\[\n\\RV{\\xv} \\sim \\gauss{\\muv, \\A \\A^\\trans}.\n\\]\nSuppose we want to design a multivariate normal with a given covariance matrix \\(\\Sigmam\\). If we require that \\(\\Sigmam\\) be positive semi-definite (see exercise), the we can take \\(\\A = \\Sigmam^{1/2}\\), and use that to construct a multivariate normal with covariance \\(\\Sigmam\\).\n\n\n\n\n\n\nNotation\n\n\n\nI will write \\(\\RV{\\xv} \\sim \\gauss{\\muv, \\Sigmam}\\) to mean that \\(\\RV{\\xv}\\) is a multivariate normal random variable with mean \\(\\muv\\) and covariance matrix \\(\\Sigmam\\).\n\n\nNote that we will not typically go through the construction \\(\\RV{\\xv} = \\Sigmam^{1/2} \\RV{\\uv}\\) — we’ll take for granted that we can do so. The construction in terms of univariate random variables is simply an easy way to define multivariate random variables without having to deal with multivariate densities.\n\n\n\n\n\n\nExercise\n\n\n\nShow that if you have a vector-valued random variable with a non positive semi-definite covariance matrix, you can construct a univariate random variable with negative variance, which is impossible. It follows that every vector covariance must be postive semi-definite.\n\n\nA few useful properties come out immediately from properties of the univariate normal distribution:\n\nThe entries of a multivariate normal random vector are independent if an only if the covariance matrix is diagonal.\nAny linear combination of a multivariate normal random variable is itself multivariate normal (possibly with different dimension).\n\\(\\prob{\\RV{\\xv} \\in S} = \\prob{\\RV{\\uv} \\in \\{\\uv: \\A \\uv \\in S\\}}\\)"
  },
  {
    "objectID": "lectures/Lecture8.html#generic-random-vectors",
    "href": "lectures/Lecture8.html#generic-random-vectors",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "Generic random vectors",
    "text": "Generic random vectors\nIn general, we can consider a random vector — or a random matrix — as a collection of potentially non-independent random values.\nFor such a vector \\(\\RV{\\xv} \\in \\rdom{P}\\), we can speak about its expectation and covariance just as we would for a multivariate normal distribution. Specifically,\n\\[\n\\expect{\\RV{\\xv}} =\n\\begin{pmatrix}\n\\expect{\\RV{\\x}_1} \\\\\n\\vdots \\\\\n\\expect{\\RV{\\x}_P} \\\\\n\\end{pmatrix}\n\\quad\\textrm{and}\\quad\n\\cov{\\RV{\\xv}} =\n\\expect{\\left(\\RV{\\xv} - \\expect{\\RV{\\xv}} \\right)\n        \\left(\\RV{\\xv} - \\expect{\\RV{\\xv}} \\right)^\\trans} =\n\\begin{pmatrix}\n\\var{\\RV{\\x_1}} & \\cov{\\RV{\\x_1}, \\RV{\\x_2}} & \\ldots & \\cov{\\RV{\\x_1}, \\RV{\\x_P}} \\\\\n\\cov{\\RV{\\x_2}, \\RV{\\x_1}} & \\ldots & \\ldots & \\cov{\\RV{\\x_2}, \\RV{\\x_P}} \\\\\n\\vdots & & & \\vdots \\\\\n\\cov{\\RV{\\x_P}, \\RV{\\x_1}} & \\ldots & \\ldots & \\var{\\RV{\\x_P}} \\\\\n\\end{pmatrix}.\n\\]\nFor these expressions to exist, it suffices for \\(\\expect{\\RV{\\x}_p^2} &lt; \\infty\\) for all \\(p \\in \\{1,\\ldots,P\\}\\).\nWe will at times talk about the expectation of a random matrix, \\(\\RV{\\X}\\), which is simply the matrix of expectations. The notation for covariances of course doesn’t make sense for matrices, but it won’t be needed. (In fact, in cases where the covariance of a random matrix is needed in statistics, the matrix is typically stacked into a vector first.)"
  },
  {
    "objectID": "lectures/Lecture8.html#the-law-of-large-numbers-for-vectors",
    "href": "lectures/Lecture8.html#the-law-of-large-numbers-for-vectors",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "The law of large numbers for vectors",
    "text": "The law of large numbers for vectors\nThe law of large numbers is particularly simple for vectors — as long as the dimension stays fixed as \\(N \\rightarrow \\infty\\), you can simply apply the LLN to each component separately. Suppose you’re given a sequence of vector-valued random variables, \\(\\RV{\\xv}_n \\in \\rdom{P}\\). Write \\(\\expect{\\RV{\\xv}_n} = \\muv_n\\), and \\(\\var{\\RV{\\xv}_{np}} = \\v_{np}\\) for each \\(p \\in \\{1,\\ldots,P\\}\\). Assume that \\(\\max_{n,p} \\v_{np} &lt; \\infty\\), and that \\(\\muv_n \\rightarrow \\overline{\\muv}\\).\n\n\n\n\n\n\nNote to future instructors\n\n\n\nAs with the univariate LLN, we really need \\(\\lim_{N\\rightarrow\\infty} \\max_{n=1,\\ldots,N, p} \\frac{1}{N} \\v_{np} = 0\\), which is more convenient when we actually use this for the fixed regressor setting, but which complicates the exhibition here.\n\n\nThen we can apply the LLN to each component, getting\n\\[\n\\overline{\\RV{\\xv}} :=\n\\meann \\RV{\\xv}_n \\rightarrow \\overline{\\muv}\n\\quad\\textrm{as }N\\rightarrow \\infty.\n\\]\nYou might be worried that we’re combining probabilistic arguments, even though the entries of \\(\\xv_n\\) might not be independent.\n(If you are worried about that, wonderful!) Since the dimension is fixed, note that we can write\n\\[\n\\prob{\\abs{\\overline{\\RV{\\xv}}_p - \\overline{\\muv}_p} &lt; \\varepsilon \\textrm{ for all }p} =\n\\prob{\\abs{\\overline{\\RV{\\xv}}_1 - \\overline{\\muv}_1} &lt; \\varepsilon \\ldots\\textrm{ and }\\ldots\n  \\abs{\\overline{\\RV{\\xv}}_P - \\overline{\\muv}_P} &lt; \\varepsilon\n} \\le\n\\sum_{p=1}^P\n  \\prob{\\abs{\\overline{\\RV{\\xv}}_p - \\overline{\\muv}_p} &lt; \\varepsilon}\n  \\rightarrow 0,\n\\]\nsince each entry in the sum goes to zero."
  },
  {
    "objectID": "lectures/Lecture8.html#extensions",
    "href": "lectures/Lecture8.html#extensions",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "Extensions",
    "text": "Extensions\nSince we see that the shape of the vector doesn’t matter, we can also apply the LLN to matrices. For example, if \\(\\RV{\\X}_n\\) is a sequence of random matrices, with \\(\\expect{\\RV{\\X}_n} = \\A\\), then\n\\[\n\\meann \\RV{\\X}_n \\rightarrow \\A.\n\\]\nWe will also use (without proof) a theorem called the continuous mapping theorem, which says that, for a continous function \\(f(\\cdot)\\), then\n\\[\nf\\left(\\meann \\RV{\\xv}_n\\right) \\rightarrow f(\\overline{\\mu}).\n\\]\nNote that the preceding statment is different than saying\n\\[\n\\meann  f\\left( RV{\\xv}_n \\right) \\rightarrow \\expect{f\\left( RV{\\xv}_n \\right)},\n\\]\nwhich may also be true, but which applies the LLN to the random variables \\(f\\left( RV{\\xv}_n \\right)\\)."
  },
  {
    "objectID": "lectures/Lecture8.html#the-central-limit-theorem-for-vectors",
    "href": "lectures/Lecture8.html#the-central-limit-theorem-for-vectors",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "The central limit theorem for vectors",
    "text": "The central limit theorem for vectors\nWe might hope that we can apply the CLT componentwise just as we did for the LLN, but we are not so lucky. It is true that, assuming (as before) that \\(\\meann \\v_{np} \\rightarrow \\overline{v}_p\\) for each \\(p\\), that\n\\[\n\\sqrt{N} (\\overline{\\RV{\\xv}}_p - \\overline{\\mu}_p) \\rightarrow\n  \\gauss{0, \\overline{v}_p}.\n\\]\nHowever, this does not tell us about the joint behavior of the random variables. Here’s a simple example that illustrates the problem. Consider two settings, based on IID standard normal scalar variables \\(\\RV{u}_n\\) and \\(\\RV{v}_n\\).\n\nSetting one\nTake\n\\[\\RV{\\xv}_n =\n\\begin{pmatrix}\n\\RV{u}_n\\\\\n\\RV{v}_n\n\\end{pmatrix}.\n\\]\nWe know that \\(\\frac{1}{\\sqrt{N}} \\RV{\\xv}_n\\) is exactly multivariate normal with mean \\(\\zerov\\) and covariance matrix \\(\\id{}\\) for all \\(N\\). So, a fortiori,\n\\[\n\\frac{1}{\\sqrt{N}} \\sumn \\RV{\\xv}_n \\rightarrow \\gauss{\\zerov, \\id{}}.\n\\]\n\n\nSetting two\nTake\n\\[\\RV{\\yv}_n =\n\\begin{pmatrix}\n\\RV{u}_n\\\\\n\\RV{u}_n\n\\end{pmatrix}.\n\\]\nWe know that \\(\\frac{1}{\\sqrt{N}} \\RV{\\yv}_n\\) is exactly multivariate normal with mean \\(\\zerov\\) and covariance matrix \\(\\onev \\onev^\\trans\\) for all \\(N\\).\nSo, a fortiori,\n\\[\n\\frac{1}{\\sqrt{N}} \\sumn \\RV{\\yv}_n \\rightarrow \\gauss{\\zerov, \\onev \\onev^\\trans}.\n\\]\nFor both \\(\\RV{\\xv}\\) and \\(\\RV{\\yv}\\), each component converges to a standard normal random variable. But for \\(\\RV{\\xv}\\) the two components were independent, and for \\(\\RV{\\yv}\\) they were perfectly dependent. The behavior of the marginals cannot tell you about the joint behavior.\nConsequently, we have to consider the behavior of the whole vector. In particular, write\n\\[\n\\Sigmam_n := \\cov{\\RV{\\xv}_n}\n\\]\nand assume that\n\\[\n\\meann \\Sigmam_n \\rightarrow \\overline{\\Sigmam}\n\\]\nfor each entry of the matrix, where \\(\\overline{\\Sigmam}\\) is element-wise finite. (Note that this requires the average of each entry of the diagonal to to converge!) Then\n\\[\n\\frac{1}{\\sqrt{N}} \\left( \\sumn \\RV{\\xv}_n - \\expect{\\RV{\\xv}_n} \\right)\n\\rightarrow \\RV{\\zv} \\quad\\textrm{ where }\n\\RV{\\zv} \\sim \\gauss{\\zerov, \\overline{\\Sigmam}}.\n\\]"
  },
  {
    "objectID": "lectures/Lecture8.html#extensions-1",
    "href": "lectures/Lecture8.html#extensions-1",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "Extensions",
    "text": "Extensions\nFinally, we note that the continuous mapping theorem applies to the CLT as well. That is, if \\(f(\\cdot)\\) is continuous, and\n\\[\n\\frac{1}{\\sqrt{N}} \\left( \\sumn \\RV{\\xv}_n - \\expect{\\RV{\\xv}_n} \\right)\n\\rightarrow \\RV{\\zv}\n\\quad\\textrm{ where }\n\\RV{\\zv} \\sim \\gauss{\\zerov, \\overline{\\Sigmam}},\n\\]\nthen\n\\[\nf\\left( \\frac{1}{\\sqrt{N}} \\left( \\sumn \\RV{\\xv}_n - \\expect{\\RV{\\xv}_n} \\right) \\right)\n\\rightarrow f\\left( \\RV{\\zv} \\right).\n\\]"
  },
  {
    "objectID": "lectures/LinearAlgebraReview.html",
    "href": "lectures/LinearAlgebraReview.html",
    "title": "Linear Algebra topics",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\n\\(\\LaTeX\\)\n\nSummary of prerequisite requirements\nFor 151A, we require familiarity with the following basic concepts from linear algebra. In rough order of importance,\n\nEssential: Basic vector and matrix operations\n\nAddition and subtraction\nMultiplication by scalars\nMatrix-matrix and matrix-vectror multiplication\nVector dot products, norms, and orthogonality\nMatrix transposes and inverses, symmetric matrices\n\nEssential: Linear systems\n\nRepresenting a linear system as a matrix equation and vice-versa\nSolving exactly specified linear systems using matrix inverses\n\nVery important: Vector spaces and bases\n\nRank and null spaces of matrices\nSolution space of linear systems that are not exactly identified\nOrthogonal vectors and orthonormal bases\n\nHelpful: Eigendecomposition of symmetric, square matrices\n\nEigenvalues and eigenvectors\nThe relationship between an inverse and the eigendecomposition\nEigenvectors as an orthonormal basis\nThe trace and determinant as a function of eigenvalues\nPositive definiteness\n\n\nThere are some (wonderful, useful, fascinating) linear algebra topics that we won’t require in 151A, but which are commonly included in an introductory linear algebra course:\n\nNot very important for 151A (but great to know anyway):\n\nManually solving systems of equations (e.g., row-echelon form, permutations)\nAdvanced properties of determinants (e.g. Cramer’s rule)\nSingular value decompositions\nLinear operators on abstract spaces (e.g. spaces of polynomials)\nEigendecompositions of non-symmetric matrices\nFactorizations (e.g. LU, QR)\nGram-Schmidt orthogonalization (other than knowing that it can be done)\nJordan canonical forms\nMatrix exponentiation and power series\n\n\n\n\nIntroduction to Applied Linear Algebra by Boyd and Vandenberghe\nA good book which is freely available online is Introduction to Applied Linear Algebra by Boyd and Vandenberghe (B&V). For 151A, the following sections and their exercises are useful:\n\nVector operations\n\nB&V 1.1-1.4 (addition, scalar multiplication, dot product)\nB&V 3.1, 3.2 (norms — we will mostly use the Euclidian (L2) norm)\n\nMatrix operations\n\nB&V 6.1-6.4 (addition, zero and identity, trace, transpose)\nB&V 10.1 (multiplication)\nB&V 11.1-11.2 (inverses)\n\nLinear systems\n\nB&V 8.1, 8.2.2, 8.3 (definition and examples)\nB&V 11.3 (solution via matrix inversions)\n\nVector spaces and bases\n\nB&V 5.1-5.3 (linear depenAbsolutely necessarydence, bases, orthonormality)\n\n\nSome more advanced topics not treaded in B&V are eigenvalues and nullspaces.\n\n\nMIT Open Courseware with Gilbert Strang\nThere are good recorded lectures in the MIT open courseware linear algebra series. The corresponding textbook, Strang’s Introduction to Linear Algebra, is also good, but is unfortunately not available online through the library website, and there are only a few hard copies available.\nAssuming you can get your hands on a copy of the book, here is what I recommend reviewing. The sections refer to readings. The sections in the videos do not appear to line up perfectly with the readings, so where they’ve deviated, I’ve made my best guess as to the corresponding video.\n\nEssential: Sections 3, 6, 10, 14\nVery important: Sections 7, 15, 16, 10 (video 9), 11 (video 9)\nVery helpful: Sections 17, 21, 27 (video 25), 28 (video 25), 33 (video 30)\n\n\n\nYu Tsumura’s website\nThe website by Yu Tsumura has a wealth of interesting problems with solutions. Based on the titles, the following sections could be useful for 151A:\n\nIntroduction to Matrices\nElementary Row Operations\nSolutions of Systems of Linear Equations\nLinear Combination and Linear Independence\nNonsingular Matrices\nInverse Matrices\nSubspaces in Rn\nBases and Dimension of Subspaces in Rn\nBases and Coordinate Vectors\nLinear Transformation from Rn to Rm\nOrthogonal Bases\nIntroduction to Eigenvalues and Eigenvectors\nEigenvectors and Eigenspaces\nDot Products and Length of Vectors"
  },
  {
    "objectID": "quizzes/c45bd141bc098d09ca957d5dd3885992.html",
    "href": "quizzes/c45bd141bc098d09ca957d5dd3885992.html",
    "title": "STAT151A Quiz 1 (Jan 30th)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[2in]\\]\nFor this quiz, we’ll consider the linear model \\(y_n = \\beta_1 \\z_n + \\beta_2 \\w_n + \\res_n\\).\nNote that there is no intercept, and instead are two scalar regressors, \\(\\z_n\\) and \\(\\w_n\\).\nRecall that the inverse of a 2x2 matrix is given by\n\\[\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}^{-1} =\n\\frac{1}{ad - bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\n\n\n(a)\nWrite the set of equations\n\\[\ny_n = \\beta_1 \\z_n + \\beta_2 \\w_n + \\res_n\n\\]\nfor \\(n \\in \\{1, \\ldots, N\\}\\) in matrix form. That is, let \\(\\X\\) denote an \\(N \\times\n2\\) matrix, \\(\\Y\\) and \\(\\resv\\) length–\\(N\\) column vectors, and \\(\\bv= (\\beta_0, \\beta_1)^\\trans\\) a length–\\(2\\) column vector. Then express the matrices \\(\\Y\\), \\(\\X\\), and \\(\\resv\\) in terms of the scalars \\(\\y_n\\), \\(\\z_m\\), \\(\\w_n\\), and \\(\\res_n\\) so that \\(\\Y= \\X\\bv+ \\resv\\) is equivalent to the set of regression equations.\n\n\n\n(b)\nDefine the following quantities: \\[\n\\begin{aligned}\n    \\overline{z} :={}& \\meann \\z_n &\n    \\overline{w} :={}& \\meann \\w_n &\n    \\overline{y} :={}& \\meann \\y_n &\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    \\overline{ww} :={}& \\meann \\w_n^2 &\n    \\overline{zw} :={}& \\meann \\z_n \\w_n &\n    \\overline{zz} :={}& \\meann \\z_n^2 &\n    \\overline{wy} :={}& \\meann \\w_n \\y_n &\n    \\overline{zy} :={}& \\meann \\z_n \\y_n.\n\\end{aligned}\n\\]\nIn terms of these quantities and \\(N\\) alone, write expressions for \\(\\X^\\trans \\X\\), \\(\\X^\\trans \\Y\\), and \\((\\X^\\trans \\X)^{-1}\\).\n\n\n\n(c)\nNow, for only this part of the quiz, assume that \\(\\overline{wz} = 0\\). Under this assumption, write an expression for the least squares solution \\(\\betahat\\) which minimizes \\[\n\\betahat := \\argmin{\\beta} \\sumn (\\y_n - \\beta_1 \\z_n - \\beta_2 \\w_n)^2.\n\\]"
  },
  {
    "objectID": "quizzes/f5dddb3a5731f04712b8ebdca7fc4c00.html",
    "href": "quizzes/f5dddb3a5731f04712b8ebdca7fc4c00.html",
    "title": "STAT151A Quiz 2 (Feb 13th)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[1in]\\]\nFor this quiz, we’ll consider the linear models\n\\[\n\\begin{aligned}\ny_n ={} \\betav^\\trans \\xv_n + \\res_n\n&\\quad\\textrm{and}\\quad\ny_n ={} \\gammav^\\trans \\zv_n + \\eta_n\n\\end{aligned}\n\\]\nwith\n\\[\n\\begin{aligned}\n\\xv_n ={} (1, \\x_n)^\\trans\n&\\quad\\textrm{and}\\quad\n\\zv_n ={} (1, \\z_n)^\\trans \\textrm{ where}\n\\\\\n\\overline{\\x} :={} \\meann \\x_n\n&\\quad\\textrm{and}\\quad\n\\z_n :={} \\x_n - \\overline{\\x}.\n\\end{aligned}\n\\]\nAssume that \\(\\x_n\\) is not a constant (i.e., for at least one pair \\(n\\) and \\(m\\), \\(\\x_n \\ne \\x_m\\).).\nLet \\(\\X\\) denote the \\(N \\times 2\\) matrix whose \\(n\\)–th row is \\(\\xv_n^\\trans\\), and \\(\\Z\\) denote the \\(N \\times 2\\) matrix whose \\(n\\)–th row is \\(\\zv_n^\\trans\\).\nRecall that the inverse of a 2x2 matrix is given by\n\\[\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}^{-1} =\n\\frac{1}{ad - bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\n\n\n(a)\nFind a \\(2 \\times 2\\) matrix \\(\\A\\) such that \\(\\Z = \\X \\A\\).\n\n\n\n(b)\nSuppose I tell you that the OLS estimate of \\(\\beta\\) is given by \\(\\betahat = (2, 3)\\), and that \\(\\overline{x} = 4\\). What is the value of \\(\\gammahat\\), the OLS estimate of \\(\\gamma\\)?\n\n\n\n(c)\nIn general, can you say whether one regression will provide a better fit than the other? That is, can you say which of \\(\\meann (\\y_n - \\zv_n^\\trans\\gammahat)^2\\) and \\(\\meann (\\y_n - \\xv_n^\\trans\\betahat)^2\\) is smaller? Argue why or why not."
  },
  {
    "objectID": "quizzes/f4e48853e74f0a0081cdf9475a6deb7a.html",
    "href": "quizzes/f4e48853e74f0a0081cdf9475a6deb7a.html",
    "title": "STAT151A Quiz 4 (Mar 12th)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[1in]\\]\nThis question will take the residuals of the training data to be random, and will consider variablity under sampling of the training data. The regressors for both the training data and test data will be taken as fixed.\nThe inverse of a 2x2 matrix is given by \\[\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}^{-1} =\n\\frac{1}{ad - bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nThe OLS estimator is given by \\(\\betahat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\\).\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\n\n\n(a)\nFor this quiz, we will assume that \\(\\y_n = \\beta^\\trans \\xv_n + \\res_n\\) for some \\(\\beta\\), and that the residuals \\(\\res_n\\) are IID with \\(\\expect{\\res_n} = 0\\) and \\(\\expect{\\res_n^2} = 1\\), but not necessarily normal.\nLet \\(\\xv_n  = (1, \\z_{n})^\\trans\\), where \\(\\meann \\z_n = 0\\) and \\(\\meann \\z_n^2 = \\delta &gt; 0\\). That is, assume we are regressing on a constant and a single mean-zero regressor. For this question, take the regressors to be fixed (not random).\nFind the limiting distribution of \\(\\sqrt{N}(\\betahat - \\beta)\\) as \\(N \\rightarrow \\infty\\).\n\n\n\n(b)\nDefine the expected prediction error \\[\n\\begin{aligned}\n\\y_\\new :={} \\beta^\\trans \\xv_{\\new} + \\res_\\new\n\\quad\\textrm{and}\\quad\n\\yhat_\\new :={}  \\betahat^\\trans \\xv_\\new.\n\\end{aligned}\n\\]\nUnder the conditions given in part (a), find the limiting distribution of\n\\[\n\\sqrt{N} (\\yhat_\\new - \\expect{\\y_\\new} )\n\\]\nas \\(N \\rightarrow \\infty\\), as a function of \\(\\xv_\\new\\). That is, the limiting distribution will depend on \\(\\xv_\\new\\), so please make the dependence explicit.\nYou may use your answer from part (a).\n\n\n\n(c)\nAssume the conditions and definitions given in (a) and (b). Assume that \\(\\delta \\ll 1\\) (that is, \\(\\delta\\) is much smaller than \\(1\\).)\nFind the limiting distribution of \\(\\sqrt{N} (\\yhat_\\new - \\expect{\\y_\\new} )\\) when\n\n\\(\\xv_\\new = (1, 0)\\) and\n\\(\\xv_\\new = (1, 1)\\).\n\nWhich of the two is larger?\nYou may use your answer from parts (a) and (b)."
  },
  {
    "objectID": "lectures/Lecture10_code.html",
    "href": "lectures/Lecture10_code.html",
    "title": "Investigating the law of large numbers by simulation",
    "section": "",
    "text": "In this exercise / lecture, we’ll be simulating data to observe the law of large numbers (LLN) in action. We’ll proceed by:\nAlong the way, we’ll be particularly interested in ways the LLN — and results which rely on it — break down.\nlibrary(tidyverse)\nlibrary(mvtnorm)\n\ntheme_update(text = element_text(size=24))\noptions(repr.plot.width=12, repr.plot.height=6)\n\n── Attaching core tidyverse packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "lectures/Lecture10_code.html#some-vector-lln-questions",
    "href": "lectures/Lecture10_code.html#some-vector-lln-questions",
    "title": "Investigating the law of large numbers by simulation",
    "section": "Some vector LLN questions:",
    "text": "Some vector LLN questions:\n\nHow does the covariance matrix affect convergence? Try:\n\nHighly correlated observations\nA nearly singular matrix\n\nHow do transformations affect convergence? Some to try: inverse, eigenvalues, sum of squares of entries, eigenvectors.\nCan you find a covariance matrix for which the LLN does very badly with the inverse?"
  },
  {
    "objectID": "lectures/Lecture13_code_v2.html",
    "href": "lectures/Lecture13_code_v2.html",
    "title": "Training set variability",
    "section": "",
    "text": "library(tidyverse)\n\ntheme_update(text = element_text(size=24))\noptions(repr.plot.width=12, repr.plot.height=6)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n# fage: Father's age in years.\n# mage: Mother's age in years.\n# mature: Maturity status of mother.\n# weeks: Length of pregnancy in weeks.\n# premie: Whether the birth was classified as premature (premie) or full-term.\n# visits: Number of hospital visits during pregnancy.\n# gained: Weight gained by mother during pregnancy in pounds.\n# weight: Weight of the baby at birth in pounds.\n# lowbirthweight: Whether baby was classified as low birthweight (low) or not (not low).\n# sex: Sex of the baby, female or male.\n# habit: Status of the mother as a nonsmoker or a smoker.\n# marital: Whether mother is married or not married at birth.\n# whitemom: Whether mom is white or not white.\n\ndf_raw &lt;- read.csv(\"../datasets/births14.csv\")\n\n\n# Drop missing data\ndf_raw %&gt;% summarise_all(~ sum(is.na(.)))\ndf &lt;- df_raw %&gt;% drop_na(everything())\ncat(\"Dropped \", nrow(df_raw) - nrow(df), \" rows \\n\")\nhead(df)\n\n# Meaning of truncated variables\ndf %&gt;% group_by(mature) %&gt;% summarize(min(mage), max(mage))\ndf %&gt;% group_by(premie) %&gt;% summarize(min(weeks), max(weeks))\ndf %&gt;% group_by(lowbirthweight) %&gt;% summarize(min(weight), max(weight))\n\n\nA data.frame: 1 × 13\n\n\nfage\nmage\nmature\nweeks\npremie\nvisits\ngained\nweight\nlowbirthweight\nsex\nhabit\nmarital\nwhitemom\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n114\n0\n0\n0\n0\n56\n42\n0\n0\n0\n19\n0\n0\n\n\n\n\n\nDropped  206  rows \n\n\n\nA data.frame: 6 × 13\n\n\n\nfage\nmage\nmature\nweeks\npremie\nvisits\ngained\nweight\nlowbirthweight\nsex\nhabit\nmarital\nwhitemom\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\n34\n34\nyounger mom\n37\nfull term\n14\n28\n6.96\nnot low\nmale\nnonsmoker\nmarried\nwhite\n\n\n2\n36\n31\nyounger mom\n41\nfull term\n12\n41\n8.86\nnot low\nfemale\nnonsmoker\nmarried\nwhite\n\n\n3\n37\n36\nmature mom\n37\nfull term\n10\n28\n7.51\nnot low\nfemale\nnonsmoker\nmarried\nnot white\n\n\n4\n32\n31\nyounger mom\n36\npremie\n12\n48\n6.75\nnot low\nfemale\nnonsmoker\nmarried\nwhite\n\n\n5\n32\n26\nyounger mom\n39\nfull term\n14\n45\n6.69\nnot low\nfemale\nnonsmoker\nmarried\nwhite\n\n\n6\n37\n36\nmature mom\n36\npremie\n10\n20\n6.13\nnot low\nfemale\nnonsmoker\nmarried\nwhite\n\n\n\n\n\n\nA tibble: 2 × 3\n\n\nmature\nmin(mage)\nmax(mage)\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nmature mom\n35\n47\n\n\nyounger mom\n14\n34\n\n\n\n\n\n\nA tibble: 2 × 3\n\n\npremie\nmin(weeks)\nmax(weeks)\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nfull term\n37\n46\n\n\npremie\n23\n36\n\n\n\n\n\n\nA tibble: 2 × 3\n\n\nlowbirthweight\nmin(weight)\nmax(weight)\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nlow\n1.12\n5.50\n\n\nnot low\n5.50\n10.42\n\n\n\n\n\n\ny_col &lt;- \"weight\"\n# We don't want to deal with collinear regressors in small samples\n#x_cols &lt;- c(\"mage\", \"fage\", \"sex\", \"habit\", \"marital\")\nx_cols &lt;- c(\"mage\", \"fage\", \"sex\")\nreg_form &lt;- sprintf(\"%s ~ 1 + %s\", y_col, paste(x_cols, collapse=\" + \"))\nx_all &lt;- model.matrix(formula(reg_form), df)\ny_all &lt;- df[[y_col]]\n\nn_all &lt;- length(y_all)\nstopifnot(nrow(x_all) == n_all)\n\n\n# Let's look at a single test point\ntest_ind &lt;- sample(n_all, 1) # Choose a single test point\nx_test &lt;- x_all[test_ind, ]\ny_test &lt;- y_all[test_ind]\n\n\n# Sanity check that lm() is doing what we expect\nn_obs &lt;- 500\ntrain_inds &lt;- sample(setdiff(n_all, test_ind), n_obs, replace=FALSE)\nx &lt;- x_all[train_inds, ]\ny &lt;- y_all[train_inds]\nreg_fit &lt;- lm(formula(reg_form), df[train_inds, ])\n\nbetahat &lt;- solve(t(x) %*% x, t(x) %*% y)\n\nbind_cols(\n  coefficients(reg_fit),\n  betahat)\n\nres &lt;- y - x %*% betahat\nmax(abs(res - summary(reg_fit)$residual))\n\nsigmasqhat &lt;- sum((y - x %*% betahat)^2) / (nrow(x) - ncol(x))\nc(sigmasqhat, summary(reg_fit)$sigma^2)\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n\n\n\nA tibble: 4 × 2\n\n\n...1\n...2\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n6.846416527\n6.846416527\n\n\n0.016162589\n0.016162589\n\n\n-0.008610284\n-0.008610284\n\n\n0.307132574\n0.307132574\n\n\n\n\n\n7.70494779089859e-14\n\n\n\n1.666863480254971.66686348025497\n\n\n\nnum_sims &lt;- 20\nn_obs &lt;- 500\nstopifnot(n_obs &lt; n_all - 1)\n\nintervals_df &lt;- data.frame()\nfor (sim_ind in 1:num_sims) {\n    train_inds &lt;- sample(setdiff(n_all, test_ind), n_obs, replace=FALSE)\n    reg_fit &lt;- lm(formula(reg_form), df[train_inds, ])\n    betahat &lt;- coefficients(reg_fit)\n    sigmahat &lt;- summary(reg_fit)$sigma\n\n    miscoverage_prob &lt;- 0.2\n    za &lt;- qnorm(1 - miscoverage_prob / 2)\n\n    y_pred &lt;- sum(betahat * x_test)\n    width_pred &lt;- za * sigmahat\n\n    intervals_df &lt;- bind_rows(\n      intervals_df,\n      data.frame(sim_ind=sim_ind, \n                 y_pred=y_pred, \n                 width_pred=width_pred, \n                 y_test=y_test,\n                 n_obs=n_obs)\n    )\n}\n\nggplot(intervals_df) +\n  geom_point(aes(x=sim_ind, y=y_test), color=\"red\", size=3) +\n  geom_errorbar(aes(x=sim_ind, ymin=y_pred - width_pred, ymax=y_pred + width_pred),\n                width=0.1) +\n  xlab(\"Simulation number (order doesn't matter)\") +\n  ylab(\"Weight at birth (pounds)\\nPredictive intervals and (unobserved) truth\")\n   \n\n\nnum_sims &lt;- 20\nintervals_df &lt;- data.frame()\n\nfor (n_obs in c(30, 50, 100, 200, 500)) {\n  stopifnot(n_obs &lt; n_all - 1)\n  \n  for (sim_ind in 1:num_sims) {\n    train_inds &lt;- sample(setdiff(n_all, test_ind), n_obs, replace=FALSE)\n    reg_fit &lt;- lm(formula(reg_form), df[train_inds, ])\n    betahat &lt;- coefficients(reg_fit)\n    sigmahat &lt;- summary(reg_fit)$sigma\n    \n    miscoverage_prob &lt;- 0.2\n    za &lt;- qnorm(1 - miscoverage_prob / 2)\n    \n    y_pred &lt;- sum(betahat * x_test)\n    width_pred &lt;- za * sigmahat\n    \n    intervals_df &lt;- bind_rows(\n      intervals_df,\n      data.frame(sim_ind=sim_ind, \n                 y_pred=y_pred, \n                 width_pred=width_pred, \n                 y_test=y_test,\n                 n_obs=n_obs)\n    )\n  }\n}\n\n\nggplot(intervals_df) +\n  geom_point(aes(x=sim_ind, y=y_test), color=\"red\", size=3) +\n  geom_errorbar(aes(x=sim_ind, ymin=y_pred - width_pred, ymax=y_pred + width_pred),\n                width=0.1) +\n  xlab(\"Simulation number (order doesn't matter)\") +\n  ylab(\"Weight at birth (pounds)\\nPredictive intervals and (unobserved) truth\") +\n  facet_grid(.~n_obs)"
  },
  {
    "objectID": "lectures/Lecture18_code.html",
    "href": "lectures/Lecture18_code.html",
    "title": "Interpreting the coefficients and R output",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\n\\(\\,\\)\n\nlibrary(tidyverse)\nlibrary(sandwich)\n\ntheme_update(text = element_text(size=24))\noptions(repr.plot.width=12, repr.plot.height=6)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ninstall &lt;- FALSE\n\n# https://avehtari.github.io/ROS-Examples/examples.html\nif (install) {\n    install.packages(\"rprojroot\")\n    remotes::install_github(\"avehtari/ROS-Examples\", subdir=\"rpackage\")    \n}\n\nlibrary(rosdata)\ndata(kidiq)\n\n\nhead(kidiq)\n\n\nA data.frame: 6 × 5\n\n\n\nkid_score\nmom_hs\nmom_iq\nmom_work\nmom_age\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n65\n1\n121.11753\n4\n27\n\n\n2\n98\n1\n89.36188\n4\n25\n\n\n3\n85\n1\n115.44316\n4\n27\n\n\n4\n83\n1\n99.44964\n3\n25\n\n\n5\n115\n1\n92.74571\n4\n27\n\n\n6\n98\n0\n107.90184\n1\n18\n\n\n\n\n\n\nfit_hs &lt;- lm(kid_score ~ 1 + mom_hs, kidiq)\nprint(fit_hs)\n\nggplot(kidiq) +\n    geom_histogram(aes(x=kid_score)) +\n    facet_grid(mom_hs ~ ., scales=\"free\")\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = kidiq)\n\nCoefficients:\n(Intercept)       mom_hs  \n      77.55        11.77  \n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nKid score = \\(\\beta_0\\) + \\(\\beta_1\\) mom_iq\n\nfit_iq &lt;- lm(kid_score ~ mom_iq, kidiq)\nprint(fit_iq)\n\nggplot(kidiq) +\n    geom_point(aes(x=mom_iq, y=kid_score))\n\n\nCall:\nlm(formula = kid_score ~ mom_iq, data = kidiq)\n\nCoefficients:\n(Intercept)       mom_iq  \n      25.80         0.61  \n\n\n\n\n\n\n\n\n\n\n\nfit_hs_iq &lt;- lm(kid_score ~ mom_hs + mom_iq, kidiq)\nggplot(kidiq) +\n    geom_point(aes(x=mom_iq, y=kid_score, color=factor(mom_hs), group=mom_hs)) +\n    facet_grid(. ~ mom_hs)\n\nprint(fit_hs_iq)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs + mom_iq, data = kidiq)\n\nCoefficients:\n(Intercept)       mom_hs       mom_iq  \n    25.7315       5.9501       0.5639  \n\n\n\n\n\n\n\n\n\n\n\nprint(fit_hs)\nprint(fit_iq)\nprint(fit_hs_iq)\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = kidiq)\n\nCoefficients:\n(Intercept)       mom_hs  \n      77.55        11.77  \n\n\nCall:\nlm(formula = kid_score ~ mom_iq, data = kidiq)\n\nCoefficients:\n(Intercept)       mom_iq  \n      25.80         0.61  \n\n\nCall:\nlm(formula = kid_score ~ mom_hs + mom_iq, data = kidiq)\n\nCoefficients:\n(Intercept)       mom_hs       mom_iq  \n    25.7315       5.9501       0.5639  \n\n\n\n\nSimulations\nLet’s look at the behavior of the standard errors of \\(\\betahat_1\\) when regressing \\(\\y_n \\sim \\beta_0 + \\beta_1 \\x_n\\), where \\(\\var{\\res_n} = \\sigma^2\\) and \\(\\var{\\x_n} = \\sigma_x^2\\).\n\nsigma_x_vals &lt;- seq(1.0, 10, by=2)\nsigma_eps &lt;- 4.0\n\nn_obs &lt;- 1000\n\nbeta_0 &lt;- 1.0\nbeta_1 &lt;- 2.0\n\nfits &lt;- list()\ndf_comb &lt;- data.frame()\nfits &lt;- data.frame()\nfor (sigma_x in sigma_x_vals) {\n    x &lt;- rnorm(n_obs, mean=1.0, sd=sigma_x)\n    eps &lt;- rnorm(n_obs, mean=0, sd=sigma_eps)\n    y &lt;- beta_0 + beta_1 * x + eps\n    df &lt;- data.frame(y=y, x=x, eps=eps, sigma_x=sigma_x)\n    fit &lt;- lm(y ~ 1 + x, df)\n    coeffs &lt;- summary(fit)$coefficients[\"x\", , drop=FALSE] %&gt;% as.data.frame() %&gt;% mutate(sigma_x=sigma_x)\n    fits &lt;- bind_rows(fits, coeffs)\n    df &lt;- df %&gt;% mutate(y_pred=fit$fitted.values)\n    df_comb &lt;- bind_rows(df_comb, df)\n}\n\n\nggplot(df_comb) +\n    geom_point(aes(x=x, y=y)) +\n    geom_line(aes(x=x, y=y_pred)) +\n    facet_grid(~ sigma_x)\n\n\n\n\n\n\n\n\n\nfits\n\n\nA data.frame: 2 × 5\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\nsigma_x\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nx...1\n1.983005\n0.131168439\n15.118\n1.193266e-46\n1\n\n\nx...2\n1.998115\n0.004886705\n408.888\n0.000000e+00\n26"
  },
  {
    "objectID": "lectures/Lecture2.html",
    "href": "lectures/Lecture2.html",
    "title": "Simple regression: a quick review.",
    "section": "",
    "text": "Some \\(\\LaTeX\\) needs to go here for Quarto’s own crazy reasons."
  },
  {
    "objectID": "lectures/Lecture2.html#alternative-derivation",
    "href": "lectures/Lecture2.html#alternative-derivation",
    "title": "Simple regression: a quick review.",
    "section": "Alternative derivation",
    "text": "Alternative derivation\nAlternatively, our criterion can be written in matrix form as\n\\[\n\\begin{pmatrix}\n1  & \\overline{x} \\\\\n\\overline{x} & \\overline{xx}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\bhat_0 \\\\\n\\bhat_1\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\overline{y} \\\\\n\\overline{xy}\n\\end{pmatrix}\n\\]\nUsing the 2D matrix inversion formula, we see that, if \\(\\overline{xx} - \\overline{x}^2 \\ne 0\\),\n\\[\n\\begin{pmatrix}\n\\bhat_0 \\\\\n\\bhat_1\n\\end{pmatrix} =\n\\frac{1}{\\overline{xx} - \\overline{x}^2}\n\\begin{pmatrix}\n\\overline{xx}  & - \\overline{x} \\\\\n-\\overline{x} & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\overline{y} \\\\\n\\overline{xy}\n\\end{pmatrix}\n\\]\nFrom this we can read off the familiar answer\n\\[\n\\begin{align*}\n\\bhat_1 ={}& \\frac{\\overline{xy} - \\overline{x}\\,\\overline{y}}{\\overline{xx} - \\overline{x}^2}\\\\\n\\bhat_0 ={}& \\frac{\\overline{xx}\\,\\overline{y} - \\overline{xy}\\,\\overline{x}}{\\overline{xx} - \\overline{x}^2}\\\\\n  ={}& \\frac{\\overline{xx}\\,\\overline{y} -\n      \\overline{x}^2 \\overline{y} + \\overline{x}^2 \\overline{y} - \\overline{xy}\\,\\overline{x}}\n    {\\overline{xx} - \\overline{x}^2}\\\\\n  ={}& \\overline{y} - \\frac{\\overline{x}^2 \\overline{y} - \\overline{xy}\\,\\overline{x}}\n    {\\overline{xx} - \\overline{x}^2} \\\\\n  ={}& \\overline{y} - \\bhat_1 \\overline{x}.\n\\end{align*}\n\\]\nThis derivation was a bit tedious, and we’ll explore more general ways to think about these formulas in the coming lectures, as well as how to think more carefully about when this is the only solution. For now, it’s important to note that an explicit expression exists.\nFor the rest of the lecture let’s think about some other ways to solve the problem of fitting a line through data in a way that will shed some light on the strenghts and weakness of least squares."
  },
  {
    "objectID": "lectures/Lecture2.html#nonparameteric-lines",
    "href": "lectures/Lecture2.html#nonparameteric-lines",
    "title": "Simple regression: a quick review.",
    "section": "Nonparameteric lines",
    "text": "Nonparameteric lines\nFor only one variable, regression is actually no a very good solution. We would actually probably use something more “nonparametric.” For example, we could bucket up the regressors and keep an average in each bucket.\n\nnum_buckets &lt;- 20\nspotify_bucketed_df &lt;- spotify_df %&gt;%\n  mutate(danceability_bucket=cut(danceability, num_buckets)) %&gt;%\n  group_by(danceability_bucket) %&gt;%\n  summarise(mean_popularity=mean(track_popularity),\n            bucket_danceability=median(danceability),\n            count=n())\n\nPlotBaseSpotify() +\n  geom_line(aes(y=y_on_x$fitted.values, color=\"Y ~ X least squares\")) +\n  geom_line(aes(x=bucket_danceability, \n                y=mean_popularity,\n                color=\"Averages in buckets\"),\n            data=spotify_bucketed_df)\n\n\n\n\n\n\n\n\nWhat do you think of this line’s behavior for low danceability?\nNote that cut takes evenly spaced slices, which leads to a very uneven distribution of observations in each bucket. How do you think this affects the graph we saw before?\n\nggplot(spotify_bucketed_df) +\n  geom_line(aes(x=bucket_danceability, y=count))\n\n\n\n\n\n\n\n\nThere are more sophisitcated ways to do “nonparametric” smoothers like this automatically, including built-in functions in R.\n\nPlotBaseSpotify() +\n  geom_line(aes(y=y_on_x$fitted.values, color=\"Y ~ X least squares\")) +\n  geom_smooth(aes(color=\"Ggplot smooth\"))\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nThe core problem with nonparametrics is the curse of dimensionality. Suppose we have \\(P\\) regressors, and we divide each into ten buckets. In total, we have \\(10^P\\) buckets, and, on average, \\(N / 10^P\\) observations per bucket. Here are some values for the Spotify dataset:\n\nregressors &lt;- c(\"danceability\", \"energy\", \"loudness\", \"speechiness\", \n                 \"acousticness\", \"instrumentalness\", \"liveness\")\nstopifnot(all(regressors %in% names(spotify_df)))\n\nfor (p in 1:length(regressors)) {\n  num_buckets = 10^p\n  cat(sprintf(\"With %d buckets, we have %0.3f observations per bucket.\",\n              p, nrow(spotify_df) / num_buckets), \"\\n\")\n}\n\nWith 1 buckets, we have 3283.300 observations per bucket. \nWith 2 buckets, we have 328.330 observations per bucket. \nWith 3 buckets, we have 32.833 observations per bucket. \nWith 4 buckets, we have 3.283 observations per bucket. \nWith 5 buckets, we have 0.328 observations per bucket. \nWith 6 buckets, we have 0.033 observations per bucket. \nWith 7 buckets, we have 0.003 observations per bucket. \n\n\nSo the strength of regression for EDA is really in higher-dimensional settings. For example, if we wanted to look at all the outputs at once:\n\nlm_formula &lt;-\n  paste0(\"track_popularity ~ 1 + \",\n         paste(regressors, collapse=\" + \"))\nprint(lm_formula)\n\nspotify_reg_all &lt;- lm(formula(lm_formula), spotify_df)\n\nprint(summary(y_on_x))\nprint(summary(spotify_reg_all))\n\n[1] \"track_popularity ~ 1 + danceability + energy + loudness + speechiness + acousticness + instrumentalness + liveness\"\n\nCall:\nlm(formula = track_popularity ~ danceability, data = spotify_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.024 -18.415   2.934  19.480  57.105 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   35.1757     0.6361   55.30   &lt;2e-16 ***\ndanceability  11.1497     0.9484   11.76   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 24.93 on 32831 degrees of freedom\nMultiple R-squared:  0.004192,  Adjusted R-squared:  0.004162 \nF-statistic: 138.2 on 1 and 32831 DF,  p-value: &lt; 2.2e-16\n\n\nCall:\nlm(formula = formula(lm_formula), data = spotify_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.259 -17.640   3.169  19.037  73.063 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       70.78317    1.40436  50.402  &lt; 2e-16 ***\ndanceability       7.26292    0.95757   7.585 3.42e-14 ***\nenergy           -28.96580    1.15306 -25.121  &lt; 2e-16 ***\nloudness           1.66617    0.06295  26.469  &lt; 2e-16 ***\nspeechiness       -5.21887    1.35498  -3.852 0.000118 ***\nacousticness       4.86004    0.72643   6.690 2.26e-11 ***\ninstrumentalness -12.82676    0.61686 -20.794  &lt; 2e-16 ***\nliveness          -4.39948    0.88735  -4.958 7.16e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 24.23 on 32825 degrees of freedom\nMultiple R-squared:  0.05934,   Adjusted R-squared:  0.05914 \nF-statistic: 295.8 on 7 and 32825 DF,  p-value: &lt; 2.2e-16\n\n\n\nNote that the high dimensional fit can look a bit chaotic when projected onto one dimension:\n\nPlotBaseSpotify() +\n  geom_line(aes(y=spotify_reg_all$fitted.values, \n                color=\"Y ~ X high dimensional least squares \")) +\n    geom_line(aes(y=y_on_x$fitted.values, \n                color=\"Y ~ X simple least squares\"))"
  },
  {
    "objectID": "lectures/Lecture5.html",
    "href": "lectures/Lecture5.html",
    "title": "Simple regression: a quick review.",
    "section": "",
    "text": "Some \\(\\LaTeX\\) needs to go here for Quarto’s own crazy reasons."
  },
  {
    "objectID": "lectures/Lecture5.html#nonparameteric-lines",
    "href": "lectures/Lecture5.html#nonparameteric-lines",
    "title": "Simple regression: a quick review.",
    "section": "Nonparameteric lines",
    "text": "Nonparameteric lines\nFor only one variable, regression is actually no a very good solution. We would actually probably use something more “nonparametric.” For example, we could bucket up the regressors and keep an average in each bucket.\n\nnum_buckets &lt;- 20\nspotify_bucketed_df &lt;- spotify_df %&gt;%\n  mutate(danceability_bucket=cut(danceability, num_buckets)) %&gt;%\n  group_by(danceability_bucket) %&gt;%\n  summarise(mean_popularity=mean(track_popularity),\n            bucket_danceability=median(danceability),\n            count=n())\n\nPlotBaseSpotify() +\n  geom_line(aes(y=y_on_x$fitted.values, color=\"Y ~ X least squares\")) +\n  geom_line(aes(x=bucket_danceability, \n                y=mean_popularity,\n                color=\"Averages in buckets\"),\n            data=spotify_bucketed_df)\n\nWhat do you think of this line’s behavior for low danceability?\nNote that cut takes evenly spaced slices, which leads to a very uneven distribution of observations in each bucket. How do you think this affects the graph we saw before?\n\nggplot(spotify_bucketed_df) +\n  geom_line(aes(x=bucket_danceability, y=count))\n\nThere are more sophisitcated ways to do “nonparametric” smoothers like this automatically, including built-in functions in R.\n\nPlotBaseSpotify() +\n  geom_line(aes(y=y_on_x$fitted.values, color=\"Y ~ X least squares\")) +\n  geom_smooth(aes(color=\"Ggplot smooth\"))\n\nThe core problem with nonparametrics is the curse of dimensionality. Suppose we have \\(P\\) regressors, and we divide each into ten buckets. In total, we have \\(10^P\\) buckets, and, on average, \\(N / 10^P\\) observations per bucket. Here are some values for the Spotify dataset:\n\nregressors &lt;- c(\"danceability\", \"energy\", \"loudness\", \"speechiness\", \n                 \"acousticness\", \"instrumentalness\", \"liveness\")\nstopifnot(all(regressors %in% names(spotify_df)))\n\nfor (p in 1:length(regressors)) {\n  num_buckets = 10^p\n  cat(sprintf(\"With %d buckets, we have %0.3f observations per bucket.\",\n              p, nrow(spotify_df) / num_buckets), \"\\n\")\n}\n\nSo the strength of regression for EDA is really in higher-dimensional settings. For example, if we wanted to look at all the outputs at once:\n\nlm_formula &lt;-\n  paste0(\"track_popularity ~ 1 + \",\n         paste(regressors, collapse=\" + \"))\nprint(lm_formula)\n\nspotify_reg_all &lt;- lm(formula(lm_formula), spotify_df)\n\nprint(summary(y_on_x))\nprint(summary(spotify_reg_all))\n\nNote that the high dimensional fit can look a bit chaotic when projected onto one dimension:\n\nPlotBaseSpotify() +\n  geom_line(aes(y=spotify_reg_all$fitted.values, \n                color=\"Y ~ X high dimensional least squares \")) +\n    geom_line(aes(y=y_on_x$fitted.values, \n                color=\"Y ~ X simple least squares\"))"
  }
]